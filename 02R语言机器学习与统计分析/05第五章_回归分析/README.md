# 第五章 回归分析
## 5.0引言
　　“回归”一词最早是由英国著名统计学家高尔顿及其学生皮尔逊在19世纪末期研究孩子与他们父母身高时提出的。研究结果表明,父母的身高虽然会遗传给子女,但子女的身高却有逐渐“回归到身高的平均值”的趋势,高尔顿和皮尔逊称其为一种回归效应，而他们发现的研究两个数值变量的方法被称为回归分析。
　　回归分析（Analysis of Regression）是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否相关、相关方向与强度,并建立数学模型以便通过观察特定变量来预测或控制研究者感兴趣的变量，它是一种典型的有监督的学习方法。

　　在大数据分析中，回归分析是一种预测性的建模技术，也是统计理论中的最重要的方法之一，它主要解决目标特征为连续性的预测问题。例如，根据房屋的相关信息，预测房屋的价格；根据销售情况预测销售额；根据运动员的各项指标预测运动员的水平等。在回归分析中，通常将需要预测的变量称为因变量（或被解释变量），如房屋的价格，而用于预测因变量的变量称为自变量（或者解释变量），如房子的大小、占地面积等信息。
　　回归分析按照涉及变量的多少，分为一元回归分析和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。针对分类变量，人们提出了Logistic回归，在自变量筛选和多重共线性问题上，人们提出了逐步回归、Lasso回归、Ridge回归等广义线性回归。
　　本章主要介绍回归模型的建立与预测，以及在R环境下的实现。

## 5.1一元回归模型

　　一元回归主要研究一个自变量和一个因变量之间的关系，其中一元线性回归是分析两个变量之间的线性关系，一元多项式回归是分析因变量和自变量次方的关系。
### 5.1.1一元线性回归
   
　　设y是一个可观测的随机变量，它受到一个非随机变量因素$x$和随机误差$ε$的影响。如果$y$与$x$有如下线性关系
$y=ax+b+ε$(5-1)
且$ε$服从正态分布 $N(0,σ^{2})$ ,其中 $a$ , $b$是固定的未知参数，称为回归系数，$y$ 称为因变量，$x$称为自变量，则式(5-1)为一元线性回归方程。
对于样本$(x_{i},y_{i})$ ,模型的预测值为 $y_{i}=ax+b$ ,真实值y与预测值y的差称为样本 $(x_{i},y_{i})$的残差，记为ε=y-y。
给定训练集D={(x1,y),(x2,y2)…,(x,y)},回归分析的目标是找到一条
















