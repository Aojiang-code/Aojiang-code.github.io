# [Multi-head attention](https://nn.labml.ai/transformers/mha.html)

**多头注意力 (MHA)**

This is a tutorial/implementation of multi-headed attention from paper Attention Is All You Need in PyTorch. The implementation is inspired from [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html).








