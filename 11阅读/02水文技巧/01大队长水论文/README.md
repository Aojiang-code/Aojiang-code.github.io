# 大队长手把手带你发论文

## 目录



### 大队长手把手带你发论文



#### 第一章通道注意力模块及其变体

注意:每个模块给定的输入维度可能有微小的差异，例如:字母表示不同，或者省略了某个维度，这是因为原文中的模型图可能会标注具体的维度，为了让读者能够在详解中找到对应的公式和符号，我们的详解主要是以论文的计算流程为主。当然了，详解一定是特别详细的，细化到每个维度解释，每个维度变换，每两个符号相乘的含义以及维度变化，都清楚的标注了出来，相信大家一定能够轻松理解，这样的话看代码也是事半功倍!而且，代码每一行的维度变换，含义我也一一用注释表明了。这份全解我相信一定能够帮助到大家，让大家论文无忧!

##### 1.1 通道注意力(SENet，CVPR2018)

![图1Figure 1: A Squeeze-and-Excitation block.](00图片/20240710/图1.png)


![图2Figure 2:  左边是inception module，右边是SE-inception 模块](00图片/20240710/图2.png)


###### 1.1.1 基本信息

- 标题：Squeeze-and-Excitation Networks
- 会议/期刊：CVPR2018
- 代码：https://github.com/hujie-frank/SENet
- 引用次数：25946
- 原文：[Squeeze-and-Excitation Networks ](http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf)
- 即插即用代码：[SENet]()


###### 1.1.2 思想分析


- 简介:特征图中的不同通道(channel)代表不同的对象，通道注意力显式建模通道之间的相互依赖性，通过网络自适应学习每个通道的重要程度，并为每个通道赋予不同的权重系数，从而来强化重要的特征，抑制非重要的特征。
- 详解:Squeeze(压缩)和Excitation(激励)是SENet的两个核心步骤。Squeeze是指在空间维度上收集全局信息，并将其压缩为一个全局向量表示。Excitation模块对全局向量进行非线性变换，并通过sigmoid函数生成权重向量，然后再使用权重向量与输入相乘，从而加强通道上有用的特征，抑制通道上不太有用的特征。


###### 1.1.3 计算流程

![计算流程](00图片/20240710/计算流程.png)



- 计算细节解释:
1)Squeeze使用的是全局平均池化，在这里可以替换为其他的聚合操作。
2)Excitation中的两个全连接层是简单并且易于推广的，先缩小通道，再恢复到原来通道的大小，目的是捕捉通道之间的依赖关系，并且还能参数化门控机制，因为控机制本身没有参数。
3)SENet在进行全局平均池化前，首先对输入特征 X 做了一次卷积，然后基于卷积输出的特征图 U 计算通道得分，文中解释这么做的原因是:卷积相当于计算了一个局部特征统计量，这个结果再去池化更有意义;

###### 1.1.4 推广和改进

- 模型集成:
SENet可以很容易的与其他模型集成到一起，例如:将变换F替换为 Inception、ResNet 网络或者自己构建的网络。如Figure 2所示。
- 改进Squeeze模块:
特征聚合的增强:SENet使用的是全局平均池化操作，全局平均池化可以被替换为其他模块，例如自己可以构建一个transformerencoder，但要记住输出一定是一个全局向量表示。
- 改进Excitation模块:
SENet的Excitation模块中全连接层和门控单元构成，全连接层主要是为了学习通道之间的关系，可以替换为多尺度卷积网络(学习多尺度的通道依赖性)，或者transformer、UNet网络等。门控单元通常不变。

##### 1.2 变体SKNets



- 标题:Selective Kernel Networks
- 会议/期刊:CVPR2019
- 代码:https://github.com/implus/SKNet
- 引用次数:1978
- 简介:考虑到SENet中只进行了单一尺度的映射，这限制了模型的表达能力。因此，SKNets从多尺度出发，设计了多个具有不同感受野大小的映射层，并基于它们来获得全局信息表示，进一步地，通过不同尺度信息引导的softmax注意力，从而获得每个尺度对应的权重表示，最后对它们进行加权求和。


##### 1.3 变体CBAM


- 标题:CBAM: Convolutional Block Attention Module
- 会议/期刊:ECCV2018
- 代码:https://github.com/luuuyi/CBAM.PyTorch
- 引用次数:14692
- 简介:SE只考虑了通道之间的相关性，而忽略了位置信息的重要性，而在视觉任务中，通常空间结构是最重要的。因此，本文将通道注意力和空间注意力进行了串联，并且在squeeze阶段使用了最大池化和平均池化。

##### 1.4 变体TripletAttention



##### 1.5 变体Coordinate Attention



##### 1.6 变体ECA-Net




##### 1.7 变体DANet





##### 1.8 总结

本节主要介绍了通道注意力和它的一些变体，通道注意力通过聚合全局空间信息表示，进而学习每个通道的重要性。它的变体SKNets弥补了SENet在多尺度特征学习方面的不足，通过引入多个不同大小的卷积核学习空间上的多尺度特征，并进一步通过多尺度信息引导的softmax注意力，来获得每个尺度对应的权重表示，最后加权求和。CBAM将通道注意力的思想应用到空间注意力上，通过聚合通道上的特征表示，来学习每个空间位置的重要性，并进一步将通道注意力和空间注意力串联，取得了更好的效果。TripletAttention提出了三重注意力，在三个分支上分别考虑了不同维度之间的交互，CoordinateAttention将位置信息嵌入到通道注意力，ECA-Net提出了不降维的局部跨通道交互策略，在涉及少量参数的同时具有明显的性能增益。



<br>[返回章标题](#第一章通道注意力模块及其变体)


<br>[返回目录](#目录)




#### 第六章研究生生涯大解救
本章旨在为广大的硕士研究生(或是直博生)提供一整套的能够使你顺利毕业的指南，包含：
- 方向的选择、
- 怎么打好基础、
- 论文的精读、
- idea的来源、
- 代码的实现、
- 论文的撰写等。

用最直观、最简洁的语言描述这其中的点点滴滴，为每一个研究生的顺利毕业保驾护航，你就看大队长的东西干货不干货就完事了!
##### 6.1 如何选方向

选方向是一个永恒的话题，方向没有好坏，只有难易之分。
毫不夸张的说，方向的难易直接决定了你这三年的生活，甚至决定了你以后的命运。
举个例子，如果你选的方向很难，实验室没人做甚至全球都很少有人做，开源代码也是少得可怜，恰巧你的能力也一般，那你这三年很可能过的很难受。
- 实验室没人做意味着你要从头开始，
- 论文很少导致你接受不到足够idea的启发(从其他领域接受idea的启发不是一个新手能做到的)，
- 代码少导致你无法直接在前人的基础上改进(对新手来说在没有参考的情况下，从头码代码是一个非常非常困难的事情)，
- 对于能力一般的你来说，再碰到一个push的导师，可能第一年就会让你怀疑人生，甚至产生退学的念头。

###### 6.1.1 经验之谈

那么应当如何选方向，才能好过这三年，甚至在毕业后也能有自己选择的权利呢?



- 进入实验室的一步：请教一下师兄师姐，调查一下他们的研究方向和研究成果，多听取一下他们的建议。
  - 注意事项：
    - 礼貌大方，请客吃饭
  - 了解内容:
  - 他们的经验、他们踩过的坑：导师的性格、与导师的相处方式
- 跟随研究成果多的师兄师姐
  - 遇到不懂的问题可以及时请教
  - 有代码基础，不必从头开始
  - 注意事项：
    - 礼貌大方
    - 请勿理所当然认为师兄师姐有义务帮你
    - 你与师兄师姐的利益是一致的：发论文+毕业，所以面临站队时，请和师兄师姐站在一起，因为你们是一条船上的人。
- 不要傲慢
  - 请不要因为师兄师姐研究成果少而认为他们的能力不行，因为你不知道他们经历了什么，也不知道自己未来会不会重蹈他们的覆辙。
  - 请多和他们聊天，听听他们的经验，避免他们踩过的坑。
  - 使没有可以继承的研究方向，那么你也能够尽量避免掉入很坑的方向。
- 关于新方向的选择
  - 如果导师给你分配了一个实验室从来没有人做过的方向，请慎重考虑
    - 如果全球范围内做过的人多、论文数量足够，开源代码够多，还是可以尝试接触一下的。
    - 如果很少有人做，论文、开源代码都很少的话，甚至数据集也基本找不到，那么这个方向可能是个死胡同，很难做的下去。
    - 建议如下：
      - 你可以先用一个月的时间研究一下，如果看不到希望，果断把目标瞄准相似领域的研究(即数据集类型相似，使用的方法也是相似的，但是应用场景是不同的)
      - 这样的话，既有内容给老师汇报，也可以轻松地、顺理成章地转变研究方向，也算是给自己留了后路。




###### 6.1.2 选什么方向

- 众所周知，NLP和CV是目前两个发展最快的领域，因此，最具创新性和实用性的技术和方法往往出现在这两个领域。
  - 如果你选择了NLP和CV里最主流的方向，那么这意味着你要和这个世界上能力最强大的一群人竞争，对于普通学校+普通的你来说，不是一件明智之举的事情。
  - 当然，并不是不推荐做CV和NLP，而是不推荐做CV和NLP里面热门的方向，尤其是对于学校一般、导师一般、资源一般的学生来说。
- 尽量选择计算资源较小的研究方向
  - 选择计算资源较小的研究，对于GPU的压力也会小很多，模型训练也相对轻松，至少一上午就可以看到结果，效果不好，可以随时更改模型结构。
  - 如果研究方向需要巨大的计算资源，而你恰巧又没有GPU，首先你得花钱去租多个GPU，并且训练时间会很长，一等就是几天，效果也不知道好不好。倘若有人带路，那还算好说，如果既没资源，又缺少引路人的话，选择方向还是要谨慎一些。
- 只要不是CV和NLP里面的热门方向，并且不需要大量计算资源的方向，都是值得尝试的。
  - 首先，CV和NLP领域提供了最先进的技术和研究思路，可以将其迁移到80%的其他方向上，如果你有能力赶得上这一波热点，在你的领域冲一区轻轻松松，这是一桩稳赚不赔的买卖。
  - 其次，非CV和NLP的方向，尽管有的方向也很热门，但卷的程度还是有限的，发表论文的难度不算很高。
  - 最后，审稿人水平相对有限，不如CV和NLP的审稿人见多识广，如果你迁移了CV和NLP的技术，他不一定见过，这就是论文的信息差。
- 如果你依然拿不定注意，可以选择在知乎上搜一下相关领域的博主。
  - 现在知乎的知识区发展如火如荼，入住了各个研究领域的专业博主。如果你足够细心，那么找到一些专业的博主对于你来说，绝非难事。
  - 付费咨询是一种高效的方式，你只需要花点小钱，就可以获得一些信息差，物超所值，请你一定不要吝啬!
  - 能够找到可以咨询的专业大牛，是一件可遇不可求的事情!


###### 6.1.3 总结
1)选非CV/NLP的热门方向，论文多、代码多，容易follow，并且还能用CV/NLP的新技术
2)跟随研究成果多的师兄师姐
3)没人做的方向，没论文的方向，没代码的方向，不做




##### 6.2 如何打好基础
深度学习方向的研究生，应该怎么样打好基础?
在本章中，将给出适合大多数学生的入门学习方案，旨在帮助每一位研究生能够快速的打好基础，进入论文阅读阶段。
###### 6.2.1 基础算法


- 值得强调的是，我们研究的方向是以深度学习为主，而不是机器学习
  - 这意味着机器学习并不是必需的
  - 通常来说，如果你的目标远大，时间足够，并且以后想要继续深造，可以花一些时间去认真学习机器学习算法和统计学习，
  - 但是如果你仅仅只是想混个毕业、或者普普通通的发几篇论文，只学深度学习算法是完全足够的。
- 深度学习的基础算法有:
  - 多层感知机(MLP)，
  - 卷积神经网络(CNN)、
  - 循环神经网络(RNN、LSTM、GRU)、
  - 注意力机制(Attention)
  - 它们出现在目前80%以上的论文中，是当之无愧的算法之王。
  - 经典的算法还有：
    - 图卷积神经网络(GCN)、
    - 生成对抗网络(GAN)、
    - 深度强化学习(DRL)等
    - 它们通常出现在特定的领域当中，在打基础阶段，不用专门去学习
    - 当你确定方向之后，再去学习领域内常用的专用的算法。
  - 除了以上算法，建议大家认真学习反向传播算法，一定要知道参数是怎么更新的，可以说，反向传播算法是深度学习最核心的一个知识。
- 一定不要在基础算法上用太多的时间，一般来说，半个月即可。
  - 也不用去学更多的算法，因为没有人可以在一时之间消化吸收这么多东西，并且后期用不到那些算法的话，遗忘的也很快，所以一定要选择性学习，学习最重要的有限的知识。
  - 至于很多优秀的算法，则需要时间去积累，不必急于一时。


###### 6.2.2 代码框架选择:pytorch

- 只需要学习python，python的入门很快，难度偏低，b站上找个视频，学学基础语法，学习两天即可，一旦超过两天，不学了!
- 学完python之后，建议选择pytorch来练习深度学习的那些基础算法，TensorFlow现在已经不值得入坑了，目前github开源的论文代码已经很少能见到TensorFlow的影子了，并且，TensorFlow由于版本问题，可能会出现一些难以修改的问题，所以大家一定要选择pytorch。
- 对代码的真正理解，我们还是要放到论文的代码上面，这直接决定着我们的idea的产出、论文的产出。
  - 在所研究领域内找一篇经典的论文，其代码也必须是开源的，逐行阅读，逐行研究，哪怕你花了一个月的时间来研究，那也是真正的花在了刀刃上
  - 只要你吃透了一篇论文的代码，那么你对于pytorch的认知，甚至对于论文的理解，都是跨越性的，你会第一次理解到，读懂论文是一种什么感觉。

###### 6.2.3 阅读论文的基础
论文阅读没有捷径可言，逐字逐行抓段阅读才是真理，也是实现能力跨越性提升的必经之路。
这一部分我在下一小节给大家梳理。
##### 6.3 如何精读论文
毫不夸张的说，精读论文是实现个人学术能力飞跃的关键，精读代码是实现个人学术能力第二次飞跃的关键，两者相辅相成。
对于初学者来说，精读论文，很困难，需要很长的时间，这是研究生的第一道坎，需要很大的勇气和耐心去翻越它。
我不知道该用什么样的语言描述它的重要性，但是现在回头再去精读那些经典论文，我仍然会有不一样的收获!
###### 6.3.1 何为经典论文


经典论文是所属领域的一面旗帜，是绝对经受得起考验的论文，它通常有很多的引用，在github上有很多的star，是学者们争相改进的baseline。
它有这么几个特点:
- 在写作上，
  - 要么是模板级别的，你按照它的写作逻辑、表达方式来写自己的论文是绝对正确的
  - 要么是不走寻常路的，因为作者有足够的底气使他成为经典
- 在方法上，
  - 通常是简单有效，可拓展的
  - 经典论文之所以是经典论文，就是因为它是足够简单有效的，可拓展性强
  - 纵观古今，经典论文不知道养活了多少论文，transformer发表在nips2017，至今已有108423次引用
- 在代码上，
  - 是经得住考验的，效果绝对是实打实好的，要不然也不至于成为经典论文。
  - 这意味使用它就会有性能的提升，这对于每个科研人都是一个机会，能不能抓得住就看你的了。
  - 至于代码写的好不好，规范不规范，这都不重要，只要它是经典论文，很快就会出现很多版本的代码，你选择一个你喜欢的就ok了。



###### 6.3.2 为什么要读经典论文
1. 经典论文，通常会在引言或相关工作将领域的发展脉络的明明白白，使你快速把握该问题的发展历史和方法，你完全可以按照它引用论文的顺序去阅读论文，这样的话你能够快速入门该领域，最小化你的学习时间成本。不至于无法把握方向，随便找论文阅读，读了一个月发现没有代码，白白浪费时间。
2. 它将会成为你的写作模板，代码模板。经过多次的认真的精读经典论文，每次都感叹竟有人能把论文写的这么好。我这篇论文十年的功力，你顶得住吗。这样经得起考验的论文，你说你不参考它，参考啥。代码成百上千个star，你不用它，你用谁?
3. 你的工作极有可能在它的工作上进行拓展，经典论文的idea出发点一定是该领域的痛点，从它入手准没错!完全值得你在它的基础上进行二次改进，这是代价最小，成功率最大的方式。
4. 需要注意力的是，新手可能无法识别什么是经典论文，可能当这篇论文已经成为经典论文之后，你才能意识到，学术老手可能在这篇论文刚发表之后就能够意识到它的潜力。这就是你和学术老手的差距，当你不断的阅读论文，不断的写代码，随着能力的提升，你也会逐渐具有这种能力，所以，想以后在学术圈混的，想认真做科研的，一定要耐心的猥琐发展。想混论文毕业的，找几篇近两年高引用的论文魔改就完事儿了。



###### 6.3.3 什么人需要精读论文?
1. 我曾经发表了几篇论文，在那之后，我认为一篇论文我只需要10分钟就能读懂。在一次偶然的机会，我将一篇经典论文逐字逐句翻译，并将它们写到我的文档里，然后对每句话进行分类、提炼核心,一篇十页的论文用了我两天的时间。那可是我读过很多遍的论文呀，足足用了两天，因为我意识到，论文里关于方法的很多细节我第一次才真正的搞懂，我感觉我又学到了新鲜的知识。在那之后，我以同样的方式，精读了八篇经典论文，用了很长很长时间，但是对我来说，这是一次能力的升华。至今为止，如果去读另一个从没接触过领域的论文，我会以同样的方式去精读，事实证明，我能够用最短的时间去理解透彻。
2. 回到主题。首先，我认为刚入门科研的，大三大四的学生，还有研一、博一的科研小白，这是必须精读的人群。
3. 还有研二的同学，可能读了很多论文，但是对自己所研究领域依然丝毫没有头绪的，对于这种情况我只能认为你没有真正读懂过任意一篇论文。以我的观点来说，读懂一篇论文，你要明白这篇论文包含的一个深层次的意义，就是说，我不仅知道他用的什么方法解决的什么问题，那么我还要知道他为什么要用这种方法(从理论上，明确算法的一个特点/优点)，这个方法为什么能够解决这个问题(从实践上，实验结果所证明)。前者呢，主要是需要我们在理论上明确算法的一个优点，也就是说，它的优点正好能够克服这个挑战，对症下药。后者呢，主要是在实践中得到，很多时候，即使理论上说得通，但是效果未必就好，哪个模块效果最好，你要到实验中找答案，当你找到了真正起作用的模块，那么以后就可以作为你的trick常出现在你的论文中。
4. 大家可以反复思考一下，你们读过的论文，是否能够得到以上这两个答案，如果没有的话，我建议你再去原文中寻找一下答案。精读绝非易事，需要付出很大的精力和时间。但是，如果想第一年就有论文产出，吃点苦又算得了什么呢。想想自己有论文之后的快乐生活，想想很快就能为真正的目标而奋斗想想有论文之后老师不会再push你，这都值得你花一些时间去精读，要时刻记住，只有精读，才能有很快的产出。



###### 6.3.4 怎么样精读论文?
1. 我在公众号:科研水神大队长，发表过两篇精读论文的笔记，大家关注一下就可以收到自动回复。

感谢关注科研水神大队长大队长！VX：Brilliance_07
如果你是深度学习领域的学习者，关注我可能会有意想不到的收获！
SCI精读与写作逻辑深度剖析笔记：https://pan.baidu.com/s/1q-BLEE7HUs7qO6vSAgW7qg 提取码：yjkl
深度学习创新点系列笔记：https://pan.baidu.com/s/1ylvn82nm2X6yulYGk5d9-A 提取码：kljt
深度学习基础学习资料：https://pan.baidu.com/s/1-0-5Zh1TgDsHQlketW0USg 提取码：mp0o

2. 简单来说，主要是，在读每一个部分的时候，我们需要明确它对应哪些知识点。例如:
3. 摘要:是一篇论文的精髓，通常按照“背景- 挑战-方法·贡献” 的顺序来进行写作。
4. 引言:可以看作是摘要的完整版，主要分为四部分，首先明确问题所处的一个背景，阐述其现实意义，然后阐述该问题的历史发展(对应的是大家提出了什么样的方法来解决你所研究的这个问题);接着，表明当前面临的一个挑战，这些挑战是之前的方法无法解决的;最后，提出并简单介绍我们的方法，总结贡献。
5. 相关工作:阐述当前问题的发展历史，研究该问题的主流算法的发展历史。
6. 方法:一般遵循总分，先整体概括模型，然后分别详细介绍每一个模块(动机+方法+公式表达+解释)。
7. 实验:通常会介绍数据集、模型参数设置、通过不同的实验内容来验证模型的性能，当然了，在介绍每一个实验时，也要遵循 实验内容+实验描述+分析原因的顺序。
8. 结论:首先描述在一个什么样的背景下，我们来研究这个问题，然后，我们提出了什么样的方法，怎么做的，做的程度怎么样，最后，展望未来，有一些什么没有解决的问题，或者说是在哪一方面我们还能够再进一步。

因为内容过多，具体可见我的b站视频:https://www.bilibili.com/video/BV1cj411B7tk/?spm_id_from=333.999.0.0&vd source=8a5d2efc04d2b9e4b9d59aa1c8f90834
https://www.bilibili.com/video/BV12h4y107F7/?spm id from=333.788&vd source=8a5d2efc04d2b9e4b9d59aa1c8f90834



##### 6.4 如何精读代码
代码是每个科研人都绕不过的槛，对有的人来说很简单，对有的人来说难如登天，一想起来就头疼。
对于新手来说，如果代码基础仅限于了解python语法、或者了解简单基础算法的实现，那你也别担心，因为绝大多数人站在同一起跑线上。
深度学习算法代码是一个新的风格，和开发类的代码并不相似，大家不用担心，要有自信，真的不难。
对于刚入门科研的同学来说，精读是你的第一道坎，那么精读代码就是你的第二道坎(如果你按照我的方式来选择论文精读，那么它必然含有代码)。

经典论文的代码通常在github上发布，并且通常有很多人复现过，所以大家有选择的权利，要选择基于pytorch的代码，即使它不是官方代码。
拿到代码的第一件事，就是按照代码的markdown文档查看说明，配置环境，为了快速复现，大家可以去网上租GPU环境、框架都是集成好的，基本五分钟就能够上手，还是非常方便的。
然后，不惜一切代价运行起来当然，能直接运行的代码少之又少，你需要具备一些修改错误的能力，如果不会，那就百度、google.GPT、issue提问、给作者发邮件请教，如果实在改正不了错误，那就到github上找别人复现过的代码一个一个轮着试，毕竟是经典论文，总能复现成功的。
当代码跑起来之后，那你的正式代码学习过程就开始了，一个深度学习算法通常有:数据处理层、训练层、模型层、工具类函数等。学习流程如下:
1. 首先，大家要学会debug，即一行一行的查看代码的运行情况，有助于了解数据的计算流程，以及输入前后，数据的shape变化情况，这极其重要，可以说，深度学习玩的就是shape，只要shape能对得上，模块就可以往里面塞。
2. 其次，建议大家从数据处理层开始，好好研究一下数据集被处理成什么样子，怎么样的数据划分标准，以及输入到模型层之前应当重塑成什么样的shape。
3. 第三，模型层是重中之重，一定要一行一行的debug，这样才能够使你真正明白每个模块的每个公式到底是怎么样以代码的形式实现的，以及输入前后的变化是什么，这个时候，你才算是真正读懂论文
4. 当然，精读代码的意义远不止于此，你还要明白，这个模型为什么work，真正起作用的部分在哪里?每个模型都或多或少使用了一些trick，如果你不精读代码，你只知道作者声称这个模块很有用，但却不知道为什么用到自己的模型中效果就不好。所以，大家要对经典代码的每一部分进行验证，找到最有效的那部分，那么它以后就可以作为你的trick出现在你的论文中去。
5. 代码是论文实打实的实践成果，在实践中，你才能知道什么是对的，什么是错的;什么是有效的，是什么是无效的;什么是高效的，什么是低效的;在实践中，你才能通向更远的路，拥有更多有趣的想法，没有实践，只有纸上谈兵，终究是昙花一现，无法得到有效的成长。
6. 如果大家能够按照以上部分来认真精读两篇论文以上的代码，我保证大家能够打下一个很好的基础再加上我给大家整理的最经典最新的模块，大家可以省去很多找论文、阅读代码的时间，能够以最快最有效的方式产出自己的第一篇论文、第二篇论文!

##### 6.5 什么是科研创新
到底什么是创新，这是困扰广大研究生新生的一个问题，大家可能总觉得我需要在这个领域做出一个新的模块、新的方法，没有人发现过的，这才叫创新，我只能说大家还饱有对这个世界的热爱，拥有最纯真的想法，没有被现实吊打过。
最近，抖音上看了一个很有趣的视频，指出一个观点:创新性模仿，首先是模仿，普通人做事，切忌一上来就创新，你没那个能力，你的认知和能力不够，盲目去做会浪费大量的精力和财力，应对去模仿有结果的人做事，例如你看到一个爆款视频，可以去研究他的文案，讲了什么样的故事，在什么样的场景，怎么拍摄的，怎么剪辑的，然后在这个基础上去创新，可以去修改一下文案，或者换一个类似的故事来讲。
贾玲的热辣滚烫不也是这样吗，难道这部电影不优秀吗，贾玲不优秀吗。
同理，科研创新也是这样，每一个跨世纪的创新都是经过无数次的微小创新一步一步演变而来的，可能我们只是替换了某个模块，就恰巧验证了整体模型的优越性，这也是我们的贡献呀。
本小节我们会具体的讲解一下，到底什么是创新。

###### 6.5.1 科研创新的分类
我们首先来了解一下，创新的方式有哪些，哪种创新的方式最适合我们。
1. 算法创新
CNN、RNN、GCN、Attention、GAN，这些都是伟大的算法，是推动深度学习大进步的算法，定深度学习的基石，这里面的作者要么是图灵奖的获得者，要么是在获得图灵奖的路上。这些作者是站在金字塔顶尖的人，只可仰望。
2. 架构创新
例如ResNet，深度神经网络的开拓者，transformer，CV和NLP的大一统模型。前者谷歌引用20W+次，后者引用10W+次，分别在不同的时期奠定了深度学习的主流架构，开启了属于自己的时代。
现如今，transformer的所有作者都已经独立创业了，这是属于他们的时代。
3. 迁移创新
Transformer发表在nips2017，主要是为了解决机器翻译问题，
vision transformer(VIT)发表在ICLR2021，用于CV的图像分类任务，但是因为其模型“简单“且效果好，可扩展性强，成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究，目前谷歌学术引用30396次。它打破了CV和NLP在模型上的壁垒，开启了CV新时代，并把CV任务刷了个遍。这是一个伟大的迁移创新的例子
4. 方法创新
据统计，
CVPR2021的Transformer工作:43+篇;
ICCV2021的Transformer工作:120+篇，
那么这些论文呢，绝大多数最主要是对attention的改进，它们的基准模型都是transformer，
时至今日，对attention的改进，依然是深度学习领域最热门的创新应用之一。
本书也是顺应时代潮流，以attention为主，将其应用于通道、空间、时间，以及关于线性注意力的最新创新。
5. 组合创新
组合创新我认为是方法创新的一种，在这里我单独拎出来。
组合创新通常避循:(基准模型+模块)这么一个思路。
基准模型最好是最新的顶会顶刊的模型(例如Transformer，u-net网络等)，模块也是顶会顶刊提出的某个具体算法，例如卷积、图卷积、注意力等。
当然了，大家可千万别小看组合创新，它上可发顶刊顶会，下可发三区四区E1;它不仅是普通科研小白入门科研的最佳方式，也是很多大佬学生仍然在使用的方式;它上能够帮助你留高校任教，下能够帮你轻松毕业!
听我的，大家一起来给组合创新磕一个!
组合创新姥爷，新年好!
值得一提的是，本书为组合创新而生，我为大家提供了超过30种的模块，除了通道注意力部分，其余的均是近三年内的最新模块。
这一部分的内容具体可见:https://www,bilibili.com/video/BV1Bz4y1T72F/?spm_id from=333.788&Vd source=8a5d2efc04d2b9e4b9d59aa1c8f90834


###### 6.5.2 科研人的实力等级划分
1. 一级学者，称之为入门科研玩家，通常喜欢使用旧方法来解决旧问题，这是一种玩别人剩下的行为别人吃肉(发顶会)，你也能喝口汤(发三区四区)，还是挺不错的。
2. 二级学者，称之为普通科研玩家，通常这一阶段需要独立思考去构建比较新颖的方法，或者率先迁移其他领域的论文的方法，论文即可轻松发表到二区或者一区。别人吃肉，你当第一个喝汤的。
3. 三级学者，称之为小牛科研玩家，通常是开辟了一个新的研究领域，你的论文将作为该领域的开山之作，获得无数的引用。
4. 四级学者，称之为大牛科研玩家，提出了极致的新方法来解决旧问题，例如transformer、GCN等完全开辟了一种新的技术，在深度学习史上留下了最浓重的一笔。
5. 五级学者，称之为院士科研玩家，提出了新的方法来解决新的问题，例如:CNN的提出，用于图像识别，足以成为深度学习的的奠基人。

对于我们广大的普通学生，入门科研玩家来说，用一些旧方法来解决旧问题，也能混个毕业，如果想以后继续读博，走学术的路子，大家就要朝着普通科研玩家努力，这并不是一条简单的路。
本书的目的，就是尽可能的，给大家整理较新的方法，让大家以最短的时间来学习，最短的时间来实现，从而来帮助大家尽可能的发表好的期刊和会议，
##### 6.6 科研创新的流程
在本章节，我们首先阐述正确的科研流程是什么，它适用于哪些人群，然后明确它的利弊，并提出第二个更适合一般学生的科研流程，并通过举例来帮助大家深入的理解。



###### 6.6.1 一般的科研流程
1. 首先，明确领域内存在的问题，其次，提出合适的算法去解决
2. 我们都知道，通常呢，创新来源于问题，首先你要明白这个领域存在什么问题，其次你才能选择合适的算法去解决它。这是一般科研的流程，但是大多数人会卡在第一阶段，我们怎么样才能找到领域内存在的问题呢。就我的经验来说，这不仅需要你具有一个广泛的论文阅读基础，还需要你有深刻的思考能力，才能精准的发现问题。当然，对新手来说，你的第一次，很有可能发现了一个没用的问题。
3. 怎么样破局?
- 第一，你所在的实验室资源充足，有大量的顶会顶刊积累，那你拥有选择的权利，随便follow一个做的好的方向即可;
- 第二，你的实验室存在良师益友，导师愿意听取你的想法，会给你建议并支持你去做你想要做的，师兄师姐同门们个个人好心善，大家喜欢聚在一起交流问题，共谋学术发展;
- 第三，你自己足够的努力，有足够的天赋用有限的时间去搞懂这一个领域存在的问题，并提出新的方法。
- 以上三点，我个人认为只要你能够满足任何一点，你就可以追寻真正的学术之路了，我真诚的希望每一个认真的科研人都能够做出优秀的科研成果，为我国科研事业添砖加瓦，并拥有美好的未来。
4. 总结一下:当你费劲巴拉运气爆炸提出了一个有质量的问题之后，如何提出一个优秀的算法?
首先你需要有充分的理论基础(用于帮助你理解每个算法的优点和缺点)，充分的论文阅读基础(帮助你了解经典、最新的顶会顶刊的模型架构)，
当你有了这些基础之后，你是不是才能提出属于自己的模型呀。
这样看似合理的科研流程，不知道劝退了多少硕士生，那么在我看来，他合理，但他绝对不适用大多数普通学生。
那么我们怎么样才能提出一个适合大多数人的科研流程呢? 组合创新闪亮登场!!!



###### 6.6.2 组合创新的科研流程
1. 首先，提出一个组合模型，然后，找能够解决的问题。
2. 在第一个阶段，你需要去了解一些经典的、最新的顶会顶刊的模型框架，以及具体的模块，通过总结，然后组合出一个属于你自己的模型。(相比一般的科研流程，这里只是排列组合已有的工作，难度大大降低)。
3. 在第二个阶段，要找到你的模块来源的论文，要去精读，通过这些论文，要明确它们的动机，它们的贡献，以及它们要解决的挑战是什么。以这样的方式，大家可以组合出一套挑战，以及对应的解决方案。
4. 组合创新牛逼就牛逼在，它暂时抛弃了之前那些繁琐的过程(寻找问题，寻找对应解决方案)，简化了整个科研流程(从寻找问题、寻找方法，改变为组合问题、组合方法)。当然了，出来混都是要还的，如果以后还想在科研圈混，这些知识都是要慢慢学回来的。但是，在目前阶段，我们可以不需要它!!
5. 但是一定要注意，组合创新也需要精读两三篇论文打基础的，并不是说零基础就能够做得了的。并且，精读两三篇论文后你同样是一位科研新手，距离大佬还有很远很远的距离。想要摸透一个领域，没个一两年的功夫是做不到的，想做出好的成果，那就更加需要时间的沉淀了。大家一定要稳扎稳打，切忌心浮气躁。
综合以上分析，毫无疑问，我向广大的普通学生推荐:组合创新!

###### 6.6.3 组合创新的数学方程及深度分析






图40：来自论文"Enriched CNN-Transformer Feature Aggregation Networks for Super-Resolution"的模型图，上图是（a）,下图是（b）



1. 组合创新=基准模型(Transformer,U-Net)+模块(Attention、GCN、CNN)
2. 对于组合创新来说，基准模型，可以是Transformer、UNet这种成熟的框架，我们只需要对其中的模块进行修改替换即可;也可以是近两年的顶会顶刊模型，将它们的框架拿来用，但是模块要进行修改替换;又或者是串行并行等简单结构。以上三类是目前模型架构最重要的来源。
3. 对于基准模型来说，基准模型=论文模型-模块(Attention、GCN、CNN等等)=MLP+Add+Norm+连接方式。
- 基准模型是模型的模板，也就是说，它的架构不能变，但是模块可以变。
- 类似装修房子，无论软装硬装，但是房子架构是不会变化的。
- 因此，基准模型=MLP+Norm+连接方式，这四个要素构成了基准模型的骨架，如图40(b)所示，当去除模块之后，剩下的架构就是基准模型，我们完全可以在这个基础上进行二次、三次开发，如果我们再机智一些，重新画个图，那就更棒了!(如果你有信心把房子的架构整的更好，更高大上，那对基准模型进行修改也是完全可以的，如果没有这个能力就老老实实用这套方式来找基准模型)
4. 如图(41)所示，我们将图(40)的绘图风格进行了修改，我们沿用了图(40)的基准模型，即两分支结构。
- 此外，我们对整个模型进行了两部分的修改，
  - 首先，我们将Transformer分支和CNN分支分别替换为了通道注意力和膨胀卷积(这是模块的替换)，
  - 其次，我们在encoder和decoder之间额外加入了一个转换的模块(模块的增加)。
  - 当进行了绘图风格的改变、块的替换等修改之后，我们构造出了一个属于我们自己的模型，它建立在图(40)的基础上，但却又看不到图(40)的影子。
  - 如果这是跨领域的应用呢?毫无疑问，这足以称之为是我们的原创模型。
5. 如果我们积累了足够多的新模块，新模型，甚至是其他领域出色的模型，这意味着什么?
意味着创新点对我们来说是手到擒来，只有我们想不想，没有做不到这一说。(狗头)


图41：将图（40）的画图风格改变










###### 6.6.4 组合创新的例子-1
1. 为了使各位读者更直观的认识组合创新的威力，我们举了两个例子来帮助大家理解，第一个是生活中的例子，例如现在有两篇论文分别研究了两道菜:
- 第一篇论文的主题:为了增加蛋白质的摄入，我们做了一盘水煮鸡胸肉;
- 第二篇论文的主题:为了增加维生素的摄入，我们做了一盘菠菜炒鸡蛋;
2. 那么将两篇论文汇总，
- 食材有:鸡胸肉，菠菜，鸡蛋，
- 烹方式有:水煮，炒
3. 然后，我们可以将这些食材和烹饪方式进行二次组合，
- 例如:
  - 鸡胸肉+菠菜，我们不选择水煮和炒，我们选择凉拌的烹饪方式。
  - 在此基础上，我们进行食材的修改，将鸡胸肉切成鸡丝，
  - 那我们最终的主题就是:鸡丝凉拌菠菜。
4. 对应的，我们通过结合两篇论文的动机，将其重新描述为:为了同时增加蛋白质和维生素的摄入，来多方面补充人体营养，我们做了一盘鸡丝凉拌菠菜。
5. 将这个例子套入到我们的组合模型中去，食材就是模块，烹饪方式代表基准模型(组合方式)，将鸡胸肉改为鸡丝，这是模块的改进。
###### 6.6.5 组合创新的例子-2
1. 再举个关于CNN和Attention在CV中的例子，例如:图像分类、目标检测、实例分割等领域
-  第一篇论文的主题: 为了建模视觉数据中的高频特征(局部边缘、纹理)，我们利用CNN，通过感受野内的局部卷积覆盖更多的局部信息，从而有效的提取高频表征。
-  第二篇论文的主题: 为了建模视觉数据中的低频特征(场景或对象的全局形状和结构)，我们利用Attention，通过在图片中对所有的patch进行信息交换来捕捉全局信息(低频)。
4. 那么将两篇论文汇总，
- 存在的模块只有:CNN，Attention 两种，
- 那我们的论文可以定义为:为了同时建模视觉数据中的高频和低频特征，我们提出了一种新的模型CAT，
- 它以一种互补的方式集成了CNN和Attention的优点:具有同时捕捉高频和低频信息的能力。
5. 即使这个例子非常简单，只是简单组合了CNN和Attention，但它依然包含了最重要的思想，即:任意两个模块是都可以组合的。
- 例如，
  - 如果我们将两个模块定义为:A=A1+A2，B=B1+B2，
    - 它们既可以以全局的方式组合(A+B)，
    - 也可以以局部的方式组合(A1+B1)，
    - 还可以以全局+局部的方式组合(A+B1)。
  - 所以大家一定要打开思维，组合创新绝对不仅仅是A+B。
  - 组合创新，广阔天地，大有可为!
  - (这里要注意一下，这里的模块组合之后可以作为一个新的模块嵌入到基准模型中去)


###### 6.6.6 组合创新的升华
根据组合模型的公式:组合创新=基准模型(Transformer,U-Net)+模块(Attention、GCN、CNN)
我们以盖房的例子来形象的说明:
1. 盖房的原材料:砖头、水泥，对应组合创新中的模块。
2. 盖房的图纸:即房子的架构，对应组合创新中的基准模型，
3. 新房的装修:对应组合创新的优雅、高大上程度，唬人的程度。
4. 综上三部分，怎么样才能使组合创新更牛逼，更容易让人接受，从而投稿到更好的期刊或者会议呢?直观上说，如果我们用最坚硬的材料、质量最好的水泥，会使房子更加坚固，甚至能抗九级、十级地震，这是强大的贡献，对应的就是模块的改进，也是本书的核心所在，见第一章、第二章、第三章、第四章、第五章，大学学会使用,水一篇三区四区轻轻松松。
5. 其次，伟大的设计师，他伟大就伟大在，设计出了前无古人的建筑，例如，上百层的高楼，之前这样的高度面临着不稳定、容易坍塌的危险，解决这个世纪难题毫无疑问会使建筑行业进入到一个全新的时期，这对应的就是基准模型，例如:transformer，实现了CV和NLP的大一统，将深度学习推向了一个新的高度，而现在，我们也能轻松驾驭这个基准模型，使我们的型更加有效、更加稳定。基准模型的改进有一定的困难，大家可以选择经典的、最新顶会顶刊的基准模型进行二次改进。
6. 最后，原材料和架构是房子最根本的功能性结构，是所有功能的来源。但，房屋的装修程度，软装硬装是否奢华舒适，同样会让人眼前一亮，即使它并没有带来根本的改进，即使是可替代的，但仍旧会让人产生心理上的喜爱。这代表着组合模型的高大上程度，例如:绘图是否高级、模型细节是否处理的好、融合的方式是否直观，容易让人接受，即使它们没有大的改进、甚至没有改进，但仍旧不能忽视他们存在的意义。毕竟，模型也是任人打扮的小姑娘，打扮的好看，喜欢的人就多。
##### 6.7 基准模型的选择
本章节为大家罗列出了并行融合基准模型、串行基准模型、transformer基准模型、多尺度融合基准模型、unet基准模型，大家的重点不要放在这个模块上，能不能看懂无所谓，重点要看这个架构能否套用到你的领域中去。
###### 6.7.1 并行融合基准模型
并行融合基准模型通常存在两条分支，并在每一层之后两部分进行融合，融合后的输出要么用于两分支的输入(见第一个、第三个模型)，要么用于输出(见第二个)。
第一个:







图42：来自论文"Enriched CNN-Transformer Feature Aggregation Networks for Super-
Resolution",, WACV2023.



第二个：







图43：来自论文"SEAFORMER: SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE 
SEMANTIC SEGMENTATION. ICLR2023

第三个：









图44：来自论文"GA-HQS: MRI RECONSTRUCTION VIA A GENERICALLY ACCELERATED 
UNFOLDING", arxiv2023.








###### 6.7.2 串行基准模型
串行是当今模型的主流，模块主要是基于transform encoder的改进，通过堆叠多个进行输出，论文核心创新点在于对attention的改进或完善。
第一个:











图45：来自论文"P2T: Pyramid Pooling Transformer for Scene Understanding", TPAMI2022.














第二个：













图46：来自论文"Inception Transformer", CVPR2022.





###### 6.7.3 多尺度融合基准模型
多尺度融合是所有领域都绕不开的一个思路，以下是两个多尺度建模的模型，都是先聚合多尺度信息然后进行统一建模。
第一个:














图47：来自论文"PP-MobileSeg: Explore the Fast and Accurate Semantic Segmentation Model 
on Mobile Devices", Arxiv2023.


第二个：












图48：来自论文"IncepFormer: Efficient Inception Transformer with Pyramid Pooling for 
Semantic Segmentation", Arxiv2023.



###### 6.7.4 Transformer基准模型
基于Transformer的改进是当前深度学习领域最热门的改进方式，在CV领域，主要是堆叠多尺度的transformer encoder层，在序列建模领域，主要是对原始的transformer模型进行改进(第二个)，或者将其他思想融入原始transformer模型(第一个)。
第一个:将局部注意力和U-Net架构结合到Transformer中















图49：来自论文"Do we really need temporal convolutions in action segmentation", arxiv2022.


第二个：改进注意力机制，并且在encoder和decoder中间添加额外的模块










图50：来自论文"Learning All Dynamics: Traffic Forecasting via Locality-Aware Spatio-Temporal Joint Transformer", TITS2022.


###### 6.7.5 Unet基准模型
Unet具有对多尺度建模的特性，也是被广泛拓展到各个领域的出色的模型，大家可以多多尝试。unet的输出层:可以是将多个尺度的信息逐步融合，最后统一输出(第一个)，也可以将不同的尺度独立输出，最后融合多尺度信息(第二个)。
第一个:








图51：来自论文"CellViT: Vision Transformers for Precise Cell Segmentation and Classification", arxiv2023.
 
第二个：
















图52：来自论文"U-MixFormer: UNet-like Transformer with Mix-Attention for Efficient Semantic Segmentation", arxiv2023.

##### 6.8 模块的选择、改进、迁移
当我们选中某个基准模型之后，我们应当对模块进行改进或者替换，或者组合多个模块来取代它。这是我们论文当中的核心创新点，在本章中，我将详细为大家介绍:应当如何选择合适的模块?模块应当如何进行修改?如何进行模块的迁移?
###### 6.8.1 模块的选择
1)通道注意力适用于任何领域，通道注意力机制可以帮助模型重点关注特定的通道，提高型对重要特征的提取能力，减少对无关信息的关注，从而提升模型的性能。本书在第一章介绍的通道注意力机制，集中于多尺度通道建模，通道和空间联合建模，以及局部通道交互等。这些模块可以作为任意模型的创新点出现。此外，可以将通道注意力看作是一种思想，是指通过压缩空间信息来获得通道描述符表示，这同样可以应用到任意一个领域。
2)空间注意力的应用极为广泛，例如CV、NLP、语音识别、时空数据预测等，只要是存在不同类型的数据点，空间注意力机制都可以应用。空间注意力旨在动态建模不同空间位置之间的相关性，并根据其重要性进行加权，从而使模型能够聚焦于重要的空间区域。本书在第二章介绍了注意力机制和最新的空间注意力机制的改进，这些模块能够作为核心创新点插入到已有的模型中去，并带来有效的提升。
3)时间注意力主要应用于序列数据，例如时间序列预测、时空数据预测等。时间注意力主要是建模序列中不同时间步之间的相关性，从而使模型在不同的时间步聚焦于其他不同时间步的特征。本书在第三章详细介绍了多种时间注意力机制和时间序列建模模块，能够有效的为你的时间序列模型添加出色的创新点，助力于高质量论文发表!
4)我们在第四章介绍了线性注意力机制，第五章介绍了多种融合机制和常用的特征提取模块，你可以应用前者降低整体模型的复杂度，使模型具有线性复杂度以及拥有更快的计算速度、更小的计算资源。后者提供的模块基本适用于绝大多数领域，大家作为创新点来用，是一点问题都没有的!即插即用!
5)只要大家能够将模块和问题联系起来，找到一个有效的连接点，那么模块就可以完美的嵌套到自己的模型当中去。即使找不到，也要硬找，无非就是讲故事嘛!


###### 6.8.2 模块的改进-实例分析1








图53：模块改进的实例分析。












图54：模块改进的实例分析。











我们将输入定义为时间序列数据(任何数据均可，只是为了举例):XeRxD
1)众所周知，以往的卷积神经网络都是通过串行的方式堆起来的，层数一深容易产生过拟合现象，而Inception conv的提出解决了这个问题，通过使用并行的多分支结构来保证稀疏性，还能提取时间席列的多分辨率特征，如图53(a)所示。
2)那么(a)的使用已经过于频繁，为了显得更先进一些，我们将普通的卷积层替换为dilatedconv(膨胀卷积，其他的分组卷积、深度卷积等等都可以)，通过设置膨胀率来提取具有更大更稀疏感受野的信息，那么可以将三分支分别设置不同的膨胀率，以此来捕捉多尺度的长程信息，最后将三部分进行相加并输出。我们将(b)图其命名为:inception dilatedconv。
3)仅仅更换为膨胀卷积还是不足以体现出新颖性，我们换个角度出发，多个分支并行，提取的是不同尺度，或者说是不同角度的信息，那么它们所起的重要性不一定是相同的。在不同的时刻，每个分支所起的重要性是不同的，因此，我们不能简单的将三分支的输出进行加权。而是应当设置一种加权融合的方法，我们在第五章提供了多种不同的融合方法，在这里可以完美替代普通的相加操作。(这里我们省略了模块图)
4)除了(3)中提到的多尺度融合方法，我们还可以使用门控机制的思想来融合三分支的输出。如图53(c)所示，在这里，我们引入了门控机制(由sigmoid实现)，通过设置一个门控单元来控制信息的流向，从而选择一部分有用的信息进行输出。这里有两种做法，第一种是为三分支分别设置门控单元，然后对选择的三分支信息进行相加操作。第二种是先对三分支进行相加，然后使用一个门控单元来控制总信息的选择比例。我们将(c)(d)图其命名为:Gatedinception dilatedconv。
5)在图53的基础上，我们再次改进，如图54所示，这次我们不再使用自身信息生成的权重了，而是又另外设置了一个相同的分支:左边分支后接一个tanh激活函数，然后作为输出，右边分支作为门控单元的输入来生成权重，最后两部分逐点相乘获得最终的输出。具体的模块可以是图53中的任意一个。这一个思想和section3.5是相似的，大家可以对比一下。
6)我们还可以将三分支结构分别设置不同的模块，例如:膨胀卷积+空间注意力+通道注意力，但是要记住，要根据自己研究的问题来设计合适的模块，不能随意拼凑，有啥用啥，这样不合适。(大家可以多泛读一些论文，看看顶会都常用哪些算法，然后基于这些算法在本书中找到对应的相似的算法类型来使用)


###### 6.8.3 模块的改进-实例分析2











图55：模块改进的实例分析。（a）是发表在CVPR2019的 “Selective Kernel Networks” （见section1.2）。（b）是发表在CVPR2022的 “Beyond Fixation: Dynamic Window Visual Transformer”

1)我们先简单回顾一下SKNets:SKNets从多尺度出发，设计了多个具有不同感受野大小的映射层，并基于它们来获得全局信息表示，进一步地，通过不同尺度信息引导的softmax注意力，从而获得每个尺度对应的权重表示，最后对它们进行加权求和。
2)定晴一看，直观上来说，图55(b)的动态多尺度窗口模块是等价于SKNets的。但在动态多尺度窗口模块之前，额外设计了一个多尺度窗口多头自注意力模块，来捕捉多尺度的窗口信息。后者则是对多尺度信息进行自适应加权融合。
3)简单来说，(b)相当于SKNets在每个尺度将卷积操作替换为窗口自注意力，是一个变动不大的操作，但可能时机赶的好一些。所以证明了一个道理:经典永不过时，哈哈，不过建议大家尽量还是要选取新一点的模块，这样更容易唬人，如果你是跨领域迁移方法的话，那可选择的模块可就多了!


###### 6.8.4 模块的迁移-实例分析














图56:大队长实例分析，将通道注意力和EMAttention迁移到时间序列预测领域
通道注意力机制…--迁移……->时间序列预测
1)给定时间序列 X∈RxD，工是时间序列的长度，D是通道数量，通过在时间维度上应用最大池化层，将其压缩为通道描述符:X。€R1xD:
X e RlxD = AvgPool(X)
(178)
然后与输入逐点相乘(加权操作)，从而强调或者抑制每一个通道特征表达:
YéRxD=X,@X
(179)























2)或者在通道维度上应用最大池化层，将其压缩为时间描述符:X:€Rx1，然后与输入逐点相乘来加强或者抑制每一个时间步特征的表达:
X,éR-AvgPool(X)Y; ∈RLxP =X X
(180]
如果你仅仅只能想到上述两种变换，那就需要再多读两遍大队长写的书了!这仅仅是将通道注意力机制的思想迁移到了时间序列预测领域，创新比较一般，为了使我们的创新更加新颖，我们将section 2.3提到的EMAttention迁移到我们的模型当中，进而与通道注意力嵌套到一起!这绝对是当前时间下最新的最最原创的模块，你直接拿去用好了。
(通道注意力机制+EMAttention)…-迁移……->时间序列预测
1)如图56所示，我们为输入 X∈R“xD设置三分支，前两个分支分别在时间维度、通道维度上执行全局平均池化操作，然后通过 Sigmoid 得到对应的通道描述符，并对输入 X 进行重新加权得到Xc∈RxP。第三个分支通过在时间维度上执行1x3Conv 来聚合局部时间窗口信息，得到输出Xs E IRLxD
2)左边分支通过聚合全局时间信息，然后对每一个时间步特征的所有通道进行加权，这意味着每个时间步拥有全局感受野，我们将左分支命名为:全局表示XcERLxD，右分支命名为:局部表示Xs E RLxD
3)我们提出了一种不同时间感受野的跨时间信息聚合方法，以实现更丰富的持征聚合。这里引入了两个张量，分别是全局表示Xc和局部表示Xs，然后，利用全局平均池化层对全局表示再次进行时间信息编码:
X;€ R'D = AvPool(Norm(Xc))(181]X1€ R!x"编码了全局时间信息，然后为其应用 Softmax 获得归一化后的通道描述符 X」€ RlxD
X:ERIDSoftmnx(X(18214)然后，将通道描述符 Xī∈ RIxD 与右分支输入 Xs€ RLxP相乘，相当于对每个时间步的所有通道特征进行加权求和，从而获得当前左分支尺度下的全局时间注意力表示:Yie Rx1
(183)
Y,é R…l= reshape(X1·Xs)
5)和上述同样的操作，利用平均池化层对右分支进行全局时间信息编码，得到Xs€RxP,并应用Softmax获得归一化后的通道描述符 X,€ R1xD，然后将其与左分支输入 Xc∈ RxP相乘，获得右分支尺度下的全局注意力表示:Y;€ R5x1:
Y;é RLx1=reshape(X;·Xc)
(184)
6)两个全局时间注意力保存了不同尺度的时间信息，最后，将两个时间注意力聚合，并通过 sigmmoid门控函数得到权重表示 W ∈ Rx1:
Y=Yi+ YaW= sigmoid(Y]
(185)
并通过权重再次校准输入 X ∈ RLxP,得到输出 Out ∈ RxD:
###### 总结
所讲的关于模块迁移的实例分析，是完全基于本书所提供的资料提出来的，我认为，只要大家好好花费一个月时间好好阅读这一本书，真的能够获得超级超级多的收获，至少，组装一个一区的idea真的不难!!大队长提供的这套资料完全有这个实力!


##### 6.9 实验分析
本章主要讲解在进行模型实验时所遇到的一些问题，包括实验性能、、一些涨点的技巧等，都是大家最常提问的问题。
###### 6.9.1 模型性能应该达到什么层次
1)图像分类、语义分割等这种广泛被人关注的CV领域，SOTA几乎是透明的，如果今天一篇新的论文发表在了arxiv上，声称实现了SOTA，那么它会被一些知乎大V、公众号等迅速的广泛的传播，引起极大的关注，因此，做CV的同学对于SOTA要格外的谨慎，稍不注意可能就会引来同行们“找茬”。
2)除了CV，其他领域是没有这个传播速度的，即使它的发展也不错。因此，在90%的领域，可能并没有真正的SOTA，因为大家并没有看过同领域的所有的最新论文，所以也就没有一个公认的SOTA。事实上，有意义的创新并不一定性能非常好，如果你提出了一个新的思想，或者一个新的切入点，让人眼前一亮，我个人认为这也是非常有意义的。
3)如果你的目标是顶会顶刊，那说明你对自己的创新有足够的自信。那我觉得，你的性能弱一个档次是完全可以接受的。那弱到什么层次呢，只要你能比得过去年的任意一篇顶会，那就是OK的。你把它所对比的方法全挪过来，你不就是SOTA吗。你都投顶会了，审稿人也不是傻子，只会盯着效果看。更重要的是对你的创新、写作、和实验的全面评估。
4)如果你的目标是一区二区，创新还不错的样子，姑且算你是高级的组合创新，如果你能比的过你的基准模型，那你完全可以挪用它所对比的baseline，再加两个能比得过的近两年的顶会顶刊，这样也是完
全OK的。
5)如果你的目标是三区四区，低级排列组合，创新一般，如果基准模型选的好，或者说组合的好，性能好也是很有可能的。那如果性能一般的话，也是要尽量对比顶会顶刊。我相信，你肯定能找到性能比你差的近两年的顶会顶刊。如果实在找不到，那就找两年前的顶会顶刊或者普通一区论文做对比。
6)总结一下，如果你的模型性能能够比得过去年的某一篇顶会或者顶刊，就能够开始着手写论文了
###### 6.9.2 对比方法是否需要复现
1)先说结论:不需要。首先，如果是深度学习领域的话，很多论文，甚至是顶会，不开源的也比比皆是，这样的论文是不用复现甚至完全不需要复现的，复现效果好的话，你不愿意，复现效果不好的话审稿人不愿意。所以，对没有代码的这一类论文来说，直接引用它在论文中报告的结果就好。
2)其次，如果有开源代码，那么是否复现就看你的需求了，如果你的性能比他要好，那直接引用它在论文中报告的结果就好。如果它的性能更好，你对比不过它，那我建议你复现一下，在同样的实验环境下，挑选一个能够比得过它的结果。当然压低baseline性能这个做法，算是一个不太道德的行为，但是在大家都广泛使用的情况下，不用的话就很吃亏。
3)复现代码也是一个不容易的事情，建议大家还是努努力，多多复现一些经典算法，无论是提升自己对论文的理解，还是后期在实验中对这些baseline做可视化，都是有很大的帮助。大家加油!
###### 6.9.3 场景不同，如何对比
如果将一个领域的方法迁移到了另一个领域中，应当如何对比，如何进行实验:
1)这属于迁移创新，个人认为，这是一种很好的创新，论文写好了，发个一区不是问题。回到正题，将新方法迁移到了一个新的场景，这个领域的审稿人很有可能是没有接触过新方法的，因此，我们在选择对比方法的时候，并不需要把基准模型搬进来，只需要选择一些能够比得过的模型即可。
2)既然是新的方法，那就需要展现它的优势，建议添加:案例研究实验，从方法的特点入手，例如:我们需要证明我们提出的方法有很强的稳定性，那我们就可以逐步减少训练数据，或者在训练数据集上随机抛弃一些数据点，在这样的设置下，如果模型的性能下降趋势很平缓，那我们就能够认定它有很强稳定性。
3)通常情况下，参数实验、消融实验是必不可少的，大家可以在这两部分多画一些图、多做一些表来增加自己的工作量。此外，可视化也能够有效的提升论文的质量，让论文更有说服力。
###### 6.9.4 怎样提升算法性能，有什么技巧
一些被普遍认为对模型性能有影响的模型设置有:Adam优化器，学习率递减策略、正则化、参数初始化、seed选择，模型超参数。
1)我们在section6.5中提到了精读代码，值得注意的是，在代码中寻找trick是一件非常重要的事情，每个领域或多或少有一些trick(提升模型性能的小模块，提点神器)，并且很多作者并不会在论文中说明，这需要你在读代码的过程中积累。
2)尽管网上提供了很多细节的设置，但我认为，不用一个一个去看，你在精读论文、精读代码的过程中是有积累的，你要在经典代码的基础上做改进，然后替换模块，所以沿用它们的基础设置即可，至少它们不会影响你模型的效果。
3)如果发现模型效果不好，建议大家对模型进行修改，可以是改进模块，或者组合模块，替换模块等等，不用费大把时间在调参上，它们对性能的提升是有限的，真不如好好对模型进行修改替换组合。

#### 第七章总结
本书到这里就结束了!大队长花了数月的时间，每天保证八小时的撰写时间，不断的反复的阅读原论文、论文代码，终于成稿了这一本《大队长手把手带你发论文》。停笔的这一刻，感慨良多，即使这本书撰写的初衷是为了赚钱，但大队长可以自信的说，每一字每一个公式每一个代码每一个注释都是大队长认认真真敲出来的，它们的价值完全超过了这本书的价格!在大队长刚刚踏上科研路上的时候，遇到过很多困难，我也曾想过:为什么学习编程的时候资料那么多，任意类型的实战项目都有人手把手教，但在科研这一块，为啥就没人带路，没人教你读论文、读代码呢?就只能一个坑一个坑踩着过吗?如今，回头看，轻舟已过万重山。但对于很多研究生来说，他们的困难可能才刚刚开始，写书的初期是为了赚钱，写书的后期是为了把这本书写好、写完美，帮助更多的研究生们。到现在为止，我也不敢相信我竟然能坚持用这么久的时间写成一本将近六万字的书，而且是纯干货，没有一句废话(请原谅我的自信，但现在我真的自信爆棚了!!)。毫不装逼的说，如果你是个小白，你用了我提供的模块，你至少能水出一篇三区的论文。如果你有了三四区的基础，在认真学习了每个模块的基础上，认真反复理解sectiton6.8，你将会获得组合高级模块的能力，发一区真的不是难事!!各个朋友们，加油吧，我相信，在《大队长手把手带你发论文》的加持下，想idea、发论文真的是一件轻而易举的事情!!!




<br>[返回章标题](#第六章研究生生涯大解救)


<br>[目录](#目录)



