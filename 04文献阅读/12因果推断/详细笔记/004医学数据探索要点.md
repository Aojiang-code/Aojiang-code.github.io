# 0.4 医学数据探索要点（EDA for Causal Inference · 扩展版）

> 目标：不是提升预测精度，而是为“**可识别、可信赖**”的因果估计做准备。
> 关注四件事：**变量角色与时间顺序 → 数据质量与缺失 → 基线平衡与重叠 → 分布与可变换性**。
> 贯穿始终：避免把**中介/碰撞变量**当成混杂，避免**时间倒置**（immortal time bias）。


---

## 0.4.1 明确变量角色与时间线（最重要）

* **ID & 时间**：是否有患者ID（`patient_id`）、住院ID（`hadm_id`/`icustay_id`）？“治疗开始时间”一定**早于/不晚于**结局观察。
* **治疗/暴露 `treatment`**：二分类或多水平？定义清晰吗（药物名、剂量/时点、手术类型）？
* **结局 `outcome`**：二分类（死亡/并发症）、连续（评分/生化值）、或生存时间（随访月数+事件指示）。
* **混杂（pre-exposure covariates）**：治疗前可得，且会同时影响“治疗选择”和“结局”的变量（年龄、合并症、严重度评分、基线指标、入院来源、日历时间/科室等）。
* **中介/碰撞**：治疗后才发生的指标是中介，不应在倾向评分中调整；共同后果/选择变量是碰撞，**不能调整**。

**提示**：先画一张简单的因果图（哪怕是草图），把“治疗→结局”与已知/可疑混杂先列出来。

---

## 0.4.2 数据质量：规模、重复、类型、取值范围、缺失

```python
# —— 适配你的 DataFrame / 列名 ——
import pandas as pd
import numpy as np

print("数据规模:", df.shape)
print("\n类型与非空：")
print(df.info())

# 重复与唯一性（若有主键列）
if 'patient_id' in df.columns:
    dup_pct = 1 - df['patient_id'].nunique() / len(df)
    print(f"\n按 patient_id 重复比例: {dup_pct:.3%}")

print("\n缺失值（Top 20）：")
print(df.isnull().sum().sort_values(ascending=False).head(20))

print("\n数值变量的基本分布：")
display(df.select_dtypes(include=[np.number]).describe(percentiles=[.05,.25,.5,.75,.95]))
```

**看什么？为什么看？**

* **类型**：日期列应转为 `datetime`（用于定义“暴露前信息”截断）；分类变量建议用 `category` 或 `int`。
* **取值范围**：年龄是否在 0–120？生理参数是否在合理区间？
* **缺失**：哪些列缺失严重？是否成块缺失（如某些病区或年份全缺）？

---

## 0.4.3 缺失机制与处理（MCAR / MAR / MNAR）

* **MCAR**：完全随机缺失（少见）
* **MAR**：条件于其它已观测变量的缺失（常见；可多重插补/建模）
* **MNAR**：与未观测值本身相关（难，需灵敏度分析）

**快速探查：谁在“决定”缺失？**（用“缺失指示”做逻辑回归）

```python
import statsmodels.api as sm

candidate = 'albumin'  # 例：想用的基线白蛋白
if candidate in df.columns:
    miss_flag = df[candidate].isna().astype(int)
    # 用其他若干基线协变量解释“是否缺失”
    covs = [c for c in ['age','sex','sofa','comorbidity_score'] if c in df.columns]
    X = df[covs].copy()
    X = pd.get_dummies(X, drop_first=True)  # 编码分类变量
    X = sm.add_constant(X, has_constant='add')
    model = sm.Logit(miss_flag, X, missing='drop').fit(disp=False)
    print(model.summary())
```

> 若多个变量显著解释“缺失与否”，多为 **MAR**，建议在后续步骤中使用**多重插补**（如 `statsmodels/imblearn` 方案或 `sklearn` 的 `IterativeImputer`），并在因果估计里进行合并/稳健性检验。
> **注意**：不要用**结局信息**去插补治疗前协变量（会泄露）。

---

## 0.4.4 基线平衡（SMD）与“Table 1”

**目的**：看“治疗组 vs 对照组”在**治疗前协变量**上的差异。
**指标**：标准化均数差（SMD）；经验阈值 |SMD| ≤ 0.1（优），≤ 0.2（可接受）。
（后续做 PSM/IPTW/DR 后，要回头再看一次 SMD，判断平衡是否改善。）

```python
import numpy as np
import matplotlib.pyplot as plt

def smd_cont(x_t, x_c):
    mu_t, mu_c = np.nanmean(x_t), np.nanmean(x_c)
    sd_pool = np.sqrt((np.nanvar(x_t, ddof=1)+np.nanvar(x_c, ddof=1))/2)
    return (mu_t - mu_c) / (sd_pool + 1e-12)

def smd_binary(x_t, x_c):
    p1, p0 = np.nanmean(x_t), np.nanmean(x_c)
    p = np.nanmean(np.r_[x_t, x_c])
    return (p1 - p0) / np.sqrt(p*(1-p) + 1e-12)

T = 'treatment'  # 你的治疗列名
covs = [c for c in ['age','sex','bmi','sofa','charlson','sbp','dbp'] if c in df.columns]
tmask = df[T] == 1
cmask = df[T] == 0

rows = []
for v in covs:
    series = df[v]
    if series.dropna().isin([0,1]).all() or str(series.dtype) in ('category','bool'):
        val = smd_binary(df.loc[tmask, v], df.loc[cmask, v])
    else:
        val = smd_cont(df.loc[tmask, v], df.loc[cmask, v])
    rows.append((v, val))

smd_tbl = pd.DataFrame(rows, columns=['variable','SMD'])
smd_tbl['absSMD'] = smd_tbl['SMD'].abs()
display(smd_tbl.sort_values('absSMD', ascending=False))

# Love plot（水平条形图）
smd_plot = smd_tbl.sort_values('absSMD', ascending=True)
plt.figure(figsize=(6, 0.4*len(smd_plot)))
plt.barh(smd_plot['variable'], smd_plot['absSMD'])
plt.axvline(0.1, linestyle='--'); plt.axvline(0.2, linestyle='--')
plt.xlabel('|SMD|'); plt.title('Baseline Imbalance (Pre-adjustment)')
plt.show()
```

> 解释：如果很多协变量的 |SMD| > 0.2，说明**基线不平衡**明显，后续必须通过匹配/加权/调整改善平衡后再讨论因果效应。

---

## 0.4.5 重叠（Positivity/Overlap）与极端倾向

**为什么**：当某些协变量组合下“几乎全是治疗组/或全是对照组”，则无法比较（不可比）。
**怎么查**：估一个**治疗前协变量**的**倾向评分（PS）**，看两组的 PS 分布是否重叠、是否存在极端值（接近 0 或 1）。

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

pre_covs = covs  # 只放治疗前协变量
X = df[pre_covs].copy()
# 简单数值化与标准化（示例）
X = pd.get_dummies(X, drop_first=True)
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
y = df[T].astype(int).values

ps_model = LogisticRegression(max_iter=300).fit(X_std, y)
ps = ps_model.predict_proba(X_std)[:,1]
df['ps'] = ps

plt.figure()
plt.hist(df.loc[tmask,'ps'], bins=30, alpha=0.6, label='Treatment')
plt.hist(df.loc[cmask,'ps'], bins=30, alpha=0.6, label='Control')
plt.xlabel('Propensity score'); plt.ylabel('Count'); plt.legend(); plt.title('PS overlap check')
plt.show()

print("PS分布：min/1%/5%/median/95%/99%/max")
print(np.percentile(ps, [0,1,5,50,95,99,100]))
```

> 经验：若 `ps<0.01` 或 `ps>0.99` 的样本较多，后续权重会极端（IPTW 不稳定），可考虑**截尾/修剪**（如去掉极端 1%–2%）、或改变建模策略/问题设定。

---

## 0.4.6 分布、异常值与可变换性（标准化/对数/分箱）

* **为什么**：极偏/长尾分布会导致模型拟合不稳、SMD 失真、权重极端。
* **做法**：对部分生理指标（如炎症指标、肌酐）可用 `log1p`；对不同量纲做标准化；异常值可温和截断（winsorize）并汇报。

```python
from scipy.stats import skew

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
sk = pd.Series({c: skew(df[c].dropna()) for c in num_cols})
print("偏度Top10：")
print(sk.abs().sort_values(ascending=False).head(10))

# 示例：对偏度大的某列做 log1p 变换（仅示范）
candidate = sk.abs().sort_values(ascending=False).index[0]
if candidate != 'ps':
    df[candidate+'_log1p'] = np.log1p(df[candidate].clip(lower=0))
```

> 注意：变换需**符合临床解释**（如对“时间/积分类”变量不随意对数化）。如需报告原量纲效应，可在因果估计后再做回译或用不变换策略并做稳健性分析。

---

## 0.4.7 相关性与共线性（VIF）

**为什么**：倾向评分模型/回归模型中，共线性会导致不稳定与过拟合。
**怎么做**：先看相关矩阵，再算 VIF（方差膨胀因子）。

```python
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor

sub = df[pre_covs].copy()
sub = pd.get_dummies(sub, drop_first=True)
sub = sub.select_dtypes(include=[np.number]).dropna()

plt.figure(figsize=(8,6))
sns.heatmap(sub.corr().abs().clip(0,1), cmap='viridis')
plt.title('Absolute Correlation (pre-treatment covariates)')
plt.show()

X_for_vif = sm.add_constant(sub, has_constant='add')
vif = pd.Series(
    [variance_inflation_factor(X_for_vif.values, i) for i in range(1, X_for_vif.shape[1])],
    index=sub.columns, name='VIF'
).sort_values(ascending=False)
print(vif.head(15))
```

> 经验：VIF>5 或 10 可能提示严重共线性；可考虑合并高度相关的指标、做主成分/降维、或在 PS 模型里做正则化（L1/L2）。

---

## 0.4.8 生存数据特有的 EDA（如果你的结局是时间-事件）

* **删失（Censoring）**：看删失比例、随访时长分布；避免“诊断当天死亡/出院当天手术”等异常数据影响。
* **竞争风险**：如死因特异结局，需要特别建模（本阶段仅识别问题，不建模）。
* **时间对齐**：治疗开始时间要与“随访起始”一致，避免 immortal time bias。

```python
# 简单分布窥视（假定有 survival_time, event 列）
if {'survival_time','event'}.issubset(df.columns):
    print("随访中位数（月/天，取决于你的单位）：", np.median(df['survival_time'].dropna()))
    print("事件率：", df['event'].mean())
    # 生存时间直方图（仅分布观察）
    plt.figure()
    plt.hist(df['survival_time'].dropna(), bins=40)
    plt.xlabel('Survival time'); plt.ylabel('Count'); plt.title('Follow-up Distribution')
    plt.show()
```

---

## 0.4.9 “因果 EDA 清单”（打印出来贴显示器边上）

1. **时间顺序**：暴露/治疗先于结局；协变量取自**治疗前窗口**。
2. **变量角色**：明确混杂 vs 中介 vs 碰撞；避免中介/碰撞进入 PS/回归调整集。
3. **缺失**：缺失机制初查；必要时设计插补方案（不泄露结局）。
4. **基线平衡**：SMD 表 + Love plot；记录不平衡项，为后续匹配/加权做靶向改进。
5. **重叠性**：PS 分布重叠充足；极端 PS 处理策略（截尾/修剪/分层）。
6. **分布与尺度**：必要的变换与标准化；异常值处理有据可依并可复现。
7. **共线性**：相关矩阵 + VIF；必要时合并特征或正则化。
8. **抽样与外推**：纳入排除标准、时间段/科室/医院差异（必要时分层分析）。
9. **生存特性**：删失与竞争风险识别；对齐“治疗起始”和“随访起点”。

---

## 0.4.10 一键模板（把常用检查封装成函数）

```python
def causal_eda_quickcheck(df, treat='treatment', outcome='outcome', pre_covs=None):
    import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression

    print("== 基本情况 ==")
    print(df.shape)
    print(df[[c for c in [treat, outcome] if c in df.columns]].head())
    print("\n缺失(Top10):")
    print(df.isnull().sum().sort_values(ascending=False).head(10))
    print("\n数值分布(节选):")
    display(df.select_dtypes(include=[np.number]).describe().T.head(10))

    if pre_covs:
        print("\n== 基线平衡（SMD） ==")
        tmask = df[treat] == 1
        cmask = df[treat] == 0
        def smd_cont(x_t, x_c):
            mu_t, mu_c = np.nanmean(x_t), np.nanmean(x_c)
            sd_pool = np.sqrt((np.nanvar(x_t, ddof=1)+np.nanvar(x_c, ddof=1))/2)
            return (mu_t - mu_c)/(sd_pool+1e-12)
        def smd_binary(x_t, x_c):
            p1, p0 = np.nanmean(x_t), np.nanmean(x_c)
            p = np.nanmean(np.r_[x_t, x_c])
            return (p1 - p0)/np.sqrt(p*(1-p) + 1e-12)
        rows=[]
        for v in pre_covs:
            s = df[v]
            if s.dropna().isin([0,1]).all() or str(s.dtype) in ('category','bool'):
                val = smd_binary(df.loc[tmask,v], df.loc[cmask,v])
            else:
                val = smd_cont(df.loc[tmask,v], df.loc[cmask,v])
            rows.append((v, val))
        smd = pd.DataFrame(rows, columns=['var','SMD'])
        smd['|SMD|']=smd['SMD'].abs()
        display(smd.sort_values('|SMD|', ascending=False).head(15))

        print("\n== 倾向评分重叠 ==")
        X = pd.get_dummies(df[pre_covs], drop_first=True)
        X = X.select_dtypes(include=[np.number]).fillna(X.median(numeric_only=True))
        Xstd = StandardScaler().fit_transform(X)
        ps = LogisticRegression(max_iter=300).fit(Xstd, df[treat].astype(int)).predict_proba(Xstd)[:,1]
        plt.figure()
        plt.hist(ps[df[treat]==1], bins=30, alpha=0.6, label='Treatment')
        plt.hist(ps[df[treat]==0], bins=30, alpha=0.6, label='Control')
        plt.xlabel('Propensity score'); plt.ylabel('Count'); plt.legend(); plt.title('PS overlap')
        plt.show()
        print("PS分位数：", np.percentile(ps, [0,1,5,50,95,99,100]))
```

---

### 小结（给初学者的三句话）

1. **先时间线，后统计学**：确认“治疗在前、结局在后”，协变量取自治疗前。
2. **先 EDA，后估计**：SMD/PS/分布/缺失/重叠都看清楚，再谈 PSM/IPTW/DR。
3. **先可比性，后结论**：如果样本缺乏可比性，再好的估计器也救不了偏倚。

---

配套代码:`001_Env_Check_and_Quickload.ipynb`

