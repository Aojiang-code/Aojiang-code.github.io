# ğŸ¬Diabetes : EDA |ğŸŒ²Random ForestğŸŒ² + HPâš™ï¸

> [ç½‘å€](https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp)

139ä¸ªæ”¯æŒ

448ä¸ªäººcope

22,461æ¬¡æµè§ˆ


## Getting Started

The aim of this analysis is to investigate a range of health-related factors and their interconnections **to classify diabetes accurately**. These factors include aspects such as **age**, **gender**, **body mass index (BMI)**, **hypertension**, **heart disease**, **smoking history**, **HbA1c level**, and **blood glucose level**. This comprehensive examination will not only provide insights into the patterns and trends in diabetes risk but will also create a solid base for further research. Specifically, research can be built on how these variables interact and influence diabetes occurrence and progression, crucial knowledge for improving patient care and outcomes in this increasingly critical area of healthcare.


###  Domain Knowledge
#### 1. Age: 
Age is an important factor in predicting diabetes risk. As individuals get older, their risk of developing diabetes increases. This is partly due to factors such as reduced physical activity, changes in hormone levels, and a higher likelihood of developing other health conditions that can contribute to diabetes.

#### 2. Gender: 
Gender can play a role in diabetes risk, although the effect may vary. For example, women with a history of gestational diabetes (diabetes during pregnancy) have a higher risk of developing type 2 diabetes later in life. Additionally, some studies have suggested that men may have a slightly higher risk of diabetes compared to women.

#### 3. Body Mass Index (BMI): 
BMI is a measure of body fat based on a person's height and weight. It is commonly used as an indicator of overall weight status and can be helpful in predicting diabetes risk. Higher BMI is associated with a greater likelihood of developing type 2 diabetes. Excess body fat, particularly around the waist, can lead to insulin resistance and impair the body's ability to regulate blood sugar levels.

#### 4. Hypertension: 
Hypertension, or high blood pressure, is a condition that often coexists with diabetes. The two conditions share common risk factors and can contribute to each other's development. Having hypertension increases the risk of developing type 2 diabetes and vice versa. Both conditions can have detrimental effects on cardiovascular health.

#### 5. Heart Disease: 
Heart disease, including conditions such as coronary artery disease and heart failure, is associated with an increased risk of diabetes. The relationship between heart disease and diabetes is bidirectional, meaning that having one condition increases the risk of developing the other. This is because they share many common risk factors, such as obesity, high blood pressure, and high cholesterol.

#### 6. Smoking History: 
Smoking is a modifiable risk factor for diabetes. Cigarette smoking has been found to increase the risk of developing type 2 diabetes. Smoking can contribute to insulin resistance and impair glucose metabolism. Quitting smoking can significantly reduce the risk of developing diabetes and its complications.

#### 7. HbA1c Level: 
HbA1c (glycated hemoglobin) is a measure of the average blood glucose level over the past 2-3 months. It provides information about long-term blood sugar control. Higher HbA1c levels indicate poorer glycemic control and are associated with an increased risk of developing diabetes and its complications.

#### 8. Blood Glucose Level: 
Blood glucose level refers to the amount of glucose (sugar) present in the blood at a given time. Elevated blood glucose levels, particularly in the fasting state or after consuming carbohydrates, can indicate impaired glucose regulation and increase the risk of developing diabetes. Regular monitoring of blood glucose levels is important in the diagnosis and management of diabetes.


> âœ”ï¸ These features, when combined and analyzed with appropriate statistical and machine learning techniques, can help in predicting an individual's risk of developing diabetes.



## 0. INTRODUCTION

### 0.1. Preface
In this analysis, we have chosen the RandomForest classifier as our model. **The RandomForest algorithm** is an ensemble learning method that operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes for classification or mean prediction of the individual trees for regression.


> Several reasons guided our choice of **Random Forest** for this task:

* 1. **Handling of Large Data**: **Random Forest** is capable of efficiently handling large datasets with high dimensionality. Our dataset, containing a substantial number of rows and several features, falls into this category.

* 2. **Robustness to Overfitting**: **Random Forest** reduces the risk of overfitting, which is a common problem with decision trees. The algorithm accomplishes this by creating a set of **decision trees** (a "forest") and making the final prediction based on the majority vote of the individual trees.

* 3. **Handling Mixed Data Types**: In our dataset, we have both numerical and categorical features. **Random Forest** handles such mixtures smoothly, which makes it an ideal choice.

* 4. **Feature Importance**: **Random Forest** provides a straightforward way to estimate feature importance. Given our aim to investigate the impact of different factors on diabetes, this characteristic is particularly useful.

* 5. **Non-linearity**: Medical data often contains complex and non-linear relationships. **Random Forest**, being a non-linear model, can capture these relationships effectively.



> âš ï¸ It's worth noting that while **Random Fores**t is a strong candidate given its mentioned advantages, the choice of model should always be considered with a grain of salt. Other models might perform better on the task, and it's generally a good practice to try several models and compare their performance. However, for the purpose of this analysis and given our dataset, **Random Forest** **is a practical and reasonable starting point**.

###  0.2. Import libraries
```python
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å¿…è¦çš„åº“
import numpy as np  # å¯¼å…¥NumPyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import pandas as pd  # å¯¼å…¥Pandasåº“ï¼Œç”¨äºæ•°æ®åˆ†æå’Œæ“ä½œ

# å¯¼å…¥å¯è§†åŒ–åº“
import matplotlib.pyplot as plt  # å¯¼å…¥Matplotlibåº“çš„pyplotæ¨¡å—ï¼Œç”¨äºç»˜åˆ¶å›¾è¡¨
import seaborn as sns  # å¯¼å…¥Seabornåº“ï¼Œç”¨äºæ•°æ®å¯è§†åŒ–

# å¯¼å…¥æ¨¡å‹ç›¸å…³çš„åº“
from sklearn.model_selection import train_test_split, GridSearchCV  # å¯¼å…¥æ¨¡å‹é€‰æ‹©ç›¸å…³çš„train_test_splitå’ŒGridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # å¯¼å…¥æ•°æ®é¢„å¤„ç†ç›¸å…³çš„StandardScalerå’ŒOneHotEncoder
from sklearn.compose import ColumnTransformer  # å¯¼å…¥ColumnTransformerï¼Œç”¨äºå¤„ç†ä¸åŒç±»å‹çš„æ•°æ®
from sklearn.ensemble import RandomForestClassifier  # å¯¼å…¥é›†æˆå­¦ä¹ ä¸­çš„éšæœºæ£®æ—åˆ†ç±»å™¨
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # å¯¼å…¥æ¨¡å‹è¯„ä¼°ç›¸å…³çš„æŒ‡æ ‡
from sklearn.pipeline import Pipeline  # å¯¼å…¥Pipelineï¼Œç”¨äºæ„å»ºé¢„å¤„ç†å’Œæ¨¡å‹çš„æµæ°´çº¿

# å¯¼å…¥é‡‡æ ·å™¨ç›¸å…³çš„åº“
from imblearn.over_sampling import SMOTE  # å¯¼å…¥SMOTEè¿‡é‡‡æ ·å™¨ï¼Œç”¨äºå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†
from imblearn.under_sampling import RandomUnderSampler  # å¯¼å…¥éšæœºä¸‹é‡‡æ ·å™¨
from imblearn.pipeline import Pipeline as imbPipeline  # å¯¼å…¥imblearnçš„Pipelineï¼Œç”¨äºæ„å»ºè¿‡é‡‡æ ·æˆ–ä¸‹é‡‡æ ·çš„æµæ°´çº¿

# è®¾ç½®æµ®ç‚¹æ•°æ˜¾ç¤ºæ ¼å¼
pd.options.display.float_format = "{:.2f}".format  # è®¾ç½®Pandasæ˜¾ç¤ºæµ®ç‚¹æ•°æ—¶ä¿ç•™ä¸¤ä½å°æ•°
```

è¿™æ®µä»£ç é¦–å…ˆå¯¼å…¥äº†Pythonä¸­ç”¨äºæ•°æ®å¤„ç†ã€å¯è§†åŒ–ã€æœºå™¨å­¦ä¹ æ¨¡å‹æ„å»ºå’Œè¯„ä¼°çš„åº“ã€‚ç„¶åï¼Œé€šè¿‡`warnings.filterwarnings('ignore')`å¿½ç•¥è­¦å‘Šä¿¡æ¯ã€‚æ¥ç€ï¼Œè®¾ç½®äº†Pandasçš„æ˜¾ç¤ºæ ¼å¼ï¼Œä½¿å¾—æµ®ç‚¹æ•°åœ¨è¾“å‡ºæ—¶åªä¿ç•™ä¸¤ä½å°æ•°ã€‚è¿™äº›å‡†å¤‡å·¥ä½œä¸ºåç»­çš„æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚

###  0.3. Input the data

```python
# ä»æŒ‡å®šè·¯å¾„è¯»å–CSVæ–‡ä»¶åˆ°DataFrame
df = pd.read_csv("/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv")
```
è¿™è¡Œä»£ç ä½¿ç”¨Pandasåº“ä¸­çš„`read_csv`å‡½æ•°æ¥è¯»å–å­˜å‚¨åœ¨Kaggleè¾“å…¥æ•°æ®é›†è·¯å¾„ä¸‹çš„CSVæ–‡ä»¶ã€‚æ–‡ä»¶è·¯å¾„`"/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv"`æŒ‡å®šäº†CSVæ–‡ä»¶çš„ä½ç½®ã€‚è¯»å–çš„æ•°æ®å°†è¢«å­˜å‚¨åˆ°ä¸€ä¸ªåä¸º`df`çš„DataFrameå¯¹è±¡ä¸­ï¼Œä»¥ä¾¿åç»­è¿›è¡Œæ•°æ®åˆ†æå’Œå¤„ç†ã€‚


```python
df.head()
```

![0.3head](01å›¾ç‰‡/0.3head.png)



## 1 | EXPLORATORY DATA ANALYSIS

### 1. Data Quality

#### I | Handling Duplicates
```python
# Handle duplicates
# å¤„ç†é‡å¤çš„è¡Œ
# é€šè¿‡è°ƒç”¨duplicated()æ–¹æ³•æ‰¾å‡ºDataFrame dfä¸­çš„é‡å¤è¡Œï¼Œå¹¶å°†è¿™äº›è¡Œå­˜å‚¨åœ¨duplicate_rows_dataå˜é‡ä¸­
duplicate_rows_data = df[df.duplicated()]
# æ‰“å°é‡å¤è¡Œçš„æ•°é‡
print("number of duplicate rows: ", duplicate_rows_data.shape)
```

è¿™æ®µä»£ç çš„ç›®çš„æ˜¯è¯†åˆ«å¹¶å¤„ç†æ•°æ®é›†ä¸­çš„é‡å¤è¡Œã€‚é¦–å…ˆï¼Œä½¿ç”¨`duplicated()`å‡½æ•°æ£€æŸ¥`df`ä¸­çš„é‡å¤è¡Œï¼Œå¹¶å°†è¿™äº›é‡å¤çš„è¡Œç­›é€‰å‡ºæ¥ï¼Œå­˜å‚¨åœ¨`duplicate_rows_data`å˜é‡ä¸­ã€‚ç„¶åï¼Œä½¿ç”¨`shape`å±æ€§æ¥è·å–`duplicate_rows_data`ä¸­çš„è¡Œæ•°ï¼Œå¹¶å°†å…¶æ‰“å°å‡ºæ¥ï¼Œä»¥ä¾¿äº†è§£æ•°æ®é›†ä¸­å­˜åœ¨å¤šå°‘é‡å¤çš„è¡Œã€‚è¿™æ˜¯æ•°æ®é¢„å¤„ç†çš„ä¸€ä¸ªé‡è¦æ­¥éª¤ï¼Œå› ä¸ºé‡å¤çš„æ•°æ®å¯èƒ½ä¼šå½±å“æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚


```python
number of duplicate rows:  (3854, 9)
```

è¿™ä¸ªç»“æœæ˜¾ç¤ºï¼Œåœ¨æ•°æ®é›†ä¸­æœ‰3854è¡Œæ˜¯é‡å¤çš„ï¼Œè€Œæ¯è¡ŒåŒ…å«9ä¸ªç‰¹å¾ï¼ˆæˆ–åˆ—ï¼‰ã€‚è¿™å¯èƒ½æ„å‘³ç€åœ¨åŸå§‹æ•°æ®é›†ä¸­ï¼Œæœ‰äº›è®°å½•è¢«ä¸å°å¿ƒè¾“å…¥äº†å¤šæ¬¡ï¼Œè¿™äº›é‡å¤çš„è®°å½•å¯èƒ½ä¼šå¯¹æ•°æ®åˆ†æå’Œæ¨¡å‹è®­ç»ƒäº§ç”Ÿä¸åˆ©å½±å“ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬éœ€è¦å†³å®šå¦‚ä½•å¤„ç†è¿™äº›é‡å¤çš„æ•°æ®ï¼Œæ¯”å¦‚åˆ é™¤é‡å¤è¡Œæˆ–è€…åˆå¹¶é‡å¤è®°å½•ä¸­çš„ä¿¡æ¯ã€‚åœ¨è¿›è¡Œè¿›ä¸€æ­¥çš„æ•°æ®åˆ†æä¹‹å‰ï¼Œå¤„ç†é‡å¤æ•°æ®æ˜¯ä¸€ä¸ªé‡è¦çš„æ•°æ®æ¸…æ´—æ­¥éª¤ã€‚

```python
# åˆ é™¤é‡å¤çš„è¡Œ
# è°ƒç”¨drop_duplicates()æ–¹æ³•ç§»é™¤dfä¸­çš„é‡å¤è¡Œï¼Œåªä¿ç•™å”¯ä¸€çš„è®°å½•
df = df.drop_duplicates()
```

è¿™æ®µä»£ç ä½¿ç”¨Pandasåº“ä¸­çš„`drop_duplicates()`æ–¹æ³•æ¥åˆ é™¤DataFrame `df`ä¸­çš„é‡å¤è¡Œã€‚è¿™ä¸ªæ–¹æ³•ä¼šæ£€æŸ¥æ•°æ®é›†ä¸­çš„æ‰€æœ‰è¡Œï¼Œå¹¶ç§»é™¤é‚£äº›ä¸å…¶ä»–è¡Œå®Œå…¨ç›¸åŒçš„è¡Œï¼Œåªä¿ç•™å”¯ä¸€çš„è®°å½•ã€‚è¿™æ ·åšå¯ä»¥å‡å°‘æ•°æ®é›†ä¸­çš„å†—ä½™ï¼Œç¡®ä¿åç»­çš„æ•°æ®åˆ†æå’Œæ¨¡å‹è®­ç»ƒæ›´åŠ å‡†ç¡®ã€‚åˆ é™¤é‡å¤è¡Œåï¼Œæ›´æ–°çš„DataFrameä»ç„¶å‘½åä¸º`df`ã€‚


#### II | Uniqueness

```python
# Loop through each column and count the number of distinct values
# éå†æ¯ä¸€åˆ—ï¼Œç»Ÿè®¡æ¯åˆ—ä¸åŒå€¼çš„æ•°é‡
# é€šè¿‡forå¾ªç¯éå†DataFrame dfçš„æ‰€æœ‰åˆ—
for column in df.columns:
    # è®¡ç®—æ¯ä¸ªåˆ—ä¸­å”¯ä¸€å€¼çš„æ•°é‡
    num_distinct_values = len(df[column].unique())
    # æ‰“å°æ¯ä¸ªåˆ—çš„åç§°ä»¥åŠå¯¹åº”çš„ä¸åŒå€¼æ•°é‡
    print(f"{column}: {num_distinct_values} distinct values")
```

è¿™æ®µä»£ç é€šè¿‡ä¸€ä¸ªforå¾ªç¯éå†DataFrame `df`ä¸­çš„æ‰€æœ‰åˆ—ã€‚å¯¹äºæ¯ä¸€åˆ—ï¼Œä½¿ç”¨`unique()`å‡½æ•°æ‰¾å‡ºåˆ—ä¸­çš„å”¯ä¸€å€¼ï¼Œå¹¶ä½¿ç”¨`len()`å‡½æ•°è®¡ç®—è¿™äº›å”¯ä¸€å€¼çš„æ•°é‡ã€‚ç„¶åï¼Œä½¿ç”¨`print`å‡½æ•°è¾“å‡ºæ¯ä¸ªåˆ—çš„åç§°å’Œå…¶å…·æœ‰çš„ä¸åŒå€¼çš„æ•°é‡ã€‚è¿™ä¸ªè¿‡ç¨‹æœ‰åŠ©äºäº†è§£æ•°æ®é›†ä¸­æ¯ä¸ªç‰¹å¾çš„å¤šæ ·æ€§ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªç‰¹å¾çš„ä¸åŒå€¼æ•°é‡éå¸¸å°‘ï¼Œå¯èƒ½è¡¨æ˜è¿™ä¸ªç‰¹å¾å¯¹äºæ•°æ®é›†çš„åŒºåˆ†èƒ½åŠ›æœ‰é™ã€‚


```python
gender: 3 distinct values
age: 102 distinct values
hypertension: 2 distinct values
heart_disease: 2 distinct values
smoking_history: 6 distinct values
bmi: 4247 distinct values
HbA1c_level: 18 distinct values
blood_glucose_level: 18 distinct values
diabetes: 2 distinct values
```

è¿™ä¸ªç»“æœæ˜¾ç¤ºäº†æ•°æ®é›†ä¸­æ¯ä¸ªç‰¹å¾ï¼ˆåˆ—ï¼‰çš„ä¸åŒå€¼æ•°é‡ã€‚å…·ä½“æ¥è¯´ï¼š

- `gender`åˆ—æœ‰3ä¸ªä¸åŒçš„å€¼ï¼Œå¯èƒ½è¡¨ç¤ºæ€§åˆ«åˆ†ç±»ï¼ˆå¦‚ç”·ã€å¥³ç­‰ï¼‰ã€‚
- `age`åˆ—æœ‰102ä¸ªä¸åŒçš„å€¼ï¼Œè¿™å¯èƒ½æ„å‘³ç€å¹´é¾„æ•°æ®æ˜¯åˆ†å¹´é¾„æ®µè®°å½•çš„ï¼Œæˆ–è€…æ˜¯è¿ç»­çš„å¹´é¾„å€¼ã€‚
- `hypertension`å’Œ`heart_disease`åˆ—å„æœ‰2ä¸ªä¸åŒçš„å€¼ï¼Œè¿™å¯èƒ½è¡¨ç¤ºè¿™ä¸¤ä¸ªç‰¹å¾æ˜¯äºŒå…ƒçš„ï¼Œå³æœ‰æˆ–æ²¡æœ‰çš„çŠ¶å†µã€‚
- `smoking_history`åˆ—æœ‰6ä¸ªä¸åŒçš„å€¼ï¼Œè¿™å¯èƒ½è¡¨ç¤ºå¸çƒŸå²çš„ä¸åŒåˆ†ç±»ï¼ˆå¦‚ä»ä¸å¸çƒŸã€ä»¥å‰å¸çƒŸã€ç°åœ¨å¸çƒŸç­‰ï¼‰ã€‚
- `bmi`åˆ—æœ‰4247ä¸ªä¸åŒçš„å€¼ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒé«˜çš„æ•°é‡ï¼Œè¡¨æ˜BMIå€¼æ˜¯è¿ç»­è®°å½•çš„ï¼Œæœ‰å¾ˆå¤šä¸åŒçš„å¯èƒ½å€¼ã€‚
- `HbA1c_level`å’Œ`blood_glucose_level`åˆ—å„æœ‰18ä¸ªä¸åŒçš„å€¼ï¼Œè¿™å¯èƒ½æ„å‘³ç€è¿™ä¸¤ä¸ªæŒ‡æ ‡æ˜¯æŒ‰ç­‰çº§æˆ–ç±»åˆ«è®°å½•çš„ã€‚
- `diabetes`åˆ—æœ‰2ä¸ªä¸åŒçš„å€¼ï¼Œè¿™é€šå¸¸è¡¨ç¤ºè¿™ä¸ªç‰¹å¾æ˜¯äºŒå…ƒçš„ï¼Œå³è¡¨ç¤ºæ˜¯å¦æœ‰ç³–å°¿ç—…ã€‚

äº†è§£æ¯ä¸ªç‰¹å¾çš„ä¸åŒå€¼æ•°é‡å¯¹äºæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹éå¸¸é‡è¦ï¼Œå¯ä»¥å¸®åŠ©å†³å®šå¦‚ä½•å¤„ç†åˆ†ç±»ç‰¹å¾ã€æ˜¯å¦éœ€è¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œåˆ†ç®±ï¼ˆbinningï¼‰æˆ–æ ‡å‡†åŒ–ï¼Œä»¥åŠå¦‚ä½•ä¸ºæ¨¡å‹è®­ç»ƒå‡†å¤‡æ•°æ®ã€‚

#### III | Missing Values

```python
# Checking null values
# æ£€æŸ¥ç©ºå€¼
# ä½¿ç”¨df.isnull()ç”Ÿæˆä¸€ä¸ªä¸dfå½¢çŠ¶ç›¸åŒçš„å¸ƒå°”DataFrameï¼Œå…¶ä¸­æ¯ä¸ªä½ç½®çš„å€¼è¡¨ç¤ºç›¸åº”ä½ç½®çš„å€¼æ˜¯å¦ä¸ºç©ºï¼ˆTrueè¡¨ç¤ºç©ºå€¼ï¼ŒFalseè¡¨ç¤ºéç©ºå€¼ï¼‰
# ç„¶åä½¿ç”¨sum()æ–¹æ³•å¯¹å¸ƒå°”DataFrameæŒ‰åˆ—æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸€åˆ—çš„ç©ºå€¼æ•°é‡
print(df.isnull().sum())
```

è¿™æ®µä»£ç çš„ç›®çš„æ˜¯æ£€æŸ¥DataFrame `df`ä¸­çš„ç©ºå€¼ï¼ˆNaNæˆ–Noneï¼‰æ•°é‡ã€‚é€šè¿‡è°ƒç”¨`isnull()`æ–¹æ³•ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ªä¸åŸDataFrameå½¢çŠ¶ç›¸åŒçš„å¸ƒå°”DataFrameï¼Œå…¶ä¸­çš„å€¼è¡¨ç¤ºåŸå§‹DataFrameå¯¹åº”ä½ç½®çš„å…ƒç´ æ˜¯å¦ä¸ºç©ºã€‚æ¥ç€ï¼Œè°ƒç”¨`sum()`æ–¹æ³•å¯¹å¸ƒå°”DataFrameæŒ‰åˆ—æ±‚å’Œï¼Œå¾—åˆ°çš„ç»“æœæ˜¯æ¯ä¸€åˆ—çš„ç©ºå€¼æ•°é‡ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£æ•°æ®é›†ä¸­å“ªäº›ç‰¹å¾åŒ…å«ç©ºå€¼ï¼Œä»¥åŠç©ºå€¼çš„æ€»æ•°ï¼Œè¿™å¯¹äºæ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†æ˜¯éå¸¸é‡è¦çš„ä¸€æ­¥ã€‚


```python
gender                 0
age                    0
hypertension           0
heart_disease          0
smoking_history        0
bmi                    0
HbA1c_level            0
blood_glucose_level    0
diabetes               0
dtype: int64
```

è¿™ä¸ªç»“æœæ˜¾ç¤ºï¼Œåœ¨DataFrame `df` ä¸­çš„æ‰€æœ‰ç‰¹å¾åˆ—ä¸­ï¼Œæ²¡æœ‰æ£€æµ‹åˆ°ç©ºå€¼ï¼ˆnull valuesï¼‰ã€‚æ¯ä¸€åˆ—çš„ç©ºå€¼æ•°é‡éƒ½æ˜¾ç¤ºä¸º0ï¼Œè¿™æ„å‘³ç€æ•°æ®é›†åœ¨è¿™äº›åˆ—ä¸­æ˜¯å®Œæ•´çš„ï¼Œæ²¡æœ‰ç¼ºå¤±çš„æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„æ•°æ®çŠ¶æ€ï¼Œå› ä¸ºç¼ºå¤±æ•°æ®å¯èƒ½ä¼šå¯¼è‡´åœ¨æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è¿›è¡Œåç»­çš„æ•°æ®å¤„ç†å’Œåˆ†æï¼Œè€Œä¸éœ€è¦é¢å¤–æ‹…å¿ƒå¤„ç†ç©ºå€¼çš„é—®é¢˜ã€‚


```python
# Remove Unneccessary value [0.00195%]
# ç§»é™¤å æ¯”è¾ƒå°çš„ç±»åˆ« [0.00195%]
# é€šè¿‡æ¡ä»¶ç­›é€‰ç§»é™¤'gender'åˆ—ä¸­å€¼ä¸º'Other'çš„è¡Œ
df = df[df['gender'] != 'Other']
```

è¿™æ®µä»£ç çš„ç›®çš„æ˜¯ç­›é€‰æ‰`gender`åˆ—ä¸­å€¼ä¸º'Other'çš„è¡Œï¼Œè¿™é€šå¸¸æ˜¯å› ä¸ºè¯¥ç±»åˆ«åœ¨æ•°æ®é›†ä¸­å æ¯”è¾ƒå°ï¼Œå¯èƒ½å¯¹åˆ†æç»“æœçš„è´¡çŒ®æœ‰é™ã€‚é€šè¿‡ä½¿ç”¨å¸ƒå°”ç´¢å¼•ï¼Œ`df[df['gender'] != 'Other']`åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„DataFrameï¼Œå…¶ä¸­ä¸åŒ…å«'gender'åˆ—ä¸º'Other'çš„è®°å½•ã€‚è¿™æ ·ï¼Œæ›´æ–°åçš„DataFrame `df`å°†ä¸å†åŒ…å«è¿™äº›è¡Œï¼Œå¯ä»¥ç”¨äºåç»­çš„æ•°æ®åˆ†æå’Œæ¨¡å‹è®­ç»ƒã€‚

#### IV | Describe the Data

```python
# å±•ç¤ºæ•°æ®é›†çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºä¿ç•™ä¸¤ä½å°æ•°
# describe()æ–¹æ³•ç”Ÿæˆæ•°æ®é›†çš„æè¿°æ€§ç»Ÿè®¡æ¦‚è§ˆï¼ŒåŒ…æ‹¬å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€å››åˆ†ä½æ•°å’Œæœ€å¤§å€¼
# style.format("{:.2f}")å°†è¿™äº›ç»Ÿè®¡å€¼æ ¼å¼åŒ–ä¸ºä¿ç•™ä¸¤ä½å°æ•°çš„æ ¼å¼
df.describe().style.format("{:.2f}")
```

è¿™æ®µä»£ç ä½¿ç”¨Pandasåº“ä¸­çš„`describe()`æ–¹æ³•æ¥è·å–DataFrame `df`çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬å„æ•°å€¼å‹åˆ—çš„å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€ç¬¬ä¸€å››åˆ†ä½æ•°ã€ç¬¬ä¸‰å››åˆ†ä½æ•°å’Œæœ€å¤§å€¼ã€‚ç„¶åï¼Œé€šè¿‡`style.format("{:.2f}")`å°†è¿™äº›ç»Ÿè®¡å€¼æ ¼å¼åŒ–ä¸ºä¿ç•™ä¸¤ä½å°æ•°çš„æ ¼å¼ï¼Œä½¿å¾—è¾“å‡ºç»“æœæ›´åŠ æ•´æ´å’Œæ˜“äºé˜…è¯»ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬å¿«é€Ÿäº†è§£æ•°æ®é›†çš„åˆ†å¸ƒæƒ…å†µå’Œæ•°å€¼èŒƒå›´ã€‚

![1.4describe](01å›¾ç‰‡/1.4describe.png)

è¿™ä¸ªè¡¨æ ¼æ˜¾ç¤ºäº†æ•°æ®é›†ä¸­å„ä¸ªæ•°å€¼å‹ç‰¹å¾çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼š

- `age`åˆ—çš„å¹³å‡å¹´é¾„æ˜¯41.80å²ï¼Œæ ‡å‡†å·®ä¸º22.46å²ï¼Œè¡¨æ˜å¹´é¾„åˆ†å¸ƒæœ‰ä¸€å®šçš„æ³¢åŠ¨ã€‚å¹´é¾„çš„æœ€å°å€¼æ˜¯0.08å²ï¼ˆå¯èƒ½æ˜¯æ•°æ®é”™è¯¯ï¼‰ï¼Œ25%åˆ†ä½æ•°æ˜¯24å²ï¼Œä¸­ä½æ•°æ˜¯43å²ï¼Œ75%åˆ†ä½æ•°æ˜¯59å²ï¼Œæœ€å¤§å¹´é¾„æ˜¯80å²ã€‚
- `hypertension`åˆ—~~çš„å¹³å‡å€¼æ˜¯0.08ï¼Œæ ‡å‡†å·®ä¸º0.27ï¼Œè¿™è¡¨æ˜å¤§å¤šæ•°äººæ²¡æœ‰é«˜è¡€å‹ï¼ˆå¯èƒ½0è¡¨ç¤ºæ²¡æœ‰ï¼Œ1è¡¨ç¤ºæœ‰ï¼‰ã€‚25%åˆ†ä½æ•°å’Œä¸­ä½æ•°éƒ½æ˜¯0ï¼Œè¯´æ˜å¤§éƒ¨åˆ†æ•°æ®é›†ä¸­åœ¨æ²¡æœ‰é«˜è¡€å‹çš„çŠ¶æ€ã€‚~~
- `heart_disease`åˆ—~~çš„å¹³å‡å€¼æ˜¯0.04ï¼Œæ ‡å‡†å·®ä¸º0.20ï¼ŒåŒæ ·åœ°ï¼Œè¿™è¡¨æ˜å¤§å¤šæ•°äººæ²¡æœ‰å¿ƒè„ç—…ã€‚ç”±äºå¹³å‡å€¼è¾ƒä½ï¼Œå¯ä»¥æ¨æ–­å¿ƒè„ç—…åœ¨è¿™ä¸ªç¾¤ä½“ä¸­ä¸æ˜¯å¾ˆå¸¸è§ã€‚~~
- `bmi`åˆ—çš„å¹³å‡å€¼æ˜¯27.32ï¼Œæ ‡å‡†å·®ä¸º6.77ï¼Œè¿™å¯èƒ½è¡¨æ˜å¤§å¤šæ•°äººçš„ä½“é‡æŒ‡æ•°åœ¨æ­£å¸¸èŒƒå›´å†…ï¼ˆ18.5-24.9ï¼‰ï¼Œä½†æœ‰ä¸€å®šæ¯”ä¾‹çš„äººå¯èƒ½è¶…é‡æˆ–è‚¥èƒ–ã€‚
- `HbA1c_level`åˆ—çš„å¹³å‡å€¼æ˜¯5.53%ï¼Œæ ‡å‡†å·®ä¸º1.07%ï¼Œè¿™ä¸ªå€¼åæ˜ äº†è¡€ç³–æ§åˆ¶çš„å¹³å‡æ°´å¹³ã€‚HbA1cæ°´å¹³æ˜¯ç³–å°¿ç—…è¯Šæ–­å’Œç®¡ç†çš„é‡è¦æŒ‡æ ‡ã€‚
- `blood_glucose_level`åˆ—çš„å¹³å‡å€¼æ˜¯138.22 mg/dLï¼Œæ ‡å‡†å·®ä¸º40.91 mg/dLï¼Œè¿™ä¸ªå€¼æ˜¯ç©ºè…¹è¡€ç³–æ°´å¹³ï¼Œåæ˜ äº†æ•´ä½“çš„è¡€ç³–æ§åˆ¶æƒ…å†µã€‚
- `diabetes`åˆ—~~çš„å¹³å‡å€¼æ˜¯0.09ï¼Œæ ‡å‡†å·®ä¸º0.28ï¼Œè¿™è¡¨æ˜åœ¨æ•°æ®é›†ä¸­ï¼Œæœ‰ä¸€å®šæ¯”ä¾‹çš„äººæ‚£æœ‰ç³–å°¿ç—…ï¼ˆå¯èƒ½0è¡¨ç¤ºæ²¡æœ‰ç³–å°¿ç—…ï¼Œ1è¡¨ç¤ºæœ‰ç³–å°¿ç—…ï¼‰ã€‚~~

è¿™äº›ç»Ÿè®¡ä¿¡æ¯ä¸ºæˆ‘ä»¬æä¾›äº†æ•°æ®é›†çš„æ¦‚è§ˆï¼Œæœ‰åŠ©äºæˆ‘ä»¬ç†è§£æ•°æ®çš„åˆ†å¸ƒå’Œæ½œåœ¨çš„é—®é¢˜ï¼Œä¾‹å¦‚å¼‚å¸¸å€¼æˆ–æ•°æ®çš„åæ–œæƒ…å†µã€‚è¿™å¯¹äºåç»­çš„æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚


### 2. Univariate Analysis
#### I | Histogram for age
```python
# Histogram for age
# ä¸ºå¹´é¾„åˆ—ç»˜åˆ¶ç›´æ–¹å›¾
# ä½¿ç”¨plt.hist()ç»˜åˆ¶'age'åˆ—çš„ç›´æ–¹å›¾ï¼Œbinså‚æ•°è®¾ç½®ä¸º30ï¼Œè¡¨ç¤ºå°†å¹´é¾„åˆ†å¸ƒåˆ’åˆ†ä¸º30ä¸ªåŒºé—´
plt.hist(df['age'], bins=30, edgecolor='black')  # ç›´æ–¹å›¾çš„è¾¹ç¼˜é¢œè‰²è®¾ç½®ä¸ºé»‘è‰²

# è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾
plt.title('Age Distribution')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸º'Age Distribution'
plt.xlabel('Age')  # è®¾ç½®xè½´æ ‡ç­¾ä¸º'Age'
plt.ylabel('Count')  # è®¾ç½®yè½´æ ‡ç­¾ä¸º'Count'

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Matplotlibåº“ä¸­çš„`hist`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­'age'åˆ—çš„ç›´æ–¹å›¾ã€‚ç›´æ–¹å›¾æ˜¯ä¸€ç§ç»Ÿè®¡æŠ¥å‘Šå›¾ï¼Œç”¨äºå±•ç¤ºæ•°æ®åˆ†å¸ƒçš„æƒ…å†µã€‚`bins`å‚æ•°å®šä¹‰äº†ç›´æ–¹å›¾çš„åŒºé—´æ•°é‡ï¼Œè¿™é‡Œè®¾ç½®ä¸º30ï¼Œæ„å‘³ç€å¹´é¾„åˆ†å¸ƒå°†è¢«åˆ’åˆ†ä¸º30ä¸ªåŒºé—´ã€‚`edgecolor`å‚æ•°è®¾ç½®äº†ç›´æ–¹å›¾æ¯ä¸ªæŸ±çŠ¶åŒºé—´è¾¹ç¼˜çš„é¢œè‰²ã€‚æ¥ç€ï¼Œè®¾ç½®äº†å›¾è¡¨çš„æ ‡é¢˜å’Œxè½´ã€yè½´çš„æ ‡ç­¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°è§£é‡Šå›¾è¡¨å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†ç»˜åˆ¶çš„ç›´æ–¹å›¾æ˜¾ç¤ºå‡ºæ¥ã€‚è¿™æ ·çš„ç›´æ–¹å›¾æœ‰åŠ©äºæˆ‘ä»¬ç›´è§‚åœ°ç†è§£å¹´é¾„æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚

![2.1.1Histogram for age](<01å›¾ç‰‡/2.1.1Histogram for age.png>)

#### II | Bar plot for gender

```python
# ä¸ºæ€§åˆ«åˆ—ç»˜åˆ¶æ¡å½¢å›¾
# ä½¿ç”¨Seabornåº“çš„countplotå‡½æ•°æ ¹æ®'gender'åˆ—çš„å€¼è®¡æ•°ï¼Œç”Ÿæˆæ¡å½¢å›¾
sns.countplot(x='gender', data=df)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜
plt.title('Gender Distribution')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸º'Gender Distribution'

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Seabornåº“ä¸­çš„`countplot`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­'gender'åˆ—çš„æ¡å½¢å›¾ï¼Œè¯¥å›¾å±•ç¤ºäº†ä¸åŒæ€§åˆ«çš„è®¡æ•°ã€‚`countplot`æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ¡å½¢å›¾ï¼Œç”¨äºæ˜¾ç¤ºåˆ†ç±»å˜é‡çš„è®¡æ•°ã€‚åœ¨è¿™é‡Œï¼Œå®ƒå°†æ•°æ®é›†ä¸­çš„æ€§åˆ«åˆ†å¸ƒå¯è§†åŒ–ï¼Œä¸åŒçš„æ€§åˆ«ç±»åˆ«å°†åœ¨xè½´ä¸Šå±•ç¤ºï¼Œå¯¹åº”çš„è®¡æ•°ï¼ˆå³é¢‘æ•°ï¼‰å°†åœ¨yè½´ä¸Šå±•ç¤ºã€‚è®¾ç½®äº†å›¾è¡¨çš„æ ‡é¢˜ä»¥ä¾¿æ›´å¥½åœ°è§£é‡Šå›¾è¡¨å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†ç»˜åˆ¶çš„æ¡å½¢å›¾æ˜¾ç¤ºå‡ºæ¥ï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬ç›´è§‚åœ°ç†è§£æ•°æ®é›†ä¸­æ€§åˆ«çš„åˆ†å¸ƒæƒ…å†µã€‚


![2.2.1Bar plot for gender](<01å›¾ç‰‡/2.2.1Bar plot for gender.png>)


#### III | Distribution plot for BMI

```python
# Distribution plot for BMI
# ä¸ºBMIåˆ—ç»˜åˆ¶åˆ†å¸ƒå›¾
# ä½¿ç”¨Seabornåº“çš„distplotå‡½æ•°æ ¹æ®'bmi'åˆ—çš„å€¼ç»˜åˆ¶åˆ†å¸ƒå›¾ï¼Œbinså‚æ•°è®¾ç½®ä¸º30ï¼Œè¡¨ç¤ºå°†æ•°æ®åˆ’åˆ†ä¸º30ä¸ªåŒºé—´
sns.distplot(df['bmi'], bins=30)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜
plt.title('BMI Distribution')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸º'BMI Distribution'

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Seabornåº“ä¸­çš„`distplot`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­'bmi'åˆ—çš„åˆ†å¸ƒå›¾ã€‚åˆ†å¸ƒå›¾æ˜¯ä¸€ç§ç”¨äºå±•ç¤ºè¿ç»­å˜é‡åˆ†å¸ƒæƒ…å†µçš„å›¾è¡¨ï¼Œå®ƒå¯ä»¥æ˜¾ç¤ºæ•°æ®çš„ç›´æ–¹å›¾å’Œæ ¸å¯†åº¦ä¼°è®¡æ›²çº¿ã€‚åœ¨è¿™é‡Œï¼Œ`bins`å‚æ•°å®šä¹‰äº†ç›´æ–¹å›¾çš„åŒºé—´æ•°é‡ï¼Œè®¾ç½®ä¸º30æ„å‘³ç€æ•°æ®å°†è¢«åˆ’åˆ†ä¸º30ä¸ªåŒºé—´ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ›´ç›´è§‚åœ°äº†è§£BMIå€¼çš„åˆ†å¸ƒæƒ…å†µã€‚è®¾ç½®äº†å›¾è¡¨çš„æ ‡é¢˜ä»¥ä¾¿æ›´å¥½åœ°è§£é‡Šå›¾è¡¨å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†ç»˜åˆ¶çš„åˆ†å¸ƒå›¾æ˜¾ç¤ºå‡ºæ¥ã€‚


![2.3.1Distribution plot for BMI](<01å›¾ç‰‡/2.3.1Distribution plot for BMI.png>)


#### IV | Count plots for binary variables

```python
# Count plots for binary variables
# ä¸ºäºŒå…ƒå˜é‡ç»˜åˆ¶è®¡æ•°å›¾
# é€šè¿‡forå¾ªç¯éå†'hypertension', 'heart_disease', 'diabetes'è¿™ä¸‰ä¸ªäºŒå…ƒå˜é‡
for col in ['hypertension', 'heart_disease', 'diabetes']:
    # ä¸ºå½“å‰éå†çš„åˆ—ç»˜åˆ¶è®¡æ•°å›¾
    sns.countplot(x=col, data=df)  # ä½¿ç”¨Seabornåº“çš„countplotå‡½æ•°æ ¹æ®åˆ—'col'çš„å€¼è®¡æ•°ï¼Œç”Ÿæˆæ¡å½¢å›¾
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸ºå½“å‰åˆ—åçš„åˆ†å¸ƒ
    plt.title(f'{col} Distribution')  # ä½¿ç”¨f-stringæ ¼å¼åŒ–å­—ç¬¦ä¸²è®¾ç½®å›¾è¡¨æ ‡é¢˜
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç é€šè¿‡ä¸€ä¸ªforå¾ªç¯ï¼Œåˆ†åˆ«ä¸º'hypertension'ï¼ˆé«˜è¡€å‹ï¼‰ã€'heart_disease'ï¼ˆå¿ƒè„ç—…ï¼‰å’Œ'diabetes'ï¼ˆç³–å°¿ç—…ï¼‰è¿™ä¸‰ä¸ªäºŒå…ƒå˜é‡ç»˜åˆ¶è®¡æ•°å›¾ã€‚å¯¹äºæ¯ä¸ªå˜é‡ï¼Œä½¿ç”¨Seabornåº“çš„`countplot`å‡½æ•°æ ¹æ®æ•°æ®é›†ä¸­'col'åˆ—çš„å€¼è®¡æ•°ï¼Œå¹¶ç”Ÿæˆæ¡å½¢å›¾ã€‚æ¯ä¸ªæ¡å½¢å›¾çš„æ ‡é¢˜è®¾ç½®ä¸ºå¯¹åº”å˜é‡çš„åˆ†å¸ƒï¼Œä½¿ç”¨f-stringæ ¼å¼åŒ–å­—ç¬¦ä¸²æ¥åŠ¨æ€åˆ›å»ºæ ‡é¢˜ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†æ¯ä¸ªå˜é‡çš„è®¡æ•°å›¾æ˜¾ç¤ºå‡ºæ¥ï¼Œä»è€Œç›´è§‚åœ°å±•ç¤ºæ¯ä¸ªäºŒå…ƒå˜é‡çš„åˆ†å¸ƒæƒ…å†µã€‚



![2.4.1hypertension.png](01å›¾ç‰‡/2.4.1hypertension.png)


![2.4.2heart_disease](01å›¾ç‰‡/2.4.2heart_disease.png)

![2.4.3diabetes](01å›¾ç‰‡/2.4.3diabetes.png)


#### V | Count plot for smoking history

```python
# Count plot for smoking history
# ä¸ºå¸çƒŸå²åˆ—ç»˜åˆ¶è®¡æ•°å›¾
# ä½¿ç”¨Seabornåº“çš„countplotå‡½æ•°æ ¹æ®'smoking_history'åˆ—çš„å€¼è®¡æ•°ï¼Œç”Ÿæˆæ¡å½¢å›¾
sns.countplot(x='smoking_history', data=df)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜
plt.title('Smoking History Distribution')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸º'Smoking History Distribution'

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Seabornåº“ä¸­çš„`countplot`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­'smoking_history'åˆ—çš„è®¡æ•°å›¾ã€‚è¯¥å›¾è¡¨å±•ç¤ºäº†ä¸åŒå¸çƒŸå²åˆ†ç±»çš„è®¡æ•°ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£æ•°æ®é›†ä¸­å¸çƒŸå²çš„åˆ†å¸ƒæƒ…å†µã€‚å›¾è¡¨çš„æ ‡é¢˜è¢«è®¾ç½®ä¸º'Smoking History Distribution'ï¼Œä»¥ä¾¿æ¸…æ¥šåœ°è¯´æ˜å›¾è¡¨çš„å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†è®¡æ•°å›¾æ˜¾ç¤ºå‡ºæ¥ã€‚

![2.5Count plot for smoking history](<01å›¾ç‰‡/2.5Count plot for smoking history.png>)


### 3.Bivariative Analysis
#### I | Boxplot BMI vs Diabetes classification

```python
# Boxplot BMI vs Diabetes classification
# ç»˜åˆ¶BMIä¸ç³–å°¿ç—…åˆ†ç±»çš„ç®±å½¢å›¾
# ä½¿ç”¨Seabornåº“çš„boxplotå‡½æ•°æ ¹æ®'diabetes'åˆ—çš„å€¼åœ¨xè½´è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä»¥'bmi'åˆ—çš„å€¼ä¸ºyè½´æ•°æ®ï¼Œç»˜åˆ¶ç®±å½¢å›¾
sns.boxplot(x='diabetes', y='bmi', data=df)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜
plt.title('BMI vs Diabetes')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸º'BMI vs Diabetes'

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Seabornåº“ä¸­çš„`boxplot`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­'bmi'åˆ—ä¸'diabetes'åˆ—ä¹‹é—´çš„å…³ç³»ã€‚ç®±å½¢å›¾æ˜¯ä¸€ç§æ ‡å‡†åŒ–çš„æ˜¾ç¤ºæ•°å€¼æ•°æ®åˆ†å¸ƒçš„å›¾è¡¨ï¼Œå¯ä»¥æ¯”è¾ƒä¸åŒç‰¹å¾çš„ä¸­ä½æ•°ã€å››åˆ†ä½æ•°å’Œæå€¼ã€‚åœ¨è¿™é‡Œï¼Œæ•°æ®æ ¹æ®'diabetes'åˆ—çš„å€¼ï¼ˆå¯èƒ½æ˜¯0å’Œ1ï¼Œä»£è¡¨æ— ç³–å°¿ç—…å’Œæœ‰ç³–å°¿ç—…ï¼‰åœ¨xè½´ä¸Šè¿›è¡Œåˆ†ç±»ï¼Œè€Œ'bmi'åˆ—çš„å€¼ä½œä¸ºyè½´ä¸Šçš„æ•°æ®ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬è§‚å¯Ÿä¸åŒç³–å°¿ç—…çŠ¶æ€çš„äººç¾¤åœ¨BMIä¸Šçš„åˆ†å¸ƒå·®å¼‚ã€‚è®¾ç½®äº†å›¾è¡¨çš„æ ‡é¢˜ä»¥ä¾¿æ›´å¥½åœ°è§£é‡Šå›¾è¡¨å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†ç®±å½¢å›¾æ˜¾ç¤ºå‡ºæ¥ã€‚

![3.1Boxplot BMI vs Diabetes classificatio](<01å›¾ç‰‡/3.1Boxplot BMI vs Diabetes classification.png>)

#### II | Boxplot Age vs Diabetes classification
```python
# Boxplot Age vs Diabetes classification
sns.boxplot(x='diabetes', y='age', data=df)
plt.title('Age vs Diabetes')
plt.show()
```

![3.2Boxplot Age vs Diabetes classification](<01å›¾ç‰‡/3.2Boxplot Age vs Diabetes classification.png>)

#### III | Count plot of gender vs diabetes
```python
# Count plot of gender vs diabetes
sns.countplot(x='gender', hue='diabetes', data=df)
plt.title('Gender vs Diabetes')
plt.show()
```

![3.3Count plot of gender vs diabetes](<01å›¾ç‰‡/3.3Count plot of gender vs diabetes.png>)

#### III | Boxplot HbA1c level vs Diabetes classification
```python
# Boxplot HbA1c level vs Diabetes classification
sns.boxplot(x='diabetes', y='HbA1c_level', data=df)
plt.title('HbA1c level vs Diabetes')
plt.show()
```

![3.3Boxplot HbA1c level vs Diabetes classification](<01å›¾ç‰‡/3.3Boxplot HbA1c level vs Diabetes classification.png>)

#### IV | Boxplot blood glucose level vs Diabetes classification
```python
# Boxplot blood glucose level vs Diabetes classification
sns.boxplot(x='diabetes', y='blood_glucose_level', data=df)
plt.title('Blood Glucose Level vs Diabetes')
plt.show()
```


![3.4Boxplot blood glucose level vs Diabetes classification](<01å›¾ç‰‡/3.4Boxplot blood glucose level vs Diabetes classification.png>)


#### V | Pair plot for numeric features

```python
# Pair plot for numeric features
# ä¸ºæ•°å€¼å‹ç‰¹å¾ç»˜åˆ¶æˆå¯¹å…³ç³»å›¾
# ä½¿ç”¨Seabornåº“çš„pairplotå‡½æ•°æ ¹æ®æ•°æ®é›†ä¸­çš„æ•°å€¼å‹ç‰¹å¾ç»˜åˆ¶æˆå¯¹å…³ç³»å›¾ï¼Œå¹¶é€šè¿‡'diabetes'åˆ—çš„å€¼è¿›è¡Œç€è‰²
sns.pairplot(df, hue='diabetes')

# æ˜¾ç¤ºå›¾è¡¨
plt.show()  # è°ƒç”¨plt.show()åœ¨å±å¹•ä¸Šå±•ç¤ºå›¾è¡¨
```

è¿™æ®µä»£ç ä½¿ç”¨Seabornåº“ä¸­çš„`pairplot`å‡½æ•°æ¥ç»˜åˆ¶æ•°æ®é›†ä¸­æ•°å€¼å‹ç‰¹å¾çš„æˆå¯¹å…³ç³»å›¾ã€‚æˆå¯¹å…³ç³»å›¾æ˜¯ä¸€ç§å¯è§†åŒ–å¤šä¸ªå˜é‡ä¹‹é—´å…³ç³»çš„å·¥å…·ï¼Œå®ƒå±•ç¤ºäº†æ•°æ®é›†ä¸­æ¯ä¸€å¯¹æ•°å€¼å‹ç‰¹å¾çš„æ•£ç‚¹å›¾ï¼ŒåŒæ—¶è¿˜åŒ…æ‹¬äº†ç›´æ–¹å›¾ã€‚åœ¨è¿™é‡Œï¼Œé€šè¿‡`hue='diabetes'`å‚æ•°ï¼Œæ•£ç‚¹å›¾æ ¹æ®'diabetes'åˆ—çš„å€¼ï¼ˆå¯èƒ½æ˜¯0å’Œ1ï¼Œä»£è¡¨æ— ç³–å°¿ç—…å’Œæœ‰ç³–å°¿ç—…ï¼‰è¿›è¡Œç€è‰²ï¼Œä»è€Œå¯ä»¥è§‚å¯Ÿä¸åŒç³–å°¿ç—…çŠ¶æ€å¯¹ç‰¹å¾åˆ†å¸ƒçš„å½±å“ã€‚æœ€åï¼Œä½¿ç”¨`plt.show()`å‡½æ•°å°†æˆå¯¹å…³ç³»å›¾æ˜¾ç¤ºå‡ºæ¥ã€‚

![3.5Pair plot for numeric features](<01å›¾ç‰‡/3.5Pair plot for numeric features.png>)


### 4. Multivariate analysis
#### I | Scatterplot Age vs BMI colored by Diabetes classification










#### II | Violin plot of BMI against diabetes classification split by gender








#### III | Interaction between gender, BMI and diabetes








#### IV | Interaction between gender, Age and diabetes





## 2 | CORRELATION
### I | Data preparation









### II | Encoding









### III | Correlation Matrix









## 3 | PREDICTIVE ANALYSIS
### 1. Class Imbalance
From the EDA ,the dataset is imbalanced (with 9% positive cases for diabetes and 91% negative cases), it's essential to balance the data to ensure that the model doesn't get biased towards the majority class. For this purpose, the Synthetic Minority Over-sampling Technique (SMOTE) is used, which generates synthetic samples for the minority class.










### 2.Preprocessing : Scaler
Preprocessing is a crucial step before training the model. In this case, numerical features are standardized (mean removed and scaled to unit variance), and categorical features are one-hot encoded. Standardization is not required for all models but is generally a good practice. One-hot encoding is necessary for categorical variables to be correctly understood by the machine learning model.

The StandardScaler in sklearn is based on the assumption that the data, Y, follows a distribution that might not necessarily be Gaussian (normal), but we still transform it in a way that its distribution will have a mean value 0 and standard deviation of 1.</p>

In other words, given a feature vector x, it modifies the values as follows:

$ \[ Y_i = \frac{x_i - \mu(\vec{x})}{\sigma(\vec{x})} \] $

$$ Y_i = \frac{x_i - \mu(\vec{x})}{\sigma(\vec{x})} $$


where:

\( x_i \) is the i-th element of the original feature vector \( \vec{x} \),
\( \mu(\vec{x}) \) is the mean of the feature vector, and
\( \sigma(\vec{x}) \) is the standard deviation of the feature vector.
The transformed data \( Y \) (each \( Y_i \)) will have properties such that \( mean(Y) = 0 \) and \( std(Y) = 1 \).

This transformation is also known as Z-score normalization.








### 3. Model Building and Hyperparameter Tuning
A pipeline is constructed which first applies the preprocessing steps and then trains a model on the data. We use a RandomForestClassifier, which is a popular and powerful algorithm for classification tasks. The model's hyperparameters are tuned using GridSearchCV , which performs an exhaustive search over the specified parameter values for the estimator. The best performing model is selected based on cross-validation.















####  Intepret the results 
The result shows the best parameters for our Random Forest model that were found during the hyperparameter tuning process:

max_depth of 10: This indicates that the maximum depth of the trees in the forest is 10 levels. Constraining the depth of the tree helps in reducing overfitting. It appears from this result that a medium-complexity tree works best for our data. Too much complexity (a deeper tree) may capture noise, and too little (a shallower tree) may not capture the underlying structure of the data.

min_samples_leaf of 2: This means that each leaf (the end node of a decision tree, where predictions are made) must contain at least two samples. This parameter, like max_depth, is a way to control overfitting. By requiring at least two samples to make a prediction, the model prevents fitting to outliers or noise in the training data.

min_samples_split of 2: This tells us that a node must contain at least two samples in order to be split (to create two child nodes). Similar to the min_samples_leaf parameter, this can help control overfitting.

n_estimators of 50: This is the number of decision trees in the forest. The Random Forest algorithm works by averaging the predictions of many decision trees to make a final prediction, which helps reduce overfitting and variance. In this case, it seems that having 50 trees in the forest gives us the best performance.



> ğŸ’¬ These parameters are a result of the Hyperparameter tuning process , and they give us insight into the structure of the data and the complexity of the model that best captures that structure. The moderately constrained tree depth and the requirements for the number of samples at each node suggest a model that is complex enough to capture the important patterns in the data, but not so complex that it overfits to noise or outliers.This balance is crucial in creating a model that will generalize well to new data.













> Remember, these are the optimal parameters given the parameter grid we defined and the specific dataset at hand. For a different dataset or with a different parameter grid, the optimal parameters could be different.







### 4. Confusion Matrix
The trained model is evaluated on the test set. Confusion matrix is used to visualize the performance of the model. It shows the true positive, true negative, false positive, and false negative predictions of the model.



Precision:Â¶
Precision is a measure of how many of the true positive predictions were actually correct. It is defined as the number of true positives (TP) divided by the sum of true positives (TP) and false positives (FP).

\[ Precision = \frac{TP}{TP + FP} \]

Recall:
Recall (or Sensitivity) is a measure of how many of the actual positive cases were identified correctly. It is defined as the number of true positives (TP) divided by the sum of true positives (TP) and false negatives (FN).



\[ Recall = \frac{TP}{TP + FN} \]

F1-Score:
The F1 score is the harmonic mean of Precision and Recall and tries to find the balance between precision and recall. It is defined as 2 times the product of precision and recall divided by the sum of precision and recall.



\[ F1 Score = \frac{2 * Precision * Recall}{Precision + Recall} \]



> | In all of these formulas: True Positives (TP) are the cases in which we predicted yes (diabetes present), and the actual was also yes. True Negatives (TN) are the cases in which we predicted no, and the actual was also no. False Positives (FP) are the cases in which we predicted yes, but the actual was no. False Negatives (FN) are the cases in which we predicted no, but the actual was yes.


#### | Intepret the results 
Our trained Random Forest Model achieved an accuracy of around 95%. This indicates that the model correctly classified around 95% of all cases in the test set.

Looking deeper into the classification metrics, let's dissect the performance for each class (0 and 1) separately:

A | Class 0 (Non-diabetes): 
The model has a high precision (0.98) for class 0, meaning that among all instances where the model predicted non-diabetes, 98% were indeed non-diabetes.
The recall for class 0 is also high (0.96). This means that our model correctly identified 96% of all actual non-diabetes cases in the dataset.
B | Class 1 (Diabetes): 
The precision for class 1 is lower around (0.65), which indicates that when the model predicted diabetes, it was correct around 65% of the time.
However, the recall is reasonably high around (0.80). This means that our model was able to capture around 80% of all actual diabetes cases.
The F1 score, a harmonic mean of precision and recall, is around 0.97 for class 0 and around 0.72 for class 1. The weighted average F1 score is around 0.94, in line with the overall accuracy.

This discrepancy in performance between classes is likely due to the imbalance in the original dataset. Class 0 (Non-diabetes) is the majority class and has more examples for the model to learn from.


> However, the higher recall for class 1 (Diabetes) is promising. This is an essential aspect for a healthcare model, as missing actual positive cases (false negatives) can have serious implications.


> ğŸ“ In summary, while our model performs well overall, it particularly excels with the majority class (non-diabetes). To enhance performance on the minority class (diabetes), we can further address class imbalance or adjust model parameters. Despite these areas for improvement, the model's ability to accurately identify a high percentage of actual diabetes cases is encouraging at this early stage of model development. Subsequent iterations and refinements are expected to enhance precision in diabetes predictions without compromising recall.


### 5. Feature Importance
Finally, the importance of each feature is computed. This is the total decrease in node impurity (weighted by the probability of reaching that node, which is approximated by the proportion of samples reaching that node) averaged over all trees of the ensemble. The feature importance gives insight into which features are most useful for making predictions. The features are ranked by their importance and visualized using a bar plot.












###  Intepret the results 
The feature importance results provide insight into which features are most influential in predicting diabetes using our Random Forest Model. The importance of a feature is calculated based on how much the tree nodes that use that feature reduce impurity across all trees in the forest.

Here are the key findings from the feature importance results:
HbA1c_level is the most important feature with an importance of 0.44. HbA1c is a measure of the average levels of blood glucose over the past 2 to 3 months, so it's not surprising that it's a significant predictor of diabetes.

The blood_glucose_level the second most important feature with an importance of 0.32. This aligns with medical knowledge, as blood glucose levels are directly used to diagnose diabetes.

Age the third most important feature with an importance of 0.14. It's well known that the risk of type 2 diabetes increases as you get older.

BMI comes fourth in terms of importance at 0.06. Body Mass Index is a key risk factor for diabetes, and its role is well documented in medical literature.

Other features like hypertension and heart_disease show some importance (0.02 and 0.01, respectively), indicating that these health conditions might have some relevance in predicting diabetes, though not as significant as the top four factors.

Smoking history ('smoking_history_non-smoker', 'smoking_history_past_smoker', 'smoking_history_current') and gender ('gender_Female', 'gender_Male') are shown to have minimal or zero importance in our model. This could be due to a number of reasons including that these factors may not be as influential in the development of diabetes or it could be a result of how the data was collected or structured.


> âš ï¸ These results, however, should be interpreted with caution. The importance of a feature in a Random Forest model doesn't necessarily mean a casual relationship, and it is specific to this model and this dataset. Other models might find different results. Additionally, low importance doesn't mean that the feature is unimportant for predicting diabetes in general, it may just mean that the feature is not useful in the presence of the other features. A thorough feature analysis should be considered for a better understanding of the contribution of each feature in the prediction.


> Overall, our findings do align well with medical knowledge and literature about risk factors for diabetes. The most important features are blood-related measurements, followed by age and BMI, with less importance seen for comorbid conditions like hypertension and heart disease.

## SUMMARY
The analysis employed a Random Forest classifier to predict diabetes based on various health indicators and lifestyle factors. The model was trained and evaluated on a dataset of 100,000 records, and Hyperparameter tuning was performed to optimize the model's performance.

The model achieved an accuracy of approximately 95.1%, with precision of 0.98 for class 0 (non-diabetic) and 0.69 for class 1 (diabetic). It was also able to recall 96% of non-diabetic cases and 81% of diabetic cases correctly. The relatively high accuracy and balanced performance on both classes indicate that the model is well-tuned and robust.

Feature importance analysis highlighted HbA1c_level and blood_glucose_level as the most critical factors in predicting Diabetes. Age and BMI also showed significant importance. However, some features, such as smoking history and gender, had minimal or no impact on the model's predictions.

## SUGGESTION
Data Collection: If further data collection is possible, we could aim to gather more information about lifestyle factors and other potential diabetes risk factors not covered in this dataset. For instance, detailed diet information, physical activity level, family history of diabetes, and more precise information on heart disease or hypertension might improve the model's predictive capabilities.

Model Exploration: While the Random Forest model has performed well, it might be worth exploring other machine learning models. For instance, gradient boosting models like XGBoost or LightGBM could potentially offer improved performance.

Feature Engineering: More sophisticated feature engineering could potentially improve model performance. Interaction features, polynomial features, or other transformations might be worth exploring.

Model Interpretation: To better understand the influence of each feature, we could use interpretability tools such as SHAP (SHapley Additive exPlanations) or permutation feature importance, which can offer a more nuanced view of feature importance than traditional feature importance based on impurity reduction.

Addressing Class Imbalance: Despite using SMOTE to balance the classes, there is still room for improvement in the performance metrics for the minority class. Other oversampling methods, undersampling methods, or cost-sensitive learning methods could be explored to improve the recall and precision for the minority class.

# THANK YOU!
# âœ”ï¸
## If you discovered this notebook to be useful or enjoyable, I'd greatly appreciate any upvotes! Your support motivates me to regularly update and improve it. :-)

