## ä¸ƒã€å¤ç°ä»£ç 
è¿è¡Œæ—¶å‡ºç°äº†é”™è¯¯ï¼šæœ¬ç¯å¢ƒå°šæœªå®‰è£… `lightgbm` å’Œ `xgboost` åº“ã€‚

ä¸ºäº†ç¡®ä¿æ‚¨åœ¨æœ¬åœ°å¤ç°ä¸å—å½±å“ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å®Œæ•´ç‰ˆæœ¬çš„å¤ç°è„šæœ¬ï¼Œå¹¶æå‰åœ¨æœ¬åœ°å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

---

### âœ… æ–‡ä»¶åå»ºè®®ï¼š`reproduce_cabg_aki_model.py`

#### ğŸ“¦ å®‰è£…ä¾èµ–ï¼ˆå»ºè®®åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼‰ï¼š

```bash
pip install numpy pandas scikit-learn lightgbm xgboost shap matplotlib
```

---

### ğŸ§  è„šæœ¬å†…å®¹å¦‚ä¸‹ï¼š

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
import shap
import matplotlib.pyplot as plt
import os

# åŠ è½½æ•°æ®
data_path = "04æ–‡çŒ®é˜…è¯»/05è‚¾ç—…/11åŸºäºæœºå™¨å­¦ä¹ çš„å† çŠ¶åŠ¨è„‰æ—è·¯ç§»æ¤æœ¯åæ€¥æ€§è‚¾æŸä¼¤é¢„æµ‹æ¨¡å‹/01data/simulated_cabg_aki_data.csv"
df = pd.read_csv(data_path)

# ç‰¹å¾ä¸æ ‡ç­¾
X = df.drop(columns=["AKI"])
y = df["AKI"]

# æ ‡å‡†åŒ–è¿ç»­å˜é‡
numeric_cols = X.select_dtypes(include=np.number).columns
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# å®šä¹‰æ¨¡å‹
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "DecisionTree": DecisionTreeClassifier(),
    "RandomForest": RandomForestClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "LightGBM": LGBMClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier()
}

# äº¤å‰éªŒè¯å¹¶è¯„ä¼°æ¨¡å‹
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    proba = model.predict_proba(X_test)[:, 1]
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, pred),
        "Precision": precision_score(y_test, pred),
        "Recall": recall_score(y_test, pred),
        "F1 Score": f1_score(y_test, pred),
        "AUC": roc_auc_score(y_test, proba)
    })

results_df = pd.DataFrame(results).sort_values(by="AUC", ascending=False)

# ä¿å­˜ç»“æœ
output_dir = "04æ–‡çŒ®é˜…è¯»/05è‚¾ç—…/11åŸºäºæœºå™¨å­¦ä¹ çš„å† çŠ¶åŠ¨è„‰æ—è·¯ç§»æ¤æœ¯åæ€¥æ€§è‚¾æŸä¼¤é¢„æµ‹æ¨¡å‹/02output"
os.makedirs(output_dir, exist_ok=True)
results_df.to_csv(os.path.join(output_dir, "model_comparison.csv"), index=False)

# è§£é‡Šæœ€ä½³æ¨¡å‹ï¼ˆä½¿ç”¨RandomForestï¼‰
best_model = RandomForestClassifier().fit(X_train, y_train)
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# SHAPå›¾ä¿å­˜
shap.summary_plot(shap_values[1], X_test, show=False)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "shap_summary_plot.png"))

# å¯é€‰ï¼šå±•ç¤ºä¾èµ–å…³ç³»è¾ƒå¼ºå˜é‡çš„ä¾èµ–å›¾
important_vars = ["eGFR", "UA", "ALT", "BNP", "Age", "Operation_time"]
for var in important_vars:
    shap.dependence_plot(var, shap_values[1], X_test, show=False)
    plt.savefig(os.path.join(output_dir, f"shap_dependence_{var}.png"))
    plt.clf()

print("âœ… æ¨¡å‹è®­ç»ƒä¸SHAPè§£é‡Šå®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š", output_dir)
```
