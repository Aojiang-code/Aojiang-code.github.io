# KAN卷积

> 本笔记记录于2024年5月23日

## KAN资料
- 项目链接：[GitHubPage](https://kindxiaoming.github.io/pykan/)

- 论文链接：[arxiv](https://arxiv.org/abs/2404.19756)

## KAN 于MLP 的对比


### 解读

#### [01陈巍：KAN网络技术最全解析——最热KAN能否干掉MLP和Transformer？（收录于GPT-4/ChatGPT技术与产业分析）](https://zhuanlan.zhihu.com/p/695830208)

这是知乎专栏上的一篇文章，标题为“陈巍：KAN网络技术最全解析——最热KAN能否干掉MLP和Transformer？”。这篇文章由陈巍博士撰写，他是一位在自然语言处理（NLP）和大模型算法-芯片协同设计领域有丰富经验的专家。

文章主要内容包括：

1. **KAN网络结构**：KAN（Kolmogorov-Arnold Network）的灵感来自于Kolmogorov-Arnold表示定理，与传统的多层感知器（MLP）相比，KAN在边（权重）上具有可学习的激活函数，而不是在节点（神经元）上。

2. **KAN的主要特点**：KAN具有四个主要特点，包括激活函数位于边而不是节点上、激活函数是可学习的、可以使用非线性核函数替代MLP的线性函数、可以设定细粒度的结点来提高逼近精度。

3. **KAN与MLP的对比**：KAN在数据拟合和偏微分方程求解中，较小的模型可以比MLP获得更好的准确性，并且具备更好的可解释性。

4. **Kolmogorov-Arnold表示定理**：文章详细介绍了Kolmogorov-Arnold表示定理，这是KAN的理论基础。

5. **KAN架构**：文章解释了KAN的架构设计，包括使用B样条（B-spline）构建的内部函数和外部函数。

6. **架构细节**：介绍了MIT团队对KAN进行的关键优化，包括残差激活函数、初始化方式和Spline网格的更新。

7. **KAN的逼近与神经缩放**：讨论了KAN的逼近能力和神经缩放定律，即测试损失随着模型参数的增加而减小。

8. **KAN的可解释性**：文章介绍了如何通过稀疏正则化、可视化、剪枝和符号化等步骤来提高KAN的可解释性。

9. **作者团队的类似工作**：陈巍博士提到，他们的团队在2019年就发表了与KAN相近的工作，并讨论了其优势和挑战。

10. **小结**：文章最后总结了KAN的发展空间和瓶颈，指出KAN在准确性和计算效率方面有优化空间，但训练速度慢是其主要瓶颈。

文章提供了对KAN网络技术的全面解析，并探讨了它是否能取代MLP和Transformer。陈巍博士的分析为我们提供了深入理解KAN网络的视角，并对其未来的应用和发展提出了见解。

#### [02MLP一夜被干掉，MIT加州理工等革命性KAN破记录，发现数学定理碾压DeepMind](https://www.thepaper.cn/newsDetail_forward_27249707)

这是来自澎湃新闻的一篇文章，标题为“MLP一夜被干掉，MIT加州理工等革命性KAN破记录，发现数学定理碾压DeepMind”。这篇文章报道了一种新型神经网络架构——Kolmogorov–Arnold Networks（KAN），由MIT、加州理工、东北大学等机构的团队发布。

文章的主要内容包括：

1. **KAN的简介**：KAN基于柯尔莫哥洛夫-阿诺德定理，具有更少的参数、更强的性能和更好的可解释性。

2. **KAN与传统MLP的对比**：KAN将可学习的激活函数从神经元移到权重上，这一改变与数学中的逼近理论有深刻联系。

3. **KAN的实验结果**：KAN在性能上优于传统的MLP，提升了神经网络的准确性和可解释性，并在科学研究中显示出潜在的应用价值。

4. **KAN的科学应用**：KAN被用于重新发现纽结理论中的数学定律，并以更小的网络和自动化方式复现了DeepMind之前的结果。

5. **KAN的物理应用**：KAN有助于物理学家研究Anderson局域化现象，这对于理解固体中的金属-绝缘体转变等基本现象至关重要。

6. **KAN的理论基础**：介绍了柯尔莫哥洛夫-阿诺德表示定理，以及KAN如何将多变量函数的逼近问题转化为学习一组单变量函数的问题。

7. **KAN架构**：KAN设计的核心思想，包括使用B样条曲线参数化单变量函数，以及如何通过堆叠KAN层来扩展网络的深度和宽度。

8. **KAN的实现细节**：讨论了KAN训练过程中的一些技巧，如残差激活函数、初始化尺度和样条网格的更新。

9. **KAN的性能和可解释性**：展示了KAN在性能上的优越性，以及如何通过交互解释KAN来获得可解释性最强的结果。

10. **KAN的未来发展**：KAN的准确性、参数效率和可解释性使其成为AI+Science领域一个有用的模型/工具，未来在科学领域的应用还待进一步挖掘。

文章强调KAN架构的革命性，预示着机器学习领域可能迎来新纪元。KAN不仅在理论上具有创新性，而且在实际应用中展现出巨大的潜力，特别是在数学和物理问题的解决上。

#### [03号称能打败MLP的KAN到底行不行？数学核心原理全面解析](https://zhuanlan.zhihu.com/p/696518752)
这是知乎专栏上的一篇文章，标题为“号称能打败MLP的KAN到底行不行？数学核心原理全面解析”。这篇文章深入探讨了Kolmogorov-Arnold Networks（KAN）背后的数学原理，并与多层感知器（MLP）进行了比较。

文章的主要内容包括：

1. **KAN的简介**：KAN是一种新型神经网络架构，它挑战了MLP这一深度学习架构的基石，并试图解决模型的黑箱性质。

2. **KAN与MLP的对比**：MLP在节点上有固定的激活函数，而KAN在边缘上有可学习的激活函数。KAN没有线性权重，权重参数被参数化为样条的单变量函数。

3. **数学原理**：文章详细介绍了KAN背后的数学原理，包括Kolmogorov-Arnold表示定理、函数逼近、平滑性和连续性、空间填充曲线等概念。

4. **样条的作用**：样条是一种数学函数，用于创建光滑和灵活的曲线或曲面。KAN使用B样条，因为它们在处理高维数据时具有鲁棒性，并且能够形成光滑的多维表面。

5. **Kolmogorov-Arnold表示定理**：该定理表明任何多变量连续函数都可以表示为单变量连续函数和加法运算的组合，这为KAN提供了理论基础。

6. **KAN的数学模型**：KAN层使用一维函数矩阵，每个连接都由一个可以单独调整的单独函数定义，提供了更高程度的灵活性和局部控制。

7. **计算过程对比**：文章通过一个简单的例子，比较了KAN和MLP的输出计算过程，展示了KAN如何通过每个连接应用特定的函数来实现高度灵活性。

8. **总结**：KAN在参数数量较少的情况下可以达到与MLP相当甚至更高的精度，并且提供了增强的可解释性。尽管KAN的拟合速度可能不如MLP快，但它为人工智能社区带来了新的方向。

文章作者Vishal Rajput表示，尽管KAN是一个有前景的研究方向，但他仍在进行更深入的研究和测试，以评估KAN与MLP相比的具体优势和局限性。


#### [04替代MLP的KAN，被开源项目扩展到卷积了](https://www.thepaper.cn/newsDetail_forward_27434432)

> 这是一篇不错的解读文章

这是一篇来自澎湃新闻的文章，标题为《替代MLP的KAN，被开源项目扩展到卷积了》。这篇文章主要介绍了一种新型神经网络结构——Kolmogorov-Arnold Network（KAN），以及它在卷积神经网络（CNN）中的应用。

以下是文章的主要内容概述：

1. **KAN的提出**：KAN是由MIT等机构的研究者提出的一种有潜力的MLP（多层感知器）替代方法。它在准确性和可解释性方面优于MLP，并且能够以更少的参数量达到更好的效果。

2. **KAN与MLP的比较**：KAN和MLP都具有强大的数学基础，但KAN在边上具有激活函数，而MLP在节点上具有激活函数。KAN的参数效率更高，尽管每个KAN层比MLP层拥有更多的参数。

3. **KAN卷积（CKAN）**：研究者将KAN的理念扩展到卷积神经网络，提出了KAN卷积。KAN卷积不是应用点积，而是对每个元素应用可学习的非线性激活函数，然后将它们相加。

4. **KAN卷积的参数**：KAN卷积的内核相当于具有多个输入和输出神经元的KAN线性层。对于一个KxK的内核，KAN卷积总共有K²(gridsize + 2)个参数，而普通卷积只有K²个参数。

5. **初步评估**：作者进行了一些初步实验来评估KAN卷积的性能。基于MNIST数据集的测试显示，KANConv & MLP模型与传统的大型ConvNet相比在准确度上是可以接受的，但KANConv & MLP所需的参数数量是标准ConvNet所需参数的七分之一。

6. **未来工作**：作者计划在未来在更复杂的数据集上进行实验，并调整模型和比较模型的超参数。他们认为随着模型和数据集复杂度的增加，KAN卷积网络的性能应该会有所提高。

7. **结论**：目前，KAN卷积网络的性能并没有显著高于传统卷积网络。但KAN的优势在于它对参数的要求要少得多，这在处理大规模问题时可能非常有用。

文章还提到了KAN卷积的开源项目地址，感兴趣的读者可以访问GitHub页面了解更多详情。
项目地址：[GitHub](https://github.com/AntonioTepsich/Convolutional-KANs)


这篇文章为对KAN及其在卷积神经网络中应用感兴趣的读者提供了有价值的信息。


#### [05爆火后反转？「一夜干掉MLP」的KAN：其实我也是MLP](https://www.jiqizhixin.com/articles/2024-05-07-4)

这是机器之心网站上的一篇文章，标题为“爆火后反转？‘一夜干掉MLP’的KAN：其实我也是MLP”。这篇文章讨论了Kolmogorov-Arnold Networks（KAN）与多层感知器（MLP）之间的关系，并探讨了KAN是否能被视为一种具有创新性的架构，还是本质上仍然是MLP的一种形式。

文章的主要内容包括：

1. **KAN的提出**：KAN是由来自MIT等机构的研究者提出的一种新方法，它在准确性和可解释性方面优于MLP，并且能够以更少的参数量胜过更大参数量的MLP。

2. **KAN的质疑**：KAN迅速走红后，有人提出质疑，认为KAN实际上可以被改写为一个具有非典型结构的普通MLP。一个Colab文档展示了如何将KAN层改写为MLP层。

3. **KAN与MLP的比较**：文章讨论了KAN和MLP在结构上的差异，并提出了KAN层可以通过重复和移位操作改写为MLP层的观点。

4. **KAN的主要贡献**：KAN的主要贡献在于其可解释性，它提供了一种可能发现新科学定律的模型，而不仅仅是在扩展速度或准确性上超越MLP。

5. **KAN的参数效率**：有研究者质疑KAN是否真的能够以更少的参数实现与等效的NN相同的性能。文章提到，有研究者通过减少网络大小和调整训练参数，使得MLP能够以更少的参数达到KAN的性能。

6. **KAN与MLP的本质区别**：论文作者之一表示，KAN和MLP在方法上没有本质不同，KAN的主要优势在于可解释性和符号回归。

7. **KAN的创新性**：有研究者指出，KAN的思路并不新奇，20世纪80年代就有相关研究。同时，也有观点认为KAN的创新之处在于将旧的想法重新打包，并在toy数据上进行了良好的实验。

8. **KAN作者的回应**：KAN的作者之一Sachin Vaidya表示，KAN的目标是寻找可解释的AI模型，能够学习物理学家发现自然规律的洞察力。作者欢迎对KAN的批评，并认为实践是检验真理的唯一标准。

9. **KAN的未来**：KAN的作者之一刘子鸣表示，KAN专为关心高精度和可解释性的应用程序而设计，他对于KAN的成功和失败都持开放态度，并表示对包含KAN和MLP的理论框架感兴趣。

文章最后指出，尽管KAN在理论上可能并不比MLP更具革命性，但它确实为深度学习社区带来了新的思考，并推动了对可解释性的研究。


#### [06号称能打败MLP的KAN到底行不行？数学核心原理全面解析](https://avoid.overfit.cn/post/6ee2307e614b462f9c9aac26ef12252d)

这是Overfit网站上的一篇文章，标题为“号称能打败MLP的KAN到底行不行？数学核心原理全面解析”。这篇文章深入探讨了Kolmogorov-Arnold Networks（KAN）背后的数学原理，并与多层感知器（MLP）进行了比较。

文章的主要内容包括：

1. **KAN的简介**：KAN是一种新型神经网络架构，基于Kolmogorov-Arnold表示定理，提供了一种有前途的替代MLP的方案。KAN在边缘（权重）上有可学习的激活函数，而不是在节点（神经元）上。

2. **KAN与MLP的对比**：KAN没有线性权重，权重参数被参数化为样条的单变量函数。这种设计使得KAN在准确性和可解释性方面可能优于MLP。

3. **数学原理**：文章详细介绍了KAN背后的数学原理，包括Kolmogorov-Arnold表示定理、样条的定义和应用、平滑性和连续性、空间填充曲线等概念。

4. **样条的作用**：样条是一种分段多项式函数，能够在多项式块相交的地方保持高度平滑。B样条在KAN中特别有用，因为它们在处理高维数据时具有鲁棒性，并且能够形成光滑的多维表面。

5. **Kolmogorov-Arnold表示定理**：该定理表明任何多变量连续函数都可以表示为单变量连续函数和加法运算的组合，为KAN提供了理论基础。

6. **KAN的数学模型**：KAN层使用一维函数矩阵（例如B样条），其中每个连接都由一个可以单独调整的单独函数定义，提供了更高程度的灵活性和局部控制。

7. **计算过程对比**：文章通过一个简单的例子，比较了KAN和MLP的输出计算过程，展示了KAN如何通过每个连接应用特定的函数来实现高度灵活性。

8. **总结**：KAN在参数数量较少的情况下可以达到与MLP相当甚至更高的精度，并且提供了增强的可解释性。尽管KAN的拟合速度可能不如MLP快，但它为人工智能社区带来了新的方向。

文章作者Vishal Rajput表示，尽管KAN是一个有前景的研究方向，但他仍在进行更深入的研究和测试，以评估KAN与MLP相比的具体优势和局限性。无论如何，KAN为人工智能领域带来了新的思考和创新。


#### [革命性新网络KAN【第二篇-相比MLP，KAN胜在哪？】](https://blog.csdn.net/weixin_54335478/article/details/138572248)

> 与前文解读差不多

#### [08一文通透想取代MLP的KAN：通俗理解Kolmogorov-Arnold定理和KAN的方方面面](https://blog.csdn.net/v_JULY_v/article/details/139074230)

> 与前文解读差不多



## 卷积资料
- 项目地址：[卷积算法](https://github.com/vdumoulin/conv_arithmetic)


## KAN卷积资料

### 源码

- 项目地址：[GitHub](https://github.com/AntonioTepsich/Convolutional-KANs)

> 作者表示，KAN 卷积的实现是一个很有前景的想法，尽管它仍处于早期阶段。他们进行了一些初步实验，以评估 KAN 卷积的性能。

> 值得注意的是，之所以公布这些「初步」结果，是因为他们希望尽快向外界介绍这一想法，推动社区更广泛的研究。

> 在接下来的几天和几周里，作者还将彻底调整模型和用于比较的模型的超参数。虽然已经尝试了一些超参数和架构的变化，但这只是启发式的，并没有采用任何精确的方法。由于计算能力和时间的原因，他们还没有使用大型或更复杂的数据集，并正在努力解决这个问题。

> 未来，作者将在更复杂的数据集上进行实验，这意味着 KANS 的参数量将会增加，因为需要实现更多的 KAN 卷积层。

#### 结论

目前，与传统卷积网络相比，作者表示并没有看到 KAN 卷积网络的性能有显著提高。他们分析认为，这是由于使用的是简单数据集和模型，与尝试过的最佳架构（ConvNet Big，基于规模因素，这种比较是不公平的）相比，该架构的优势在于它对参数的要求要少得多。

在 2 个相同的卷积层和 KAN 卷积层与最后连接的相同 MLP 之间进行的比较显示，经典方法略胜一筹，准确率提高了 0.06，而 KAN 卷积层和 KAN 线性层的参数数量几乎只有经典方法的一半，准确率却降低了 0.04。

> 作者表示，随着模型和数据集复杂度的增加，KAN 卷积网络的性能应该会有所提高。同时，随着输入维数的增加，模型的参数数量也会增长得更快。

### 解读

#### [01What KAN I say？KAN代码全解析](https://zhuanlan.zhihu.com/p/697035684)

这篇文章详细介绍了KAN（一种新型神经网络结构）的代码实现，包括KAN的原理、代码框架、核心组件以及如何使用KAN进行前向传播和模型训练等内容。

文章提到KAN在量化领域有其优势，尤其是在可解释性方面。KAN的代码主要由以下几个部分组成：
1. Symbolic_KANLayer：实现符号激活层，使用符号函数代替传统数值激活函数。
2. KANLayer：定义基于样条函数的激活层，负责样条函数的前向传播和参数共享等。
3. spline：提供处理B样条曲线的函数。
4. LBFGS：实现了L-BFGS算法的优化器。
5. KAN：定义KAN模型，支持前向传播、模式设置、训练、剪枝等功能。

文章还给出了KAN模型的前向传播代码示例，展示了如何通过多个KANLayer和Symbolic_KANLayer层来实现模型的前向传播。KANLayer的核心实现细节包括：
- 使用残差激活函数，将激活函数分解为基础函数和样条函数的和。
- 初始化时，每个激活函数的样条部分初始化为0，权重因子使用Xavier初始化。
- 动态更新样条曲线的网格，以适应激活值的变化。

此外，文章还提到KAN支持Adam和LBFGS优化器，默认使用LBFGS。作者正在不断完善代码，后续还会有进一步优化。

这篇文章对于想深入了解KAN代码实现和原理的读者很有帮助。

#### [02KAN最新研究方向！KAN卷积神经网络来了！](https://zhuanlan.zhihu.com/p/698195925)  

> 这是一篇不错的解读文章

这篇文章介绍了KAN卷积神经网络的概念、原理、参数、初步评估以及一个构建KAN卷积网络的示例代码。

以下是文章的主要内容概述：

1. **KAN卷积神经网络的发布**：Alex Bodner团队发布了关于KAN卷积神经网络的研究成果。

2. **KAN的定义**：KAN（Kolmogorov-Arnold Network）是一个学习的B样条曲线加上一个残差激活函数，乘以一个可学习的参数。

3. **KAN卷积**：KAN卷积是一种特殊的卷积操作，它在每个边缘上应用一个可学习的非线性函数，并将它们相加。KAN卷积的核相当于一个具有多个输入和输出神经元的KAN线性层。

4. **KAN卷积中的参数**：对于一个KxK的核，每个元素都有一个ϕ，其参数数量是gridsize + 1。因此，线性层的参数数量是gridsize + 2。对于KAN卷积，总共有K²(gridsize + 2)个参数，而普通卷积只有K²个参数。

5. **初步评估**：文章提到了一些初步实验来评估KAN卷积的性能。基于MNIST数据集的测试显示，KANConv & MLP模型在准确度上与传统的大型ConvNet相比是可以接受的。KAN卷积网络在参数数量上显著少于传统卷积网络。

6. **构建KAN卷积网络的示例代码**：文章提供了一个使用PyTorch构建KAN卷积网络的示例代码，用于MNIST数据集的分类任务。


这篇文章为对KAN卷积神经网络感兴趣的读者提供了有价值的信息和资源。

#### [03替代MLP的KAN，被开源项目扩展到卷积了](https://www.thepaper.cn/newsDetail_forward_27434432)

> 这是一篇不错的解读文章

这是一篇来自澎湃新闻的文章，标题为《替代MLP的KAN，被开源项目扩展到卷积了》。这篇文章主要介绍了一种新型神经网络结构——Kolmogorov-Arnold Network（KAN），以及它在卷积神经网络（CNN）中的应用。

以下是文章的主要内容概述：

1. **KAN的提出**：KAN是由MIT等机构的研究者提出的一种有潜力的MLP（多层感知器）替代方法。它在准确性和可解释性方面优于MLP，并且能够以更少的参数量达到更好的效果。

2. **KAN与MLP的比较**：KAN和MLP都具有强大的数学基础，但KAN在边上具有激活函数，而MLP在节点上具有激活函数。KAN的参数效率更高，尽管每个KAN层比MLP层拥有更多的参数。

3. **KAN卷积（CKAN）**：研究者将KAN的理念扩展到卷积神经网络，提出了KAN卷积。KAN卷积不是应用点积，而是对每个元素应用可学习的非线性激活函数，然后将它们相加。

4. **KAN卷积的参数**：KAN卷积的内核相当于具有多个输入和输出神经元的KAN线性层。对于一个KxK的内核，KAN卷积总共有K²(gridsize + 2)个参数，而普通卷积只有K²个参数。

5. **初步评估**：作者进行了一些初步实验来评估KAN卷积的性能。基于MNIST数据集的测试显示，KANConv & MLP模型与传统的大型ConvNet相比在准确度上是可以接受的，但KANConv & MLP所需的参数数量是标准ConvNet所需参数的七分之一。

6. **未来工作**：作者计划在未来在更复杂的数据集上进行实验，并调整模型和比较模型的超参数。他们认为随着模型和数据集复杂度的增加，KAN卷积网络的性能应该会有所提高。

7. **结论**：目前，KAN卷积网络的性能并没有显著高于传统卷积网络。但KAN的优势在于它对参数的要求要少得多，这在处理大规模问题时可能非常有用。

文章还提到了KAN卷积的开源项目地址，感兴趣的读者可以访问GitHub页面了解更多详情。
项目地址：[GitHub](https://github.com/AntonioTepsich/Convolutional-KANs)


这篇文章为对KAN及其在卷积神经网络中应用感兴趣的读者提供了有价值的信息。


#### [04替代MLP的KAN，被开源项目扩展到卷积了](https://www.jiqizhixin.com/articles/2024-05-20-2)

> 这篇解读的内容和上一篇相同。

这是一篇来自机器之心的文章，标题为《替代MLP的KAN，被开源项目扩展到卷积了》。这篇文章详细讨论了一种新型神经网络结构——Kolmogorov-Arnold Network（KAN），它作为MLP（多层感知器）的替代方案，并介绍了KAN在卷积神经网络中的扩展应用。

以下是文章的主要内容概述：

1. **KAN的提出**：KAN是由MIT等机构的研究者提出的一种新型神经网络结构，它在准确性和可解释性方面优于MLP，并且能够以更少的参数量达到更好的效果。

2. **KAN与MLP的比较**：KAN和MLP都具有强大的数学基础，MLP基于通用逼近定理，KAN基于Kolmogorov-Arnold表示定理。KAN的特点是激活函数位于边上，而MLP的激活函数位于节点上。

3. **KAN卷积（CKAN）**：KAN的理念被扩展到卷积神经网络中，提出了KAN卷积。与传统卷积不同，KAN卷积在每个像素中应用可学习的非线性激活函数，然后求和。

4. **KAN卷积的参数**：KAN卷积的内核相当于具有多个输入和输出神经元的KAN线性层。对于一个KxK内核，KAN卷积的参数数量为K^2(gridsize + 2)，而普通卷积只有K^2。

5. **初步评估**：文章提到了作者进行的初步实验，以评估KAN卷积的性能。基于MNIST数据集的测试显示，KANConv & MLP模型与传统的大型ConvNet相比在准确度上是可以接受的。

6. **未来工作**：作者计划在未来在更复杂的数据集上进行实验，并调整模型和比较模型的超参数。他们认为随着模型和数据集复杂度的增加，KAN卷积网络的性能应该会有所提高。

7. **结论**：目前，KAN卷积网络的性能并没有显著高于传统卷积网络。但是，KAN的优势在于它对参数的要求要少得多，这在处理大规模问题时可能非常有用。

文章还提供了KAN卷积的开源项目地址，感兴趣的读者可以访问GitHub页面了解更多详情。
项目地址：[GitHub](https://github.com/AntonioTepsich/Convolutional-KANs)

这篇文章为对KAN及其在卷积神经网络中应用感兴趣的读者提供了有价值的信息。

#### [05替代MLP的KAN，被开源项目扩展到卷积了](https://cloud.tencent.com/developer/article/2419634)

> 这篇解读的内容和上一篇相同。

这是腾讯云开发者社区上的一篇文章，标题同样是《替代MLP的KAN，被开源项目扩展到卷积了》，由机器之心编辑部撰写。

文章的主要内容与之前提供的几篇文章类似，概述如下：

1. **KAN的介绍**：KAN（Kolmogorov-Arnold Network）是由MIT等机构研究者提出的一种有潜力的MLP（多层感知器）替代方法。KAN在准确性和可解释性方面优于MLP，并且能以更少的参数量达到胜过更大参数量MLP的效果。

2. **KAN与MLP的对比**：KAN和MLP都具有强大的数学基础。与MLP相比，KAN在边上具有激活函数，而MLP在节点上具有激活函数。KAN显示出更高的参数效率。

3. **KAN卷积（CKAN）**：KAN架构的理念被扩展到卷积神经网络中，提出了KAN卷积。KAN卷积不是应用点积，而是对每个元素应用可学习的非线性激活函数，然后将它们相加。

4. **KAN卷积的参数**：KAN卷积的内核相当于具有多个输入和输出神经元的KAN线性层。对于一个KxK内核，KAN卷积的参数数量为K^2(gridsize + 2)，而普通卷积只有K^2。

5. **初步评估**：文章提到作者进行了初步实验，以评估KAN卷积的性能。基于MNIST数据集的测试显示，KANConv & MLP模型与传统的大型ConvNet相比，在准确度上是可以接受的。

6. **未来工作**：作者计划在未来在更复杂的数据集上进行实验，并调整模型和比较模型的超参数。

7. **结论**：目前，KAN卷积网络的性能并没有显著高于传统卷积网络。但是KAN的优势在于它对参数的要求要少得多，这在处理大规模问题时可能非常有用。

文章还提供了KAN卷积的开源项目地址，感兴趣的读者可以访问GitHub页面了解更多详情。
项目地址：[GitHub](https://github.com/AntonioTepsich/Convolutional-KANs)

这篇文章为对KAN及其在卷积神经网络中应用感兴趣的读者提供了有价值的信息。

#### [06全网爆火的KAN，被开源项目扩展到卷积了](https://blog.csdn.net/weixin_47196664/article/details/139079168)

> 这篇解读的内容和上一篇相同。
#### [07替代MLP的KAN，被开源项目扩展到卷积了](https://blog.csdn.net/moxibingdao/article/details/139078876)
> 这篇解读的内容和上一篇相同。
#### [08KAN遇见卷积！代码开源！](https://blog.csdn.net/amusi1994/article/details/139080229)
> 这篇解读的内容和上一篇相同。

#### [09全新神经网络架构KAN来了！](https://cloud.tencent.com/developer/article/2416649)

您提供的链接是腾讯云开发者社区上的一篇文章，标题为“全新神经网络架构KAN来了！”。这篇文章介绍了一种新的神经网络架构——KAN（Kolmogorov-Arnold Networks），它与传统的多层感知器（MLP）架构有很大的不同。KAN架构使用更少的参数，在数学和物理问题上能够取得更高的精度。

文章中提到，KAN架构的灵感来源于Kolmogorov-Arnold表示定理（KART），该定理表明每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。KAN架构在权重上放置了可学习的激活函数，而不是像MLP那样在神经元上放置固定的激活函数。这种设计使得KAN在函数拟合、偏微分方程求解以及处理凝聚态物理任务方面比MLP表现更好。

KAN架构还有几个显著的优点：
1. **避免灾难性遗忘**：在处理大模型问题时，KAN能够天然地避免灾难性遗忘问题。
2. **易于解释和交互**：KAN能够通过符号公式揭示合成数据集的组成结构和变量依赖性，使得人类用户可以更容易地与KAN交互。
3. **发现新公式**：研究人员使用KAN重新复现了DeepMind的研究结果，并发现了Knot理论中的新公式。

尽管KAN架构在训练速度上可能较慢，但研究人员正在努力解决这个问题。此外，KAN架构已经在GitHub上开源，并在短时间内获得了很高的关注度。

文章还讨论了KAN架构是否能够替代Transformer中的MLP层，以及它在GPU上的实现和优化问题。最后，文章提供了一个决策树，帮助读者决定何时应该选择使用KAN架构。

如果您对KAN架构感兴趣，可以访问以下链接获取更多信息：
- 项目链接：[https://kindxiaoming.github.io/pykan/](https://kindxiaoming.github.io/pykan/)
- 论文链接：[https://arxiv.org/abs/2404.19756](https://arxiv.org/abs/2404.19756)

这篇文章提供了对KAN架构的全面介绍，对于对人工智能和神经网络感兴趣的开发者来说，是一个很好的资源。

