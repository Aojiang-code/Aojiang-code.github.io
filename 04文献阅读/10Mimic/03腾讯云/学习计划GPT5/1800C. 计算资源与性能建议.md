下面是\*\*附录 C：计算资源与性能建议（CSV→Parquet、列裁剪、并行）\*\*的详细学习笔记。
坚持主线：**DuckDB-free 的数据帧工作流（Polars/pandas + Parquet）**，仅把 DuckDB 当作可选“旁路工具”做性能验证；正式流水线不依赖 SQL/Postgres。

---

# C.1 基础配置：硬件与系统小贴士

**CPU / 线程**

* Polars 默认多线程（Rayon 线程池）。可用环境变量\*\*`POLARS_MAX_THREADS`\*\*限速或放开：`export POLARS_MAX_THREADS=$(nproc)`。
* I/O 与压缩都吃 CPU；NVMe + 8–32 线程效果最佳。

**内存**

* 16–32 GB 可跑“裁剪后”的 MIMIC-IV；做大表（`chartevents`）聚合建议 64 GB+。
* 使用 \*\*惰性 + 流式（streaming）\*\*管线，避免一次性 materialize 全量。

**磁盘（优先 NVMe）**

* 原始 **CSV(.gz)** → 尽快转换为 **Parquet(ZSTD)**。
* 文件系统推荐 ext4/xfs；挂载加 `noatime` 可略降元数据开销。

**进程/文件句柄**

* 批量并行转换前：`ulimit -n 65535`，避免 “too many open files”。

---

# C.2 CSV → Parquet：越早越好

> 目标：把“慢、不可列剪裁”的 CSV（尤其 `*.csv.gz`）尽快转成支持 **列裁剪 + 谓词下推 + 并行** 的 Parquet。

## C.2.1 单文件流式转换（含列裁剪/过滤）

```python
import polars as pl
from pathlib import Path

SRC = Path("data/raw/mimiciv_hosp/labevents.csv.gz")
OUT = Path("data/interim/labevents_min.parquet")

# 1) 先给 schema/类型“打样”（只采几万行，提高后续性能与一致性）
probe = pl.read_csv(SRC, n_rows=50_000)
schema = {
  "subject_id": pl.Int32, "hadm_id": pl.Int32, "itemid": pl.Int32,
  "charttime": pl.Datetime, "valuenum": pl.Float32, "valueuom": pl.Utf8
}

# 2) 惰性读取 + 列裁剪 + 过滤 → 直接 sink 到 Parquet（流式）
(
  pl.scan_csv(SRC, dtypes=schema, ignore_errors=True)      # CSV 也能按需解析列
    .select(["hadm_id","itemid","charttime","valuenum","valueuom"])
    .filter(pl.col("valuenum").is_not_null())
    .sink_parquet(OUT, compression="zstd", compression_level=6, statistics=True)
)
```

**要点**

* **`scan_csv` → `select`/`filter` → `sink_parquet`**：全程**惰性执行**，内存占用可控。
* 若 `*.csv.gz` 成为瓶颈（gzip 单线程），可用 `pigz -p 8 -d` 预解压再转；换来更高的吞吐（但占用更多磁盘）。

## C.2.2 多文件并行转换（进程级）

```python
from concurrent.futures import ProcessPoolExecutor
import polars as pl, os
from pathlib import Path

IN_DIR = Path("data/raw/mimiciv_icu")
OUT_DIR = Path("data/interim/icu_parquet"); OUT_DIR.mkdir(parents=True, exist_ok=True)

def convert_one(csv_path: Path):
    out = OUT_DIR/f"{csv_path.stem}.parquet"
    (pl.scan_csv(csv_path)
       .select(["stay_id","itemid","charttime","valuenum","valueuom"])
       .sink_parquet(out, compression="zstd", compression_level=6, statistics=True))

files = sorted(IN_DIR.glob("*.csv.gz"))
with ProcessPoolExecutor(max_workers=os.cpu_count()//2 or 2) as ex:
    list(ex.map(convert_one, files))
```

**小贴士**

* 文件级并行 + 每个任务内部 Polars 多线程 → **双层并行**。
* 控制并行度，避免占满内存与 I/O；建议 `max_workers ≈ CPU/2`。

## C.2.3 Parquet 写入参数建议

* **压缩**：`zstd`（通用且压缩比高）；`compression_level=4–8` 视 CPU 取舍。
* **文件大小**：目标 **128–512 MB/文件**；太小（<32 MB）会造成“微文件风暴”。
* **分区**：按**变量族**或**哈希桶**（见 C.5）。

---

# C.3 列裁剪与谓词下推：把过滤提前

**原则：越靠前越好**。无论 CSV/Parquet，都应：

1. **`select([...])` 只读必要列**；
2. **`filter(...)` 提前过滤**；
3. **先半连接再窗口**（对 `labevents` 先按 `hadm_id` 减小候选，再按 `intime` 过滤到 0–24h）。

```python
# 例：只拉取肌酐/尿素氮，并在“加入 ICU 锚点”前就裁剪列
cre_bun_ids = [50912, 51006]
lab = (pl.scan_parquet("data/interim/labevents_min.parquet")   # Parquet → 推下投影/过滤
         .filter(pl.col("itemid").is_in(cre_bun_ids))
         .select(["hadm_id","itemid","charttime","valuenum","valueuom"]))
```

---

# C.4 并行与流式：撑大吞吐，稳住内存

## C.4.1 Polars 多线程 + Streaming

* **聚合/连接**可设 `collect(streaming=True)` 或使用 `LazyFrame.sink_*`。
* 对**超大连接**或 groupby：能 streaming 就 streaming；不行再分桶分治。

```python
# 大窗口聚合示例（流式执行）
agg = (lab.join(icu_small, on="hadm_id", how="inner")
          .filter((pl.col("charttime") >= pl.col("intime")) &
                  (pl.col("charttime") <  pl.col("intime")+pl.duration(hours=24)))
          .groupby(["stay_id","itemid"])
          .agg([
            pl.col("valuenum").median().alias("median"),
            pl.col("valuenum").count().alias("n"),
          ])
          .collect(streaming=True))
```

## C.4.2 进程并行的“分桶”策略

* 按 **`stay_id % K`** 或 **按科室**切分任务文件，进程内只处理本桶数据，最终再拼接。
* 适合 `chartevents` 这种超大宽表特征化。

---

# C.5 分区与文件布局：让扫描“只读它需要的”

**按业务分区（推荐）**

* `data/interim/features/labs/signal=na|k|scr/...parquet`
* `data/interim/features/vitals/signal=hr|map|temp_c/...parquet`
* 读取时只扫需要的 signal 目录，避免全盘扫描。

**按键分桶（并行友好）**

* `bucket = stay_id % 100` → `.../bucket=00..99/*.parquet`
* 优点：天然支持**分区裁剪**与**多进程**；合并时只拼被命中的桶。

**文件大小**

* 控制在 **128–512 MB/文件**。多到上千个小文件会显著拖慢元数据与打开开销。

---

# C.6 类型与字典：用小类型换大空间

* **整型**：`Int32/Int16` 足矣（`subject_id/hadm_id/stay_id` 常落在 Int32）。
* **浮点**：连续变量用 `Float32`，对建模足够且省一半内存。
* **类别**：低基数字段（如 `valueuom`、`first_careunit`）→ **Categorical**；跨文件 join 推荐启用**全局字符串缓存**。

```python
import polars as pl

with pl.StringCache():  # 使分类编码在多个表/文件之间一致
    d1 = pl.read_parquet("...").with_columns(pl.col("first_careunit").cast(pl.Categorical))
    d2 = pl.read_parquet("...").with_columns(pl.col("first_careunit").cast(pl.Categorical))
    joined = d1.join(d2, on=["first_careunit"], how="left")
```

---

# C.7 经典大表（chartevents）加速配方

1. **先字典筛 `itemid`**（`d_items` → `itemid` 白名单）；
2. **只读必要列**：`stay_id,itemid,charttime,valuenum,valueuom`；
3. **先与 ICU 样本做半连接**（只保留 cohort 的 stay）；
4. **窗口过滤**：`0–24h`；
5. **就地聚合**到窄结果（例如 `median/max/last/n`），再落盘。

```python
ITEMS = pl.read_parquet("data/interim/d_items.parquet")
whitelist = ITEMS.filter(pl.col("label").str.to_lowercase().str.contains("heart rate|systolic|diastolic")) \
                 .select("itemid").to_series().to_list()

chart = (pl.scan_csv("data/raw/mimiciv_icu/chartevents.csv.gz")
           .select(["stay_id","itemid","charttime","valuenum","valueuom"])
           .filter(pl.col("itemid").is_in(whitelist))
           .join(pl.scan_parquet("data/derived/cohort_base.parquet")
                    .select(["stay_id","intime"]), on="stay_id", how="inner")
           .filter((pl.col("charttime") >= pl.col("intime")) &
                   (pl.col("charttime") <  pl.col("intime")+pl.duration(hours=24)))
           .groupby(["stay_id","itemid"])
           .agg(pl.col("valuenum").median().alias("median"))
           .sink_parquet("data/interim/chart_vitals_0_24h.parquet",
                         compression="zstd", compression_level=6, statistics=True))
```

---

# C.8 监控与调优：看得见的性能

* **Polars 配置**

  * `pl.Config.set_verbose(True)`：打印优化/执行规划；
  * `pl.Config.set_tbl_rows(30)`：节制表格渲染；
  * `pl.Config.set_fmt_str_lengths(200)`：避免字符串省略误导。

* **计时装饰器**

```python
import time, functools
def tic(func):
    @functools.wraps(func)
    def wrap(*a, **k):
        t0=time.time(); r=func(*a, **k); dt=time.time()-t0
        print(f"[{func.__name__}] {dt:.1f}s"); return r
    return wrap
```

* **I/O 指标**：优先观察**CPU 利用率≈磁盘带宽**是否拉满；如果 CPU 空闲而磁盘 100%，说明 I/O 受限→提高并行/换 NVMe。反之 CPU 满→降低压缩级别或减少并行度。

---

# C.9 与第14章缓存协同：只算“需要重算”的

* 在 **Task 指纹**中纳入：**输入文件 MD5 + 参数 JSON + 代码签名**。若三者未变 → **跳过**。
* 大表转换任务（CSV→Parquet）通常最耗时，**优先缓存**；上游一变，下游连锁重算（依赖传播）。

---

# C.10 常见性能 FAQ

**Q1：读取 `*.csv.gz` 很慢？**
A：gzip 解压单线程。方案：

* 先 `pigz -p 8 -d` 批量解压为 `.csv` 再转 Parquet；或
* 直接 `scan_csv(...).sink_parquet(...)`，接受较慢首轮转换，后续全走 Parquet。

**Q2：并行后反而更慢？**
A：I/O 饱和或频繁上下文切换。降低 `max_workers`，或把任务“分桶”成**较大**的单元（少即是多）。

**Q3：内存爆掉？**
A：未用 streaming、一次性 materialize 了巨表。

* 把 `.collect()` 改为 `.collect(streaming=True)` 或 `sink_parquet`；
* 先聚合/裁剪，再 join；
* 转 `Float32/Int32`；必要时 **分桶处理 + 合并**。

**Q4：Parquet 文件太多导致扫描很慢？**
A：把小文件合并到 128–512 MB；利用**分区目录**减少扫描面；必要时做**manifest**（你自己的文件清单）。

**Q5：为何 CSV 两次读出来类型不一致？**
A：推断不稳定。**显式 `dtypes={...}`**，或先“打样”再强制 cast，保持上下游一致性。

**Q6：join 卡顿严重？**
A：检查 join 键类型一致且**已编码为 Categorical**（同时启用 `StringCache`）；必要时先**半连接**减少右表规模。

---

# C.11 一条龙模板：从 CSV 到“可用的特征 Parquet”

```python
import polars as pl
from pathlib import Path

RAW, INTERIM, DERIVED = Path("data/raw"), Path("data/interim"), Path("data/derived")
INTERIM.mkdir(parents=True, exist_ok=True)

# 1) labs: CSV→Parquet（裁剪+过滤）
(pl.scan_csv(RAW/"mimiciv_hosp/labevents.csv.gz",
             dtypes={"hadm_id":pl.Int32,"itemid":pl.Int32,"charttime":pl.Datetime,
                     "valuenum":pl.Float32,"valueuom":pl.Utf8})
   .select(["hadm_id","itemid","charttime","valuenum","valueuom"])
   .filter(pl.col("valuenum").is_not_null())
   .sink_parquet(INTERIM/"labs_min.parquet", compression="zstd", compression_level=6))

# 2) 字典筛 itemid
d_lab = pl.read_csv(RAW/"mimiciv_hosp/d_labitems.csv.gz")
ids_scr = (d_lab.filter(pl.col("label").str.contains("(?i)creatin"))["itemid"].to_list())

# 3) cohort（首次 ICU）
icu = pl.read_csv(RAW/"mimiciv_icu/icustays.csv.gz").select(["subject_id","hadm_id","stay_id","intime"])
icu_first = icu.sort("intime").groupby("subject_id").agg(pl.all().first())

# 4) 0–24h 聚合 → 特征 Parquet
feat_scr = (pl.scan_parquet(INTERIM/"labs_min.parquet")
              .filter(pl.col("itemid").is_in(ids_scr))
              .join(icu_first.select(["hadm_id","stay_id","intime"]), on="hadm_id", how="inner")
              .filter((pl.col("charttime") >= pl.col("intime")) &
                      (pl.col("charttime") <  pl.col("intime")+pl.duration(hours=24)))
              .groupby(["stay_id"])
              .agg([
                  pl.col("valuenum").median().alias("scr_median__0_24h"),
                  pl.col("valuenum").max().alias("scr_max__0_24h"),
                  pl.col("valuenum").count().alias("scr_n__0_24h"),
              ])
              .sink_parquet(DERIVED/"features_scr_0_24h.parquet", compression="zstd"))
```

---

## 一页速查（落地前自检）

* [ ] **CSV→Parquet** 在最靠前阶段完成；压缩用 **ZSTD**，文件大小 128–512 MB
* [ ] **列裁剪/过滤** 在读取阶段就下推；`collect(streaming=True)` 或 `sink_parquet`
* [ ] **并行策略**：文件级进程并行 + Polars 线程；注意 I/O 饱和与内存
* [ ] **分区布局**：按变量族/哈希桶分区，减少扫描面
* [ ] **类型收紧**：Int32/Float32/Categorical + `StringCache`
* [ ] **大表配方**：字典筛 itemid → 半连接 → 窗口 → 聚合后落盘
* [ ] **缓存协同**：变更检测（输入 MD5 + 参数 + 代码签名），只重算必要步骤

---

这份附录把**吞吐、内存、并行**三件事捏在一起：**尽早 Parquet 化、把裁剪与过滤前置、用 streaming 与分桶做增量**。按此执行，在腾讯云 NVMe 机器上跑 MIMIC-IV 的日常 ETL/特征聚合，通常能把“小时级”的等待压到“分钟级”。
