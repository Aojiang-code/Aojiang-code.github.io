下面是**第14章：实验记录与自动化**的详细学习笔记。
仍然坚持你的技术约束：**DuckDB + CSV/Parquet + Python（pandas/Polars）+ Jupyter（VSCode）**，**不使用 SQL / Postgres**。
核心目标：把前面所有步骤固化成**可复现、可参数化、可增量**的流水线；任何一次结果都能回溯到**输入数据 + 配置 + 代码版本**。

---

# 第14章 实验记录与自动化

## 14.1 Notebooks 结构与命名规范

### 14.1.1 目录与命名

```
project/
├─ data/
│  ├─ raw/         # 原始 CSV（只读）
│  ├─ interim/     # 中间特征（可重复计算）
│  └─ derived/     # 建模表、标签、评估结果（可重复计算）
├─ notebooks/
│  ├─ 00_env_and_paths.ipynb
│  ├─ 03_duckdb_io_basics.ipynb          # 仅示范“无SQL”数据帧互操作
│  ├─ 06_labels_scr_only.ipynb
│  ├─ 07_build_cohort.ipynb
│  ├─ 09_features_labs_vitals.ipynb
│  ├─ 10_model_table_build.ipynb
│  ├─ 11_baselines_cv.ipynb
│  ├─ 12_eval_and_calibration.ipynb
│  └─ 13_explainability.ipynb
├─ src/
│  ├─ configs/     # YAML/JSON 配置
│  │  ├─ default.yaml
│  │  ├─ dev.small.yaml                   # 小样调试
│  │  └─ prod.full.yaml
│  ├─ pipeline/    # 可复用函数 & 轻量任务引擎
│  │  ├─ io.py
│  │  ├─ features.py
│  │  ├─ labels.py
│  │  ├─ model.py
│  │  └─ dag.py
│  └─ utils/
│     ├─ hashing.py
│     └─ logging.py
├─ reports/
│  ├─ figures/      # PNG/PDF
│  └─ metrics/
└─ run.py           # 一键执行若干任务（命令行参数化）
```

**命名建议**

* 以**两位数字**排序（便于批量执行）：`06_…`, `07_…`。
* 每个 Notebook 的**首格**包含：

  * `RUN_ID`（日期+配置哈希），`CONFIG_PATH`；
  * 固定随机种子；
  * 依赖版本与硬件信息（便于追溯）。

**配套建议（强烈推荐）**

* **Jupytext**：每个 `.ipynb` 绑定一个 `.py`（percent 格式），Git 友好版本化。
* **Papermill**：允许命令行注入参数（配置路径、样本上限等），批量无头执行。
* **Notebook cell tags**：将“参数”单元标记为 `parameters`，Papermill 会自动覆盖。

### 14.1.2 Notebook 顶部模板（可粘贴复用）

```python
# --- Parameters (papermill会覆盖) ---
RUN_ID = None
CONFIG_PATH = "src/configs/default.yaml"
SAMPLE_N = None  # 开发期可对子集实验，例如 10000

# --- 环境与种子 ---
import os, sys, json, platform, random, time, hashlib, inspect
from pathlib import Path
import numpy as np
import polars as pl
import pyarrow as pa

pl.Config.set_tbl_rows(30)
np.random.seed(42); random.seed(42)

# --- 路径 ---
PROJ = Path(".").resolve()
DATA = PROJ/"data"; RAW = DATA/"raw"; INTERIM = DATA/"interim"; DERIVED = DATA/"derived"
REPORTS = PROJ/"reports"

# --- 运行ID：日期+配置指纹 ---
def sha1_text(t: str) -> str: return hashlib.sha1(t.encode()).hexdigest()[:8]
cfg_txt = Path(CONFIG_PATH).read_text(encoding="utf-8")
RUN_ID = RUN_ID or (time.strftime("%Y%m%d_%H%M") + "_" + sha1_text(cfg_txt))
print("RUN_ID:", RUN_ID)

# --- 记录环境 ---
ENV_REC = {
    "python": platform.python_version(),
    "platform": platform.platform(),
    "polars": pl.__version__,
    "pyarrow": pa.__version__,
    "run_id": RUN_ID,
    "config_path": CONFIG_PATH
}
(Path("reports/metrics").mkdir(parents=True, exist_ok=True))
(Path("reports/metrics")/f"env_{RUN_ID}.json").write_text(json.dumps(ENV_REC, indent=2), encoding="utf-8")
```

---

## 14.2 配置与参数管理（YAML/JSON）

### 14.2.1 配置文件结构（示例 `default.yaml`）

```yaml
paths:
  raw: "data/raw"
  interim: "data/interim"
  derived: "data/derived"
  reports: "reports"

windows:
  feature_start_h: 0
  feature_end_h: 24
  label_start_h: 24
  label_end_h: 48

cohort:
  require_min_icu_hours: 48
  first_only: true
  keep_units: ["MICU","SICU"]   # 可留空
  adult_only: true

labels:
  route: "scr_only"             # "derived" 或 "scr_only"
  baseline_strategy: ["7d","48h","0_24h"]

features:
  labs:
    include: ["scr","bun","na","k","cl","hco3","ph"]
  vitals:
    include: ["hr","rr","temp_c","spo2","sbp","dbp","map"]
  urine:
    enable: true
    mlkgph_clip_high: 20.0
  meds:
    enable: false

train:
  cv_folds: 5
  model: "logreg"               # "logreg"|"xgb"|"lgbm"|"rf"
  class_weight: "balanced"
  random_state: 42

evaluate:
  bins_calibration: 10
  target_sensitivity: 0.85
  target_ppv: 0.5
```

### 14.2.2 读取与校验（轻量化，无外部框架）

```python
# src/utils/hashing.py
import yaml, hashlib, json
from dataclasses import dataclass

def load_yaml(path):
    with open(path,"r",encoding="utf-8") as f:
        return yaml.safe_load(f)

def hash_obj(o) -> str:
    return hashlib.md5(json.dumps(o, sort_keys=True, ensure_ascii=False).encode()).hexdigest()

@dataclass
class Config:
    data: dict
    @property
    def digest(self): return hash_obj(self.data)

# Notebook/脚本中使用
from src.utils.hashing import load_yaml, Config
CFG = Config(load_yaml(CONFIG_PATH))
print("CONFIG_DIGEST:", CFG.digest)
```

**实践要点**

* **单一“配置源”**：所有参数（时间窗、白名单、阈值、剪裁区间等）都写入 YAML。
* **派生配置**：`dev.small.yaml` 只覆盖 `default.yaml` 的差异（例如 `SAMPLE_N=10000`），按顺序合并。
* **冻结配置**：每次运行把 YAML 原文**拷贝**到 `reports/metrics/config_<RUN_ID>.yaml`，确保追溯。
* **输入快照**：把**关键中间件文件路径+MD5**写入快照（第10章已有）。

### 14.2.3 Notebook 参数化（无头执行）

* **Papermill**：`papermill notebooks/11_baselines_cv.ipynb out/11_baselines_cv.ipynb -p CONFIG_PATH src/configs/prod.full.yaml -p RUN_ID ...`
* **Jupytext**：`jupytext --to notebook notebooks/11_baselines_cv.py && papermill ...`
* （可选）**CLI 脚本**：`python run.py --task train --config src/configs/prod.full.yaml`

---

## 14.3 结果缓存与增量计算（DuckDB + Parquet 管线）

> 目标：**只计算需要更新的步骤**。当**输入文件/配置/代码**未改变时，直接复用上次结果。
> 我们用**Parquet 物化**作为中间层，结合一个**极轻量的任务引擎（DAG）**来做**指纹比对**与**依赖传播**。
> 计算本身采用 **Polars 数据帧**（与 DuckDB/Arrow 生态完全兼容），输出 Parquet 可被 DuckDB 零拷贝读取（若你需要 DuckDB 参与交互探索，也无需写 SQL）。

### 14.3.1 任务与依赖（最小可用 DAG）

```python
# src/pipeline/dag.py
from dataclasses import dataclass, field
from pathlib import Path
import hashlib, json, inspect, os
from typing import Callable, List, Dict, Any

def md5_file(p: Path) -> str:
    return hashlib.md5(p.read_bytes()).hexdigest() if p.exists() else ""

def md5_text(t: str) -> str:
    return hashlib.md5(t.encode()).hexdigest()

def md5_many(items: List[str]) -> str:
    h = hashlib.md5()
    for s in items: h.update(s.encode())
    return h.hexdigest()

CACHE = Path("reports/metrics"); CACHE.mkdir(parents=True, exist_ok=True)
INDEX = CACHE/"cache_index.json"
if INDEX.exists(): CACHE_IDX = json.loads(INDEX.read_text(encoding="utf-8"))
else: CACHE_IDX = {}

@dataclass
class Task:
    name: str
    func: Callable[[Dict[str,Any]], None]
    inputs: List[Path] = field(default_factory=list)
    outputs: List[Path] = field(default_factory=list)
    params: Dict[str,Any] = field(default_factory=dict)
    code_sig: str = ""

    def fingerprint(self) -> str:
        files_sig = [md5_file(p) for p in self.inputs]
        params_sig = md5_text(json.dumps(self.params, sort_keys=True, ensure_ascii=False))
        code_sig = self.code_sig or md5_text(inspect.getsource(self.func))
        return md5_many(files_sig + [params_sig, code_sig])

    def up_to_date(self) -> bool:
        fp = self.fingerprint()
        rec = CACHE_IDX.get(self.name, {})
        outs_exist = all(p.exists() for p in self.outputs)
        return outs_exist and rec.get("fingerprint","") == fp

    def run(self):
        self.func(self.params)
        # 原子写：函数内部写临时文件再rename（见下）
        fp = self.fingerprint()
        CACHE_IDX[self.name] = {"fingerprint": fp, "outputs": [str(p) for p in self.outputs]}
        INDEX.write_text(json.dumps(CACHE_IDX, indent=2, ensure_ascii=False), encoding="utf-8")

def run_tasks(tasks: List[Task]):
    for t in tasks:
        if t.up_to_date():
            print(f"[SKIP] {t.name}")
        else:
            print(f"[RUN ] {t.name}")
            t.run()
```

### 14.3.2 原子写与“只读原始/可重算中间”

```python
# src/pipeline/io.py
from pathlib import Path
import polars as pl, os, uuid

def write_parquet_atomic(df: pl.DataFrame, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + f".tmp_{uuid.uuid4().hex}")
    df.write_parquet(tmp)
    os.replace(tmp, path)   # 原子替换

def load_parquet(path: Path, columns=None) -> pl.DataFrame:
    return pl.read_parquet(path, columns=columns) if path.exists() else pl.DataFrame()
```

### 14.3.3 典型任务定义（示例）

```python
# notebooks 中或 run.py 中组织任务
from src.pipeline.dag import Task, run_tasks
from src.pipeline.io import write_parquet_atomic
from src.utils.hashing import load_yaml, Config
import polars as pl
from pathlib import Path

CFG = Config(load_yaml("src/configs/default.yaml")).data

RAW = Path("data/raw"); INTERIM = Path("data/interim"); DERIVED = Path("data/derived")

def task_convert_labs(params):
    # 仅作示例：把 labevents 指定列写成列裁剪后的 Parquet
    lab_csv = RAW/"mimiciv_hosp/labevents.csv.gz"
    out = INTERIM/"labs_minimal.parquet"
    df = (pl.scan_csv(lab_csv)
            .select(["hadm_id","itemid","charttime","valuenum","valueuom"])
            .collect(streaming=True))
    write_parquet_atomic(df, out)

def task_features_labs(params):
    out = INTERIM/"features_labs_0_24h.parquet"
    # ……复用第9章的函数，产生 FEA_LABS……
    fea = pl.read_parquet(INTERIM/"features_labs_0_24h.parquet")  # 此处仅示例
    write_parquet_atomic(fea, out)

def task_model_table(params):
    out = DERIVED/"model_table_latest.parquet"
    # ……复用第10章整合逻辑……
    mt = pl.read_parquet(DERIVED/"model_table_latest.parquet")
    write_parquet_atomic(mt, out)

tasks = [
    Task(
        name="convert_labs",
        func=task_convert_labs,
        inputs=[RAW/"mimiciv_hosp/labevents.csv.gz"],
        outputs=[INTERIM/"labs_minimal.parquet"],
        params={"columns":["hadm_id","itemid","charttime","valuenum","valueuom"]}
    ),
    Task(
        name="features_labs",
        func=task_features_labs,
        inputs=[INTERIM/"labs_minimal.parquet", DERIVED/"cohort_base.parquet", Path("src/configs/default.yaml")],
        outputs=[INTERIM/"features_labs_0_24h.parquet"],
        params={"windows": CFG["windows"], "features": CFG["features"]["labs"]}
    ),
    Task(
        name="model_table",
        func=task_model_table,
        inputs=[DERIVED/"label_24_48h.parquet", DERIVED/"features_basic_0_24h.parquet", INTERIM/"features_labs_0_24h.parquet"],
        outputs=[DERIVED/"model_table_latest.parquet"],
        params={}
    ),
]
run_tasks(tasks)
```

> 机制说明
>
> * **指纹包括**：所有**输入文件的 MD5** + **参数 JSON** + **任务函数代码**哈希。任一改变会触发重算。
> * **依赖**：任务顺序表达依赖（或你可显式拓扑排序）；上游输出改变会使下游的 `inputs` MD5 改变，从而连锁重算。
> * **只读原始数据**：`data/raw` 从不覆盖；中间/派生结果可随配置变化反复重建。
> * **DuckDB 兼容**：中间与产物统一为 Parquet，**DuckDB 与 Polars 可零拷贝读取**（不需要写 SQL，我们保持数据帧范式）。

### 14.3.4 结果缓存的“粒度”与“分区”

* **列裁剪 + 按需读取**：第3章的惰性扫描（Polars `scan_csv`/`scan_parquet`）本身就是 I/O 缓存。
* **分区落盘**：对**超大表**（如 `chartevents` 派生的特征），按**变量族**或**哈希桶**分区：

  * 例如 `data/interim/features/labs/` 下按 `signal=na|k|scr` 分目录；
  * 或 `stay_bucket = stay_id % 100` 分 100 份；
* **幂等性**：所有任务按照**同一配置**多次运行**结果一致**（MD5 不变化）。

### 14.3.5 与评估/报告的衔接

* 把 **RUN\_ID** 贯穿到所有产物文件名，例如：

  * `derived/model_table_{RUN_ID}.parquet`
  * `reports/figures/decision_curve_{RUN_ID}.png`
  * `reports/metrics/metrics_oof_{RUN_ID}.json`
* 在**快照 JSON**（第10章）中记录：

  * `run_id`、`config_digest`、`tasks_cache_index` 的路径、所有关键输入的 MD5。

---

## 本章产出与自检

**产出**

* `src/configs/*.yaml`（默认+开发/生产覆盖）
* `src/pipeline/dag.py`（轻量任务引擎）
* `src/pipeline/io.py`（原子写、读封装）
* `reports/metrics/cache_index.json`（任务指纹索引）
* （可选）`run.py`（命令行一键跑：`python run.py --config src/configs/prod.full.yaml --tasks convert_labs,features_labs,model_table`）

**自检清单**

* [ ] 每个 Notebook 有统一**参数单元**并记录 **RUN\_ID/CONFIG\_DIGEST/环境信息**
* [ ] 所有关键参数来自 YAML，运行时把 YAML 原文**拷贝**到 `reports/metrics/`
* [ ] 中间结果均为 **Parquet**，目录分层清晰（raw/interim/derived）
* [ ] 任务引擎基于 **输入MD5 + 参数 + 代码哈希** 做缓存；上游变化能驱动下游重算
* [ ] 原子写避免“半成品”文件；失败时不会污染产物
* [ ] 评估与图表包含 **RUN\_ID**，可对应回本次配置与数据快照

---

### 进阶建议（可选）

* **Jupytext + pre-commit**：提交前自动同步 `.ipynb` ↔ `.py`；
* **DVC** 或 **Git-LFS**：跟踪较大的 Parquet 产物与快照；
* **Hydra/OmegaConf**：若配置复杂到需要“多 profile 合并”，可引入；
* **并行/调度**：简单并行可用 `multiprocessing`；复杂工作流再考虑 Prefect/Airflow（合规前提下在本地内网运行）。

---

这样，**第14章**把“人—Notebook—脚本”三件事绑在一起：
有**清晰的结构**、**可控的参数**、**可靠的缓存**与**可复现的快照**。接下来你若需要，我可以基于本章骨架为项目生成一份**最小可跑的 `run.py`** 和两三个**模板 Notebook**（已接好参数/记录/缓存），直接在腾讯云的 VSCode 里使用。
