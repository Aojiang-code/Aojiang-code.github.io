下面是**附录 A：DuckDB SQL 对照速查（仅参考，不在主体中使用）**。
用途：当你需要交叉验证数据帧结果或快速做一次性探查时，可以用这些 **DuckDB SQL 等价写法**对照**Polars/pandas**数据帧范式。主线仍坚持“**无 SQL、数据帧工作流**”。

---

# A. DuckDB SQL 对照速查

> 备注
>
> * DuckDB 可**直接读取 CSV/Parquet/Gzip**，支持通配符与分区目录。
> * MIMIC 时间戳为**去标识的本地无时区时间**；在 DuckDB/Polars 都按 **TIMESTAMP/naive** 处理，窗口一律用**相对时间**。
> * 常用优化：`PRAGMA threads=8; PRAGMA enable_object_cache=true;`（缓存 Parquet 元数据）、`PRAGMA memory_limit='8GB';`。
> * 本附录的 SQL 仅作“校对/验证”参考，不纳入正式流水线。

---

## A.1 读取与列裁剪 / 过滤

**目标**：按需读取、只取必要列与行（等价于数据帧的惰性裁剪）

**SQL（DuckDB）**

```sql
-- 读取压缩CSV，只选列 + 条件过滤 + 投影
SELECT hadm_id, itemid, charttime, valuenum, valueuom
FROM read_csv_auto('data/raw/mimiciv_hosp/labevents*.csv.gz')
WHERE itemid IN (50912, 51006)            -- 例：肌酐/尿素氮（示意）
  AND valuenum IS NOT NULL;
```

**Polars 数据帧**

```python
pl.scan_csv(LAB) \
  .select(["hadm_id","itemid","charttime","valuenum","valueuom"]) \
  .filter(pl.col("itemid").is_in([50912,51006]) & pl.col("valuenum").is_not_null())
```

---

## A.2 基本聚合（mean/max/min/median/IQR/计数）

**SQL**

```sql
SELECT stay_id,
       AVG(valuenum)  AS scr_mean__0_24h,
       MEDIAN(valuenum) AS scr_median__0_24h,
       QUANTILE_CONT(valuenum, 0.75) - QUANTILE_CONT(valuenum, 0.25) AS scr_iqr__0_24h,
       MIN(valuenum)  AS scr_min__0_24h,
       MAX(valuenum)  AS scr_max__0_24h,
       COUNT(*)       AS scr_n__0_24h
FROM my_scr_0_24h
GROUP BY stay_id;
```

**Polars**

```python
df.groupby("stay_id").agg([
  pl.col("valuenum").mean().alias("scr_mean__0_24h"),
  pl.col("valuenum").median().alias("scr_median__0_24h"),
  (pl.col("valuenum").quantile(0.75)-pl.col("valuenum").quantile(0.25)).alias("scr_iqr__0_24h"),
  pl.col("valuenum").min().alias("scr_min__0_24h"),
  pl.col("valuenum").max().alias("scr_max__0_24h"),
  pl.count().alias("scr_n__0_24h")
])
```

---

## A.3 连接与“锚点窗口”过滤（ICU 0–24h）

**SQL**

```sql
WITH stays AS (
  SELECT stay_id, hadm_id, subject_id, intime
  FROM read_csv_auto('data/raw/mimiciv_icu/icustays.csv.gz')
),
labs AS (
  SELECT hadm_id, itemid, charttime, valuenum, valueuom
  FROM read_csv_auto('data/raw/mimiciv_hosp/labevents.csv.gz')
  WHERE itemid IN (50912,51006) AND valuenum IS NOT NULL
)
SELECT l.*, s.stay_id, s.intime
FROM labs l
JOIN stays s USING (hadm_id)
WHERE l.charttime >= s.intime
  AND l.charttime <  s.intime + INTERVAL '24' HOUR;
```

**Polars**

```python
labs.join(stays, on="hadm_id", how="inner") \
    .filter((pl.col("charttime") >= pl.col("intime")) &
            (pl.col("charttime") <  pl.col("intime")+pl.duration(hours=24)))
```

---

## A.4 单位换算与条件列（CASE / WHEN）

**SQL**

```sql
SELECT *,
  CASE
    WHEN LOWER(valueuom) IN ('µmol/l','umol/l','μmol/l') THEN valuenum/88.4
    WHEN LOWER(valueuom) = 'mg/dl' THEN valuenum
    ELSE valuenum
  END AS scr_val_mgdl
FROM scr_rows;
```

**Polars**

```python
df.with_columns(
  pl.when(pl.col("valueuom").str.to_lowercase().is_in(["µmol/l","umol/l","μmol/l"]))
    .then(pl.col("valuenum")/88.4)
    .when(pl.col("valueuom").str.to_lowercase()=="mg/dl")
    .then(pl.col("valuenum"))
    .otherwise(pl.col("valuenum"))
    .alias("scr_val_mgdl")
)
```

---

## A.5 “首/末值”与变化率（窗口函数）

**SQL**

```sql
SELECT stay_id,
       FIRST_VALUE(scr_val)  OVER (PARTITION BY stay_id ORDER BY charttime) AS scr_first,
       LAST_VALUE(scr_val)   OVER (PARTITION BY stay_id ORDER BY charttime
                                   RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS scr_last
FROM scr_0_24h;

-- 聚合到单行
SELECT stay_id,
       MIN(scr_first) AS scr_first__0_24h,
       MAX(scr_last)  AS scr_last__0_24h,
       (MAX(scr_last)-MIN(scr_first))/24.0 AS scr_rate__0_24h
FROM (
  /* 上述窗口结果 */
) t
GROUP BY stay_id;
```

**Polars**

```python
sub = df.sort(["stay_id","charttime"]) \
        .groupby("stay_id").agg([
          pl.col("scr_val").first().alias("scr_first__0_24h"),
          pl.col("scr_val").last().alias("scr_last__0_24h")
        ]) \
        .with_columns((pl.col("scr_last__0_24h")-pl.col("scr_first__0_24h"))/24.0
                      .alias("scr_rate__0_24h"))
```

---

## A.6 透视 / 反透视（宽↔长）

**SQL（条件聚合式 PIVOT）**

```sql
-- 将多 itemid 聚合到同一行（例：心率/收缩/舒张 压力的 median）
SELECT stay_id,
       MEDIAN(CASE WHEN itemid IN (220045) THEN val END) AS hr_median__0_24h,
       MEDIAN(CASE WHEN itemid IN (220179,220050) THEN val END) AS sbp_median__0_24h,
       MEDIAN(CASE WHEN itemid IN (220180,220051) THEN val END) AS dbp_median__0_24h
FROM vitals_0_24h
GROUP BY stay_id;
```

**Polars**

```python
df.groupby(["stay_id","itemid"]) \
  .agg(pl.col("val").median().alias("median")) \
  .pivot(values="median", index="stay_id", columns="itemid")  # 之后重命名列 → hr/sbp/dbp
```

---

## A.7 时间分桶与滚动窗口

**时间分桶（按小时）**

**SQL**

```sql
SELECT stay_id,
       date_trunc('hour', charttime) AS hr_bin,
       AVG(valuenum) AS hr_mean
FROM chart_0_24h
GROUP BY stay_id, hr_bin
ORDER BY stay_id, hr_bin;
```

**Polars**

```python
df.with_columns(pl.col("charttime").dt.truncate("1h").alias("hr_bin")) \
  .groupby(["stay_id","hr_bin"]).agg(pl.col("valuenum").mean().alias("hr_mean"))
```

**滚动窗口（6h 最小尿量，DuckDB 版本需≥0.9 支持基于区间的窗口）**

**SQL**

```sql
SELECT stay_id, charttime,
       SUM(urine_ml) OVER (
         PARTITION BY stay_id
         ORDER BY charttime
         RANGE BETWEEN INTERVAL '6' HOUR PRECEDING AND CURRENT ROW
       ) AS uo_6h_sum
FROM urine_0_24h;
```

**Polars**

```python
df.sort(["stay_id","charttime"]) \
  .groupby_rolling(index_column="charttime", period="6h", by="stay_id") \
  .agg(pl.col("urine_ml").sum().alias("uo_6h_sum"))
```

---

## A.8 近邻/区间连接（ASOF-like）

**需求**：将某表 `A(t)` 与“同 key 最近时间的” `B(t')` 对齐，或限定在 `[t-Δ, t]` 区间。

**SQL（最近一次观测，限定 24h 内）**

```sql
WITH pairs AS (
  SELECT a.stay_id, a.t AS t_a, b.t AS t_b, b.val,
         ROW_NUMBER() OVER (
           PARTITION BY a.stay_id, a.t
           ORDER BY ABS(EPOCH(b.t) - EPOCH(a.t))
         ) AS rn
  FROM A a
  JOIN B b ON a.stay_id = b.stay_id
          AND b.t BETWEEN a.t - INTERVAL '24' HOUR AND a.t
)
SELECT * FROM pairs WHERE rn = 1;
```

**Polars（join + 分组排序取最邻近）**

```python
pairs = A.join(B, on="stay_id") \
         .filter((pl.col("t_b")<=pl.col("t_a")) & (pl.col("t_b")>=pl.col("t_a")-pl.duration(hours=24))) \
         .with_columns((pl.col("t_a")-pl.col("t_b")).dt.seconds().abs().alias("dt")) \
         .sort(["stay_id","t_a","dt"]) \
         .groupby(["stay_id","t_a"]).agg(pl.all().first())
```

---

## A.9 去重与“首/末次就诊/ICU”

**SQL**

```sql
-- 每个 subject 的首次 ICU
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY subject_id ORDER BY intime) AS rn
  FROM read_csv_auto('data/raw/mimiciv_icu/icustays.csv.gz')
) t WHERE rn = 1;
```

**Polars**

```python
icu.sort("intime").groupby("subject_id").agg(pl.all().first())
```

---

## A.10 文本匹配（LIKE/REGEXP）与字典映射

**SQL**

```sql
-- 通过字典筛选化验项目
WITH d AS (
  SELECT itemid, label FROM read_csv_auto('.../d_labitems.csv.gz')
  WHERE LOWER(label) LIKE '%creatin%'
)
SELECT l.*
FROM read_csv_auto('.../labevents.csv.gz') l
JOIN d USING (itemid);
```

**Polars**

```python
d = d_lab.filter(pl.col("label").str.to_lowercase().str.contains("creatin"))
labs = labevents.join(d.select(["itemid"]), on="itemid")
```

---

## A.11 近似与抽样

**SQL**

```sql
SELECT APPROX_COUNT_DISTINCT(subject_id) FROM admissions;
SELECT * FROM chartevents USING SAMPLE 1%;          -- 随机 1% 抽样
```

**Polars**

```python
df.select(pl.col("subject_id").n_unique())  # 精确；也可先子样
df.sample(frac=0.01, shuffle=True)
```

---

## A.12 写 Parquet（分区/压缩）

**SQL**

```sql
-- 直接把一个 SELECT 导出成分区 Parquet
COPY (
  SELECT stay_id, itemid, stat, val FROM some_agg
) TO 'data/interim/agg_parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD, PARTITION_BY (itemid));
```

**Polars**

```python
df.write_parquet("data/interim/agg.parquet")  # 或者按列分片后分别写
```

---

## A.13 质量检查与异常时间戳

**SQL**

```sql
-- 住院时长（负时长视为异常）
SELECT hadm_id, TIMESTAMPDIFF('hour', admittime, dischtime) AS los_h
FROM admissions
WHERE dischtime < admittime;  -- 异常时间戳
```

**Polars**

```python
adm.with_columns(
  ((pl.col("dischtime")-pl.col("admittime")).dt.seconds()/3600.0).alias("los_h")
).filter(pl.col("dischtime")<pl.col("admittime"))
```

---

## A.14 典型“从 CSV→特征”的一条龙（示例汇总）

**SQL（仅参考）**

```sql
PRAGMA threads=8; PRAGMA enable_object_cache=true;

WITH icu AS (
  SELECT subject_id, hadm_id, stay_id, intime
  FROM read_csv_auto('.../icustays.csv.gz')
),
labs_raw AS (
  SELECT hadm_id, itemid, charttime, valuenum, valueuom
  FROM read_csv_auto('.../labevents.csv.gz')
  WHERE itemid IN (50912,51006) AND valuenum IS NOT NULL
),
labs_24h AS (
  SELECT l.*, i.stay_id, i.intime
  FROM labs_raw l JOIN icu i USING (hadm_id)
  WHERE l.charttime >= i.intime AND l.charttime < i.intime + INTERVAL '24' HOUR
),
scr AS (
  SELECT stay_id, charttime,
         CASE WHEN LOWER(valueuom) IN ('µmol/l','umol/l','μmol/l') THEN valuenum/88.4
              ELSE valuenum END AS scr
  FROM labs_24h
  WHERE itemid = 50912
),
scr_agg AS (
  SELECT stay_id,
         AVG(scr) AS scr_mean__0_24h,
         MEDIAN(scr) AS scr_median__0_24h,
         MIN(scr) AS scr_min__0_24h,
         MAX(scr) AS scr_max__0_24h
  FROM scr
  GROUP BY stay_id
)
SELECT * FROM scr_agg;
```

**Polars（主线推荐）**：见第 9 章相应代码段。

---

## A.15 常见函数/语法对照速查

| 任务      | DuckDB SQL                                               | Polars（示意）                                   |
| ------- | -------------------------------------------------------- | -------------------------------------------- |
| 取年份     | `EXTRACT(year FROM ts)` / `date_part('year', ts)`        | `pl.col("ts").dt.year()`                     |
| 截断到小时   | `date_trunc('hour', ts)`                                 | `dt.truncate("1h")`                          |
| 时间差（小时） | `timestampdiff('hour', t1, t2)`                          | `(t2 - t1).dt.seconds() / 3600`              |
| 条件列     | `CASE WHEN ... THEN ... ELSE ... END`                    | `pl.when(...).then(...).otherwise(...)`      |
| 正则匹配    | `REGEXP_MATCHES(col, 'pat')` / `col ~ 'pat'`             | `.str.contains("pat")`                       |
| 去重首行    | `QUALIFY ROW_NUMBER() OVER(PARTITION BY k ORDER BY t)=1` | `sort(...).groupby(k).agg(pl.all().first())` |
| 近似分位数   | `APPROX_QUANTILE(x, 0.5)`                                | `quantile(0.5)`（精确；近似可先采样）                   |
| 采样      | `USING SAMPLE 1%`                                        | `df.sample(frac=0.01)`                       |

---

## A.16 运行小贴士

* **大表**（`chartevents`）先用**字典表筛 `itemid`**，再进时间窗聚合。
* **写入**用 Parquet（ZSTD 压缩），DuckDB/Polars **零拷贝互通**；DuckDB 的 `COPY ... TO (FORMAT PARQUET)` 可做临时转换校验。
* **精度一致性**：DuckDB `MEDIAN` vs Polars `median()`、`QUANTILE_CONT` vs `quantile()` 默认插值方式略有差异，必要时指定参数对齐或只比较 3～4 位精度。
* **窗口函数版本**：基于区间的窗口（`RANGE BETWEEN INTERVAL ...`）需 DuckDB 较新版本；老版本可改用**自连接 + 限定 + `ROW_NUMBER()`** 取最邻近。
* **不要把 `outtime/dischtime/deathtime` 当特征**：这些只能用于筛选与审计（主线在第 7–10 章已强调）。

---

### 结语

这份速查表帮助你在需要**快速交叉验证**时，用 DuckDB SQL 与数据帧结果**相互印证**。正式项目与可复现流水线仍以**Polars/pandas + Parquet**为主，DuckDB 仅作临时验证与数据检查的“旁路工具”。
