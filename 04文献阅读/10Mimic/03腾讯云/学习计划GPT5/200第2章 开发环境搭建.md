下面是\*\*第 2 章：开发环境搭建（VSCode + Jupyter）\*\*的详细学习笔记，聚焦 **DuckDB + CSV + Python + Jupyter（在 VSCode 中）** 的无数据库服务、尽量“无 SQL”的数据帧工作流。你可以按 “准备 → 安装 → 验证 → 目录规范” 的顺序一步步落地。

---

# 第2章 开发环境搭建（VSCode + Jupyter）

## 2.1 VSCode + Python 环境与扩展（Jupyter、Python、Remote-SSH）

### 2.1.1 VSCode 基本安装

* **VSCode 稳定版**（Windows/Mac/Linux 任意）。
* **Python（3.10–3.11 推荐）**：保证系统 `python --version` 输出为 3.10.x 或 3.11.x（与 duckdb、polars、pyarrow、scikit-learn 协调性较好）。

> 若在腾讯云远程主机上开发，建议：本地装 VSCode + **Remote-SSH** 扩展，通过 SSH 进入云主机，在**远程**运行 Jupyter 与 Python 内核，数据也放远程。

### 2.1.2 必装扩展（VSCode 扩展市场）

* **Python**（微软）：解释器选择、Lint/Format、调试。
* **Jupyter**（微软）：Notebook 运行与内嵌可视化。
* **Pylance**：智能补全与类型提示。
* **Jupyter Keymap / Jupyter Cell Tags**（可选）：更接近 notebook 的快捷键体验。
* **Remote-SSH**：本地 VSCode 直连远程服务器开发（如腾讯云）。
* （可选）**GitLens**、**YAML**、**Better Comments**、**Error Lens** 提升研发体验。

### 2.1.3 远程开发（腾讯云）要点

1. 在本地生成 SSH 密钥并将公钥加入远程主机 `~/.ssh/authorized_keys`。
2. VSCode 中使用 Remote-SSH 连接远程主机。
3. 在远程主机上安装 Python、创建虚拟环境、安装依赖（见 §2.2–2.3）。
4. 打开远程工作区后，在 VSCode 右上角 Kernel 选择器中选中**远程虚拟环境内核**来运行 Notebook。

---

## 2.2 虚拟环境与依赖：Python 版本、pip/uv、`requirements.txt` / `pyproject.toml`

> 目标：为项目创建**独立的虚拟环境**，并用 **pip 或 uv** 管理依赖。下面提供两套等价方案，任选其一。

### 2.2.1 方案 A：内置 `venv` + pip（最通用）

```bash
# 1) 创建并激活虚拟环境
python3 -m venv .venv
# Linux/Mac:
source .venv/bin/activate
# Windows PowerShell:
# .\.venv\Scripts\Activate.ps1

# 2) 升级 pip & 安装依赖
python -m pip install -U pip
python -m pip install -r requirements.txt  # 见下文模板
```

**`requirements.txt`（建议版本范围，允许补丁更新）**：

```txt
# 计算与数据
duckdb>=0.10,<2.0
pandas>=2.2,<3.0
polars>=0.20,<1.0
pyarrow>=15,<20

# 建模与绘图
scikit-learn>=1.4,<2.0
matplotlib>=3.8,<4.0

# Jupyter & Kernel
jupyterlab>=4.0,<5.0
ipykernel>=6.29,<7.0

# 实用工具（可选）
tqdm>=4.66,<5.0
python-dotenv>=1.0,<2.0
```

> 说明
>
> * 使用“区间约束”而非强 Pin，可兼顾稳定与获取安全补丁。
> * 如你更偏好严格可复现，也可在跑通后执行 `pip freeze > requirements.lock.txt` 保存一次性锁定版本。

### 2.2.2 方案 B：`uv`（更快的包管理器）

```bash
# 安装 uv（Linux/Mac 常用；Windows 可用 winget 或 pipx）
curl -LsSf https://astral.sh/uv/install.sh | sh
# 创建并激活虚拟环境
uv venv .venv
source .venv/bin/activate
# 直接安装依赖
uv pip install -r requirements.txt
```

或直接使用 **`pyproject.toml`** 管理依赖（`uv` 会读取）：

```toml
[project]
name = "mimic-aki-duckdb"
version = "0.1.0"
requires-python = ">=3.10,<3.12"
dependencies = [
  "duckdb>=0.10,<2.0",
  "pandas>=2.2,<3.0",
  "polars>=0.20,<1.0",
  "pyarrow>=15,<20",
  "scikit-learn>=1.4,<2.0",
  "matplotlib>=3.8,<4.0",
  "jupyterlab>=4.0,<5.0",
  "ipykernel>=6.29,<7.0",
  "tqdm>=4.66,<5.0",
  "python-dotenv>=1.0,<2.0",
]
```

### 2.2.3 为 Jupyter 注册 Kernel（很重要）

```bash
python -m ipykernel install --user --name mimic-aki-duckdb --display-name "Python (mimic-aki-duckdb)"
```

之后在 VSCode 的 Notebook 右上角**选择该内核**，确保所有代码运行在你新建的环境里。

---

## 2.3 安装与验证：duckdb、pandas/Polars、pyarrow、matplotlib/scikit-learn

> 我们用一段**可直接粘贴到新 Notebook** 的“环境体检”代码，验证库是否安装成功，并以**无 SQL**的方式完成一次读写与小规模运算。

### 2.3.1 环境体检（版本打印）

```python
import sys, platform
import importlib

pkgs = ["duckdb","pandas","polars","pyarrow","sklearn","matplotlib"]
print("Python:", sys.version)
print("Platform:", platform.platform())

for p in pkgs:
    try:
        m = importlib.import_module(p)
        ver = getattr(m, "__version__", "n/a")
        print(f"OK  {p:10s} {ver}")
    except Exception as e:
        print(f"ERR {p:10s} -> {e}")
```

### 2.3.2 无 SQL 小试牛刀（CSV→DataFrame→聚合→Parquet）

> 思路：优先使用 **Polars/Pandas** 完成计算；DuckDB 在这里主要作为轻量、高效的列存**读写引擎与互操作层**（后续需要时也可把 DataFrame 注册到 DuckDB 再做更大的 Join，但我们**不直接写 SQL**）。

```python
from pathlib import Path
import pandas as pd
import polars as pl
import duckdb
import pyarrow as pa
import pyarrow.parquet as pq

DATA_ROOT = Path("./data")
RAW = DATA_ROOT/"raw"
INTERIM = DATA_ROOT/"interim"
DERIVED = DATA_ROOT/"derived"
for d in [RAW, INTERIM, DERIVED]:
    d.mkdir(parents=True, exist_ok=True)

# 1) 构造/检测示例 CSV（若你已有 MIMIC CSV，可替换路径：如 RAW/"mimiciv_hosp/patients.csv.gz"）
toy_csv = RAW/"toy_patients.csv"
if not toy_csv.exists():
    pd.DataFrame({
        "subject_id":[1,2,3,4,5],
        "age":[65,72,58,91,43],
        "sex":["M","F","M","F","M"]
    }).to_csv(toy_csv, index=False)

# 2) 以 Polars 扫描（支持大文件惰性执行），完全无 SQL
lf = pl.scan_csv(toy_csv)
# 统计：年龄分布与性别计数（示例）
age_stats = lf.select([
    pl.col("age").min().alias("age_min"),
    pl.col("age").max().alias("age_max"),
    pl.col("age").mean().round(2).alias("age_mean")
]).collect()
print(age_stats)

sex_counts = lf.groupby("sex").count().collect()
print(sex_counts)

# 3) 将结果写入 Parquet（列存，利于后续快速加载）
out_parquet = INTERIM/"toy_patients.parquet"
sex_counts.to_pandas().to_parquet(out_parquet, index=False)
print("Parquet saved to:", out_parquet)

# 4) DuckDB 读取 Parquet/CSV（不写 SQL，直接 Relation API）
con = duckdb.connect()
rel_parquet = con.read_parquet(str(out_parquet))
print("DuckDB rows:", rel_parquet.df().shape)

# （可选）将 pandas/Polars DataFrame 注册给 DuckDB 以便后续更大规模 join
con.register("patients_df", sex_counts.to_pandas())
print("Registered 'patients_df' into DuckDB (no SQL used).")
```

### 2.3.3 可视化与建模快速 Smoke Test

```python
# 可视化（Matplotlib）
import matplotlib.pyplot as plt
pdf = sex_counts.to_pandas()
plt.figure()
plt.bar(pdf["sex"], pdf["count"])
plt.title("Toy: Sex Counts")
plt.xlabel("sex")
plt.ylabel("count")
plt.show()

# 建模（scikit-learn，小样本只是验证库）
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# 伪造一个二分类目标（仅用于验证流程）
X = pd.read_csv(toy_csv)
X["label"] = (X["age"]>=60).astype(int)
X_train, X_test = train_test_split(X, test_size=0.4, random_state=42, stratify=X["label"])
pre = ColumnTransformer([("cat","passthrough",["sex"]),("num","passthrough",["age"])], remainder="drop")
clf = Pipeline([("pre", pre), ("lr", LogisticRegression(max_iter=200))])
clf.fit(X_train[["sex","age"]], X_train["label"])
proba = clf.predict_proba(X_test[["sex","age"]])[:,1]
print("Smoke AUROC:", roc_auc_score(X_test["label"], proba))
```

> 如果以上三段代码全部正常（版本打印、数据读写、可视化+简易建模），你的环境就**可用于后续处理 MIMIC-IV** 了。

---

## 2.4 目录规范与数据落盘：`/data/raw`、`/data/interim`、`/data/derived`、`/notebooks`、`/src`

> 统一的目录与命名能显著提升团队协作与复现性。下面是建议骨架：

```
project-root/
├─ data/
│  ├─ raw/              # 原始 CSV(.gz)：只读；不修改不覆盖
│  │   ├─ mimiciv_hosp/  (patients.csv.gz, admissions.csv.gz, labevents.csv.gz, ...)
│  │   └─ mimiciv_icu/   (icustays.csv.gz, chartevents.csv.gz, ...)
│  ├─ interim/          # 中间结果（清洗后、抽样、拆列、对齐单位），Parquet优先
│  └─ derived/          # 特征矩阵、模型输入输出（train/valid/test）
├─ notebooks/
│  ├─ 00_env_check.ipynb
│  ├─ 01_schema_eda.ipynb
│  ├─ 10_aki_label.ipynb
│  ├─ 11_features.ipynb
│  └─ 20_model_eval.ipynb
├─ src/
│  ├─ configs/
│  │   └─ base.yaml      # 路径、时间窗、特征开关等
│  ├─ utils/
│  │   ├─ io.py          # 统一读写CSV/Parquet；路径解析；gzip透明支持
│  │   └─ clock.py       # 时间窗/锚点工具、相对时间计算
│  └─ features/
│      ├─ labs.py        # 化验特征工程（0–24h聚合）
│      └─ vitals.py      # 生命体征聚合（可选）
├─ .env                  # 可选：DATA_ROOT 等环境变量（别提交）
├─ .gitignore            # 忽略 data/**, .env, .ipynb_checkpoints/** ...
├─ requirements.txt      # 或 pyproject.toml
└─ README.md
```

### 2.4.1 数据读写约定

* **只往 `raw/` 放原始文件**（`*.csv` 或 `*.csv.gz`），不要编辑或覆盖。
* 清洗/抽样/转换输出写到 **`interim/`（中间层）**；特征与模型输入输出写到 **`derived/`**。
* **优先使用 Parquet** 作为中间层格式（列存 + 压缩 + schema + 快速），CSV 主要用于原始数据或最终导出示例。
* 在 `src/utils/io.py` 中写一套**通用 I/O**：

  * `read_csv_auto(path, usecols=None, dtypes=None)`：自动识别 `.gz`；优先用 Polars `scan_csv`/`read_csv`；必要时降级 pandas。
  * `write_parquet(df, path)`：统一使用 pyarrow 或 Polars。
  * `ensure_dir(path)`：目录存在性保障。
  * `find_mimic_file(kind, table)`：如 `("hosp","patients") → data/raw/mimiciv_hosp/patients.csv.gz`。

### 2.4.2 VSCode 与 Notebook 规范

* 每个 Notebook 顶部必须包含**研究用途与合规声明**（参见第 1 章）。
* **参数化**：从 `src/configs/base.yaml` 载入路径和开关，Notebook 里仅做最小逻辑。
* **输出约定**：每个 Notebook 输出到 `interim/` 或 `derived/` 对应子目录，并写入 `manifest.json` 记录：输入文件哈希/行数、运行时间、参数快照、代码版本。

### 2.4.3 .gitignore 与敏感信息

* `.gitignore` 必须包含：

  ```
  data/**
  .env
  .ipynb_checkpoints/**
  **/__pycache__/**
  ```
* **绝不提交任何受限数据**；`.env`（若使用）仅用于本地路径或私密配置（不要把密钥/凭据写入代码）。

---

## 本章完成度自检（Checklist）

* [ ] VSCode 装好并能打开 Notebook；已安装 Python/Jupyter/Pylance/Remote-SSH 等扩展
* [ ] 远程主机（若有）可通过 Remote-SSH 连接，并能在远程创建/选择内核
* [ ] 创建了虚拟环境（venv 或 uv），并成功安装依赖
* [ ] 运行了 **环境体检** Notebook：版本打印、Polars 扫描 CSV、写 Parquet、DuckDB 读取、简易可视化+建模
* [ ] 初始化了项目目录骨架，`raw/interim/derived/notebooks/src` 就位
* [ ] `.gitignore` 与 `README.md`（含合规声明）已就位

---

如果这章内容 OK，我们就进入 **第 3 章：DuckDB 的“无 SQL”使用范式**——会给出**纯数据帧**风格的读写、延迟扫描、列裁剪、CSV→Parquet 渐进转换示例，以及与 pandas/Polars 的高效互操作模板。需要我顺手把 `src/utils/io.py` 与 `configs/base.yaml` 的最小可用样例也写出来吗？
