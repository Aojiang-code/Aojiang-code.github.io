下面是**第10章：数据集成与特征矩阵构建**的详细学习笔记。
全程遵循你的技术约束：**DuckDB + CSV/Parquet + Python（pandas/Polars）+ Jupyter（VSCode）**，**不使用 SQL / Postgres**。示例均为**数据帧工作流**范式，可直接嵌入 Notebook 运行。

---

# 第10章 数据集成与特征矩阵构建

> 目标：把前几章产物（cohort、标签、基础/临床特征）**在 `stay_id` 粒度**整合为**单行样本表**，完成列裁剪与类型统一，做基础异常值处理与（可选）稳健缩放预处理（仅拟合于训练集），并**版本化**保存为 Parquet + 快照清单，供建模直接使用。

---

## 10.1 以 `stay_id` 为索引的样本表

**输入约定（来自第7–9章）：**

* `derived/cohort_base.parquet`：`subject_id, hadm_id, stay_id, intime, outtime, first_careunit, ...`
* `derived/label_24_48h.parquet`：`stay_id, aki_label, aki_stage`
* `derived/features_basic_0_24h.parquet`：人口学/就诊/合并症（第8章）
* `derived/features_clinical_0_24h.parquet`：化验/生命体征/尿量/药物（第9章）

**整合步骤**：确保**1 个 `stay_id` = 1 行**；若存在重复/冲突列，使用前缀与后缀避免覆盖，最终只保留**24h 前可得**的特征列。

```python
from pathlib import Path
import polars as pl

DER = Path("data/derived"); DER.mkdir(parents=True, exist_ok=True)
INTERIM = Path("data/interim"); INTERIM.mkdir(parents=True, exist_ok=True)

cohort = pl.read_parquet(DER/"cohort_base.parquet") \
           if (DER/"cohort_base.parquet").exists() else None
labels = pl.read_parquet(DER/"label_24_48h.parquet")

X_basic   = pl.read_parquet(DER/"features_basic_0_24h.parquet")
X_clinic  = pl.read_parquet(DER/"features_clinical_0_24h.parquet")

# 1) 起点：有标签的 stay_id（避免无标签样本混入监督训练）
base = labels.unique(subset=["stay_id"])

# 2) 逐块左连接特征（确保 stay_id 唯一）
def left_join_unique(a: pl.DataFrame, b: pl.DataFrame, on="stay_id") -> pl.DataFrame:
    # 若 b 中 on 键重复，先聚合为首行（理论上不应重复；这里是保护）
    if b.select(on).n_unique() != b.height:
        b = b.groupby(on).agg([pl.all().first()])
        b = b.explode(pl.all().exclude(on))
    return a.join(b, on=on, how="left")

X = left_join_unique(base, X_basic)
X = left_join_unique(X,    X_clinic)

# 3) （可选）与 cohort 贴合可解释字段（但不要把未来信息留在特征里）
if cohort is not None:
    X = X.join(cohort.select(["stay_id","subject_id","hadm_id","intime","first_careunit"]),
               on="stay_id", how="left")

# 4) 断言：stay_id 唯一
assert X.select(["stay_id"]).n_unique() == X.height, "stay_id 非唯一，请检查上游特征生成。"

X.head()
```

> 提示：`intime/outtime/dischtime` 等**未来或标签窗相关信息**仅用于**筛选/审计**，不要作为特征列参与建模（第7章已强调）。

---

## 10.2 列裁剪、类型统一、异常值处理与稳健缩放

### 10.2.1 列裁剪与命名规范

* **保留键与标签**：`stay_id, subject_id, hadm_id, aki_label, aki_stage`
* **丢弃**：任何越窗/未来列（如 `outtime`, `dischtime`, `deathtime`）、重复辅助列、中间调试列（如 `_rank`, 临时计数）。
* **可选裁剪**：高缺失率列（如缺失 > 95%）、常数列、极端稀有二元列（阳性 < 0.5%）。
* **命名约定**：统一小写，单位/窗口写进后缀：`_mean__0_24h`、`_last__0_24h` 等。

```python
# 1) 指定应剔除的“未来/辅助/中间”列前缀或名称
drop_exact = {"outtime","dischtime","deathtime"}  # 安全起见，出现就删
drop_prefix = ("tmp_", "_tmp", "_debug", "_rank", "intime_offset_")  # 示例

cols = [c for c in X.columns if c not in drop_exact and not any(c.startswith(p) for p in drop_prefix)]
X = X.select(cols)

# 2) 常量列与高缺失列（阈值可配置）
MISS_THR = 0.95
def _missing_rate(df: pl.DataFrame):
    n = df.height
    return {c: float(df[c].is_null().sum())/max(n,1) for c in df.columns}

miss = _missing_rate(X)
const_cols = [c for c in X.columns if X[c].n_unique() <= 1 and c not in ("aki_label","aki_stage","stay_id","hadm_id","subject_id")]
high_miss = [c for c,r in miss.items() if r >= MISS_THR and c not in ("aki_label","aki_stage","stay_id","hadm_id","subject_id")]

X = X.drop(const_cols + high_miss)
```

> 建议：**裁剪阈值在训练集上决定**；若此处尚未切分，可先不删，仅记录并在建模管道中基于训练折执行。

### 10.2.2 类型统一（轻量数值化）

* **布尔/0-1 列** → `Int8`
* **计数列** → `Int32`（如 `_n__0_24h`）
* **连续数值** → `Float32`（减小内存；树模型足够）
* **类别列**（如 `first_careunit`）暂保留为字符串，后续在建模管道里做 One-Hot 或目标编码。

```python
# 轻量类型转换
int8_candidates  = [c for c in X.columns if c.endswith("__missing") or c.endswith("_used__0_24h") or c.startswith("sex_") or c.startswith("admtype_")]
count_candidates = [c for c in X.columns if c.endswith("_n__0_24h")]
float_candidates = [c for c in X.columns if (c not in int8_candidates + count_candidates + ["stay_id","hadm_id","subject_id","aki_label","aki_stage","first_careunit"]) and X[c].dtype in (pl.Float64, pl.Int64, pl.Int32, pl.Float32)]

X = X.with_columns([
    *[pl.col(c).cast(pl.Int8)   for c in int8_candidates if c in X.columns],
    *[pl.col(c).cast(pl.Int32)  for c in count_candidates if c in X.columns],
    *[pl.col(c).cast(pl.Float32) for c in float_candidates if c in X.columns],
])
```

### 10.2.3 异常值处理（医学“安全剪裁”）

* **策略**：**剪裁而非删除**（winsorize/clipping），避免因异常值丢失样本。
* **建议医学范围（可仅作示例，实际以数据分布/临床共识为准）**：

  * `hr`：20–250；`rr`：4–80；`temp_c`：25–45；`spo2`：0–100
  * `sbp`：40–300；`dbp`：20–200；`map`：20–200
  * `scr`：0–20 mg/dL；`bun`：0–200 mg/dL
  * `na`：100–180 mmol/L；`k`：1–10；`cl`：60–140；`hco3`：0–60 mmol/L
  * `uo_mlkgph__0_24h`：0–20（极端上限；按人群可调整）

```python
# 通配剪裁器：按列名包含关键前缀批量裁剪
def clip_by_prefix(df: pl.DataFrame, rules: dict[str, tuple[float,float]]) -> pl.DataFrame:
    out = df
    for prefix, (lo, hi) in rules.items():
        for c in [col for col in df.columns if col.startswith(prefix)]:
            if df[c].dtype in (pl.Float32, pl.Float64, pl.Int32, pl.Int64):
                out = out.with_columns(pl.col(c).clip(lo, hi))
    return out

CLIP_RULES = {
    "hr_":   (20, 250),
    "rr_":   (4, 80),
    "temp_c_": (25, 45),
    "spo2_": (0, 100),
    "sbp_":  (40, 300),
    "dbp_":  (20, 200),
    "map_":  (20, 200),
    "scr_":  (0, 20),
    "bun_":  (0, 200),
    "na_":   (100, 180),
    "k_":    (1, 10),
    "cl_":   (60, 140),
    "hco3_": (0, 60),
}
X = clip_by_prefix(X, CLIP_RULES)
# 特殊列（非前缀命名）
if "uo_mlkgph__0_24h" in X.columns:
    X = X.with_columns(pl.col("uo_mlkgph__0_24h").clip(0, 20))
```

> 这是“安全护栏”，不是数据校正。更细致的异常检测（如按分布分位剪裁）可在训练集上拟合后执行。

### 10.2.4 稳健缩放（仅在训练集拟合）

* **为什么**：许多模型（逻辑回归、SVM、kNN）对尺度敏感；树模型不强制需要。
* **做法**：在**训练集**上拟合 `RobustScaler`（按中位数/IQR），应用到验证/测试。
* **位置**：放在**建模管道**（第11章）。本章仅记录**数值列清单**，不在全量上拟合以避免泄漏。

```python
# 记录数值列（除去 ID/分组/标签/明显类别列）
NUM_COLS = [c for c in X.columns if c not in ("stay_id","subject_id","hadm_id","aki_label","aki_stage","first_careunit") and X[c].dtype in (pl.Float32, pl.Float64, pl.Int32, pl.Int64)]
CAT_COLS = [c for c in ("first_careunit",) if c in X.columns]

# 保存列清单，供第11章的 Pipeline 使用
import json
with open(DER/"feature_columns.json","w",encoding="utf-8") as f:
    json.dump({"num_cols": NUM_COLS, "cat_cols": CAT_COLS}, f, indent=2, ensure_ascii=False)
```

---

## 10.3 写入中间数据（Parquet）与版本化（数据快照）

**原则**：任何用于训练/评估的数据快照都应**可复现**。我们将写 Parquet，并生成一个**快照清单**（记录输入来源、行列数、缺失率、时间戳与校验和）。

### 10.3.1 版本化命名与软链接

* 文件名包含日期时间：`model_table_YYYYMMDD_HHMM.parquet`
* 维护一个 `latest.parquet` 软链接/副本，便于默认加载（若环境不方便建软链，则复制一份）。

```python
from datetime import datetime
ts = datetime.now().strftime("%Y%m%d_%H%M")
out_parquet = DER/f"model_table_{ts}.parquet"
X.write_parquet(out_parquet)

# 可选：写一份统一命名
(X.sort("stay_id")
   .select(["stay_id","aki_label","aki_stage"] + [c for c in X.columns if c not in ("stay_id","aki_label","aki_stage")])
   .write_parquet(DER/"model_table_latest.parquet"))
```

### 10.3.2 快照清单（JSON）

记录：输入文件名与大小、行列数、缺失率摘要、数值列的基本分布、列类型、生成时间与代码版本（如 Git commit）。

```python
import os, json, hashlib

def file_info(path: Path):
    h = hashlib.md5(path.read_bytes()).hexdigest() if path.exists() else None
    return {"path": str(path), "exists": path.exists(), "size_bytes": path.stat().st_size if path.exists() else None, "md5": h}

snapshot = {
  "timestamp": ts,
  "outputs": {"parquet": str(out_parquet)},
  "inputs": {
    "features_basic":  file_info(DER/"features_basic_0_24h.parquet"),
    "features_clinic": file_info(DER/"features_clinical_0_24h.parquet"),
    "labels":          file_info(DER/"label_24_48h.parquet"),
    "cohort":          file_info(DER/"cohort_base.parquet"),
  },
  "shape": {"rows": X.height, "cols": X.width},
  "missing_rate": {c: float(X[c].is_null().sum())/max(X.height,1) for c in X.columns[:200]},  # 仅前200列做摘要，避免 JSON 过大
  "dtypes": {c: str(X[c].dtype) for c in X.columns},
  "num_cols": NUM_COLS,
  "cat_cols": CAT_COLS
}
with open(DER/f"snapshot_{ts}.json","w",encoding="utf-8") as f:
    json.dump(snapshot, f, indent=2, ensure_ascii=False)
```

> 你也可以引入 **DVC/Git-LFS** 管理大文件；或自行维护 `snapshots/` 目录，保存历次快照与清单，保证**任何一次实验都可重现**所用数据。

---

## 本章产出与自检

**文件产出**

* `derived/model_table_{YYYYMMDD_HHMM}.parquet`（主表：1 行/`stay_id`）
* `derived/model_table_latest.parquet`（便捷加载）
* `derived/feature_columns.json`（数值/类别列清单）
* `derived/snapshot_{YYYYMMDD_HHMM}.json`（快照清单）

**自检清单**

* [ ] `stay_id` 唯一、与标签一一对应
* [ ] 无越窗/未来信息作为特征（`outtime/dischtime/deathtime` 已剔除）
* [ ] 常量列与超高缺失列处理策略明确（最好在训练集上决定）
* [ ] 类型统一：二元 `Int8`、计数 `Int32`、连续 `Float32`；类别列单独列出
* [ ] 异常值采用**剪裁**而非删除；医学范围可审计与复用
* [ ] 已写 Parquet 与快照 JSON，记录了输入来源与列清单

---

### 下一步（第11章预告）

* 构建 **训练 Pipeline**：基于 `feature_columns.json`，在**训练集**上拟合**缺失插补 +（可选）稳健缩放 + 类别编码**，并训练首个基线模型（Logistic/XGBoost）。
* 统一评估脚本：**分层分组切分**（第7章）、**PR-AUC/ROC-AUC**、**校准曲线**、**特征重要性/SHAP**。
