下面是**第5章：CSV 数据准备与质量校验**的详细学习笔记。
全章坚持你的技术约束：**DuckDB + CSV/Parquet + Python（pandas/Polars）+ Jupyter（VSCode）**，**不使用 SQL / Postgres**。示例均为**数据帧工作流**范式，可直接嵌入后续 Notebook。

---

# 第5章 CSV 数据准备与质量校验

> 目标：把原始 MIMIC-IV CSV(.gz) 有序落盘、量化体量与磁盘需求、完成完整性与时间戳校验、做一轮基础 EDA，并把**关键字段与单位**对齐，为后续 **CSV→Parquet 渐进转换** 与特征工程打基础。

## 5.1 文件清单、体量评估与磁盘规划

### 5.1.1 目录约定与快速盘点

```
project/
├─ data/
│  ├─ raw/
│  │   ├─ mimiciv_hosp/*.csv.gz
│  │   └─ mimiciv_icu/*.csv.gz
│  ├─ interim/      # 质检后的中间结果（Parquet）
│  └─ derived/      # 特征矩阵/模型输入（Parquet）
└─ notebooks/
```

**Notebook 片段：扫描文件清单与体量（gz/预估解压/Parquet 预算）**

```python
from pathlib import Path
import gzip, os, math, json
import polars as pl

RAW = Path("data/raw")
INTERIM = Path("data/interim"); INTERIM.mkdir(parents=True, exist_ok=True)

def list_csv_gz(root: Path):
    return sorted([p for p in root.rglob("*.csv.gz")])

def estimate_uncompressed_size_gb(gz_path: Path, ratio: float = 3.0):
    # 粗估：gzip 压缩比常见在 2–6x，默认用 3x 做上界保守预算
    return (gz_path.stat().st_size * ratio) / (1024**3)

def human_gb(x): return f"{x:.2f} GB"

files = list_csv_gz(RAW)
rows_preview = []
for f in files:
    # 仅取前若干行探测列名与样本
    df_head = pl.read_csv(f, n_rows=200, ignore_errors=True)
    rows_preview.append({
        "file": str(f.relative_to(RAW)),
        "cols": list(df_head.columns),
        "n_cols": len(df_head.columns),
        "size_gz": human_gb(f.stat().st_size / (1024**3)),
        "est_unzipped": human_gb(estimate_uncompressed_size_gb(f)),
    })

pl.DataFrame(rows_preview)
```

**规划建议**

* **磁盘冗余**：为“解压 + 中间 Parquet + 缓存”预留\*\*≥3×\*\* gzip 总量的空间。
* **优先 Parquet**：中间层落 Parquet（列存 + 压缩 + 统计），后续扫描快、体积小（常较解压 CSV 再降 3–6 倍）。
* **分阶段转换**：先转关键表（`patients/admissions/icustays/labevents` 的子集）→ 验证 → 再扩展到 `chartevents` 等巨表。

---

## 5.2 完整性检查：字段、行数、去重、异常时间戳

### 5.2.1 字段与行数校验（最小可行）

```python
import polars as pl
from datetime import datetime

REQUIRED = {
  "mimiciv_hosp/patients.csv.gz":    ["subject_id","gender","anchor_year","anchor_year_group"],
  "mimiciv_hosp/admissions.csv.gz":  ["subject_id","hadm_id","admittime","dischtime"],
  "mimiciv_icu/icustays.csv.gz":     ["subject_id","hadm_id","stay_id","intime","outtime"],
  "mimiciv_hosp/labevents.csv.gz":   ["hadm_id","itemid","charttime","valuenum","valueuom"],
}

def check_required_columns(raw_root=RAW):
    reports = []
    for rel, cols in REQUIRED.items():
        path = raw_root/rel
        head = pl.read_csv(path, n_rows=200, ignore_errors=True)
        missing = [c for c in cols if c not in head.columns]
        reports.append({"file": rel, "missing_cols": missing})
    return pl.DataFrame(reports)

check_required_columns()
```

* **通过标准**：`missing_cols` 为空。
* **失败处理**：检查下载是否完整、文件是否放对目录、版本是否匹配。

### 5.2.2 去重策略（键级别）

* `patients`：按 `subject_id` 去重。
* `admissions`：按 `hadm_id` 去重（同一住院不应重复）。
* `icustays`：按 `stay_id` 去重。
* `labevents/chartevents`（事件）：同一键（如 `hadm_id/stay_id + itemid + charttime + valuenum`）如重复，**聚合前去重**或使用稳健聚合（median/last）。

```python
def dedup_key(df: pl.DataFrame, keys: list[str]) -> pl.DataFrame:
    return df.unique(subset=keys, keep="last")

# patients 去重示例
pat = pl.scan_csv(RAW/"mimiciv_hosp/patients.csv.gz").collect()
pat = dedup_key(pat, ["subject_id"])
```

### 5.2.3 时间戳异常与跨表一致性

**常见异常**

* `intime > outtime`、`admittime > dischtime`（时间颠倒）
* 事件发生时间 `charttime` 不在住院/ICU 范围附近（可能在很久之前/之后）
* `storetime` 与 `charttime` 相差极大（数据质量/录入延迟）

**检查模板**

```python
# ICU 停留时间序列一致性
icu = pl.scan_csv(RAW/"mimiciv_icu/icustays.csv.gz").select(["stay_id","hadm_id","intime","outtime"]).collect()
viol_icustay = icu.filter(pl.col("intime") >= pl.col("outtime"))
print("ICU time violations:", viol_icustay.height)

# 事件落窗检查（以 labevents 为例，是否远离住院区间）
adm = pl.scan_csv(RAW/"mimiciv_hosp/admissions.csv.gz").select(["hadm_id","admittime","dischtime"]).collect()
lab = (
    pl.scan_csv(RAW/"mimiciv_hosp/labevents.csv.gz")
      .select(["hadm_id","charttime"])
      .filter(pl.col("charttime").is_not_null())
      .collect(streaming=True)
)
lab_chk = (
    lab.join(adm, on="hadm_id", how="left")
       .with_columns((pl.col("charttime") < pl.col("admittime") - pl.duration(hours=48) |
                      pl.col("charttime") > pl.col("dischtime") + pl.duration(hours=48)).alias("far_out"))
)
lab_off = lab_chk.filter(pl.col("far_out"))
print("Lab far-out events (±48h):", lab_off.height)
```

> 注：由于 MIMIC 有**患者级时间偏移**，同一患者内相对时间可靠；跨患者绝对日历不可比。检查也应以相对窗口为准（如住院前后 ±48h）。

---

## 5.3 基础 EDA：患者计数、住院计数、ICU 停留、化验与生命体征分布

### 5.3.1 粗粒度规模与唯一计数

```python
# 患者/住院/ICU 停留规模
pat = pl.scan_csv(RAW/"mimiciv_hosp/patients.csv.gz").select(["subject_id"]).collect()
adm = pl.scan_csv(RAW/"mimiciv_hosp/admissions.csv.gz").select(["hadm_id","subject_id"]).collect()
icu = pl.scan_csv(RAW/"mimiciv_icu/icustays.csv.gz").select(["stay_id","hadm_id","subject_id"]).collect()

summary = {
    "n_patients": pat["subject_id"].n_unique(),
    "n_admissions": adm["hadm_id"].n_unique(),
    "n_icustays": icu["stay_id"].n_unique(),
    "admissions_per_patient_median": adm.groupby("subject_id").count()["count"].median(),
    "icustays_per_admission_median": icu.groupby("hadm_id").count()["count"].median()
}
summary
```

### 5.3.2 住院与 ICU 时长分布

```python
# 住院时长（天）
adm_len = (
    pl.scan_csv(RAW/"mimiciv_hosp/admissions.csv.gz")
      .select(["admittime","dischtime"])
      .with_columns((pl.col("dischtime") - pl.col("admittime")).dt.seconds() / 86400.0
                    .alias("los_days"))
      .collect()
)
adm_len.describe()

# ICU 时长（小时）
icu_len = (
    pl.scan_csv(RAW/"mimiciv_icu/icustays.csv.gz")
      .select(["intime","outtime"])
      .with_columns((pl.col("outtime") - pl.col("intime")).dt.seconds() / 3600.0
                    .alias("icu_hours"))
      .collect()
)
icu_len.describe()
```

### 5.3.3 化验/生命体征覆盖度与分布

**化验覆盖度（itemid 频次 Top-N）**

```python
D_LAB = RAW/"mimiciv_hosp/d_labitems.csv.gz"
LAB   = RAW/"mimiciv_hosp/labevents.csv.gz"
d_lab = pl.scan_csv(D_LAB).select(["itemid","label"]).collect()
lab_counts = (
    pl.scan_csv(LAB)
      .select(["itemid"])
      .groupby("itemid").count()
      .sort("count", descending=True)
      .head(50)
      .join(d_lab, on="itemid", how="left")
      .collect()
)
lab_counts
```

**生命体征覆盖度（`chartevents` Top-N；可先筛选“心率/血压/体温/SpO₂”相关标签）**

```python
D_ITEMS = RAW/"mimiciv_icu/d_items.csv.gz"
CHART   = RAW/"mimiciv_icu/chartevents.csv.gz"
d_items = pl.scan_csv(D_ITEMS).select(["itemid","label","unitname"]).collect()
chart_counts = (
    pl.scan_csv(CHART)
      .select(["itemid"])
      .groupby("itemid").count()
      .sort("count", descending=True)
      .head(50)
      .join(d_items, on="itemid", how="left")
      .collect()
)
chart_counts
```

**数值分布快照（以 SCr/BUN 为例）**

```python
# 先找到肌酐/尿素氮 itemid（第4章示例可复用）
targets = (
    d_lab.with_columns(pl.col("label").str.to_lowercase())
         .filter(pl.col("label").str.contains("creatin") | pl.col("label").str.contains(r"\bbun\b"))
         .select(["itemid","label"])
)
ids = targets["itemid"].unique().to_list()

dist = (
    pl.scan_csv(LAB)
      .select(["itemid","valuenum","valueuom"])
      .filter(pl.col("itemid").is_in(ids))
      .filter(pl.col("valuenum").is_not_null())
      .collect(streaming=True)
)
dist.groupby(["itemid","valueuom"]).agg([
    pl.col("valuenum").mean().alias("mean"),
    pl.col("valuenum").median().alias("median"),
    pl.col("valuenum").quantile(0.25).alias("q25"),
    pl.col("valuenum").quantile(0.75).alias("q75"),
    pl.col("valuenum").min().alias("min"),
    pl.col("valuenum").max().alias("max"),
    pl.count().alias("n")
]).sort(["itemid","n"], descending=[False, True])
```

---

## 5.4 数据字典对齐与单位规范（SCr、尿量、电解质等）

> 目的：构建**统一单位的干净数值**，为后续窗口聚合/建模提供一致输入。优先以 `d_labitems`/`d_items` 作为**唯一映射**；对多单位项目做**标准化**。

### 5.4.1 化验字典映射与“多单位”检测

```python
# itemid → label/unitname 以及单位多样性检查
lab_unit_profile = (
    pl.scan_csv(LAB)
      .select(["itemid","valueuom"])
      .groupby(["itemid","valueuom"]).count()
      .join(d_lab, on="itemid", how="left")
      .collect()
)
# 找出同一 itemid 有多个单位的情况
multi_unit = (
    lab_unit_profile.groupby("itemid").agg(pl.count().alias("n_units")).filter(pl.col("n_units")>1)
)
multi_unit.join(d_lab, on="itemid", how="left").sort("itemid")
```

**处理策略**

* **单单位**：直接使用；
* **多单位**：建立转换函数（按 `valueuom` 条件转换），物化为统一单位字段（如 `valuenum_std`）。

### 5.4.2 常见换算与规范（示例）

* **肌酐（SCr）**：`mg/dL = μmol/L ÷ 88.4`
* **尿素氮（BUN）**：`mg/dL ↔ mmol/L`，换算 `mmol/L = mg/dL × 0.357`；反向 `mg/dL = mmol/L ÷ 0.357 ≈ mmol/L × 2.80`
* **电解质（Na⁺/K⁺/Cl⁻）**：`mmol/L` 与 `mEq/L` 在一价离子下数值相等（通常直接使用 `mmol/L` 标准）。
* **HCO₃⁻（碳酸氢根）**：常用 `mmol/L`。
* **尿量**：以 **mL** 为基础，按需标准化为 **mL/kg/h**（需体重，见 OMR 或 ICU 体重记录），后续 AKI 尿量标准用 `<0.5 mL/kg/h` 的时间窗阈值（第6章会细化）。

**标准化示例：SCr + BUN**

```python
def standardize_lab_units(df: pl.DataFrame, label_col="label"):
    df = df.with_columns(pl.col(label_col).str.to_lowercase())
    is_crea = pl.col(label_col).str.contains("creatin")
    is_bun  = pl.col(label_col).str.contains(r"\bbun\b")
    # creatinine: μmol/L → mg/dL
    df = df.with_columns(
        pl.when(is_crea & (pl.col("valueuom").str.to_lowercase().is_in(["µmol/l","umol/l","μmol/l"])))
          .then(pl.col("valuenum") / 88.4)
          .when(is_crea & (pl.col("valueuom").str.to_lowercase().is_in(["mg/dl"])))
          .then(pl.col("valuenum"))
          .otherwise(pl.col("valuenum"))
          .alias("valuenum_std")
    )
    # BUN: mmol/L → mg/dL
    df = df.with_columns(
        pl.when(is_bun & (pl.col("valueuom").str.to_lowercase()=="mmol/l"))
          .then(pl.col("valuenum") / 0.357)
          .when(is_bun & (pl.col("valueuom").str.to_lowercase()=="mg/dl"))
          .then(pl.col("valuenum"))
          .otherwise(pl.col("valuenum_std"))  # 维持已有
          .alias("valuenum_std")
    )
    return df

# 应用：先把 itemid→label 拼上再转换
lab_core = (
    pl.scan_csv(LAB)
      .select(["hadm_id","itemid","charttime","valuenum","valueuom"])
      .filter(pl.col("valuenum").is_not_null())
      .collect(streaming=True)
      .join(d_lab.select(["itemid","label"]).unique(), on="itemid", how="left")
)
lab_std = standardize_lab_units(lab_core)
```

### 5.4.3 尿量与体重（可做 mL/kg/h）

* 体重来源：`hosp.omr`（体重/身高/BMI）或 ICU 记录中的体重项（在 `chartevents/d_items` 查找 “Weight”），选择**接近 ICU 入科时**的体重。
* 计算：在目标时间窗（如 6h/12h/24h）累加尿量（mL），除以（体重 kg × 小时数）→ 得到 mL/kg/h。
* 质量门槛：极端小/大值过滤（如负数/不可能的大于几万 mL/日）。

```python
# 体重近似（示例：从 OMR 取最近一次）
OMR = RAW/"mimiciv_hosp/omr.csv.gz"
omr = pl.scan_csv(OMR).select(["subject_id","chartdate","result_name","result_value","result_unit"]).collect()
weight = (
    omr.filter(pl.col("result_name").str.to_lowercase().str.contains("weight"))
       .with_columns(pl.col("result_value").cast(pl.Float64))
       .drop_nulls(["result_value"])
       .groupby("subject_id").agg(pl.col("result_value").last().alias("weight_kg"))  # 简化示例
)

# 尿量聚合（示例：chartevents/或 derived 表中的尿量项；这里只演示接口）
# ur_df: ["stay_id","charttime","urine_ml"]
# 在给定窗长 h 内，聚合为总尿量 / (weight*窗长)
```

### 5.4.4 统一输出与“质检清单”物化

将清洗/标准化后的**关键子集**物化为 Parquet，并生成**质检报告**（JSON），包含：

* 文件名、行数、唯一键计数；
* 字段缺失率；
* 时间范围（min/max `charttime`）；
* 单位多样性摘要；
* 极值/分位数摘要（用于后续异常检测阈值）。

```python
import json, numpy as np
OUT = INTERIM/"qc_reports"; OUT.mkdir(parents=True, exist_ok=True)

def missing_rate(df: pl.DataFrame):
    n = df.height
    return {c: float((df[c].is_null()).sum())/n for c in df.columns}

qc = {
  "labevents_core": {
     "rows": lab_std.height,
     "cols": lab_std.width,
     "missing_rate": missing_rate(lab_std.select(["valuenum_std","valueuom","charttime"])),
     "charttime_min": str(lab_std["charttime"].min()),
     "charttime_max": str(lab_std["charttime"].max()),
  }
}
(lab_std
   .select(["hadm_id","itemid","label","charttime","valuenum_std"])
   .write_parquet(INTERIM/"labevents_standardized.parquet"))

with open(OUT/"labevents_core_qc.json","w") as f:
    json.dump(qc, f, indent=2)
```

---

## 本章小结（可执行要点）

1. **体量→磁盘规划**：扫描 `raw/`，估算解压与 Parquet 预算；预留至少 **3×** 冗余。
2. **完整性检查**：字段存在性、主键去重、时间戳一致性（住院/ICU 边界、事件落窗）。
3. **基础 EDA**：唯一计数、住院/ICU 时长分布、化验/生命体征覆盖度及 Top-N 项目。
4. **字典对齐与单位标准化**：`d_labitems/d_items` 统一映射；SCr、BUN、电解质、尿量等做**统一单位**，输出 **Parquet** 与 **QC 报告**。
5. **缓存思路**：会反复用的中间结果（如 `labevents_standardized.parquet`）尽早物化，作为后续特征工程的 **“稳定输入层”**。

---

如果这章内容 OK，我可以紧接着给你一份**一键 CSV→Parquet 渐进转换脚本**（覆盖你关心的表，并写出对应 QC 报告），或者直接进入 **第6章：AKI（KDIGO）定义与实现路径**，把**标签构建**与**相对时间窗**的细节写成可运行模板。你更想先推进哪一块？
