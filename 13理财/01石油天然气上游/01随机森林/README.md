## èƒŒæ™¯

æˆ‘ä½¿ç”¨pythonè¯­è¨€æ„å»ºäº†ä¸€ä¸ªå…³äºä¸€åªåŸºé‡‘çš„é¢„æµ‹æ¨¡å‹ï¼Œä½†æ˜¯æ¨¡å‹çš„å‡†ç¡®æ€§ä¼¼ä¹ä¸æ˜¯å¤ªé«˜ï¼Œæˆ‘éœ€è¦ä½ çš„å¸®åŠ©ï¼Œæˆ‘éœ€è¦ä½ å¸®åŠ©æˆ‘ä¼˜åŒ–ä»£ç ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚
æ­¤å¤–ï¼Œåœ¨æé«˜æ¨¡å‹å‡†ç¡®æ€§ä¹‹åï¼Œæˆ‘å¸Œæœ›å†å¢åŠ ä¸€äº›pythonä»£ç ï¼Œç”¨äºé¢„æµ‹æœªæ¥ä¸€å‘¨çš„åŸºé‡‘èµ°å‘è¶‹åŠ¿ï¼Œå¹¶ç»˜åˆ¶å›¾å½¢ã€‚
æ¥ä¸‹æ¥æˆ‘å°†ç»™æ‚¨æˆ‘çš„ä»£ç ï¼Œè¯·é—®ä½ å‡†å¤‡å¥½å¸®åŠ©æˆ‘äº†å—ï¼Ÿ


## ä»£ç 


ç¬¬ä¸€éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

ç¬¬äºŒéƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š


ç¬¬ä¸‰éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

ç¬¬å››éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

ç¬¬äº”éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

ç¬¬å…­éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š


```python
ç¬¬ä¸€éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

import requests
import pandas as pd
import re
from html import unescape
from io import StringIO

def fetch_fund_history(code, per=20):
    """ä»å¤©å¤©åŸºé‡‘ç½‘æŠ“å–æŒ‡å®šåŸºé‡‘çš„å†å²å‡€å€¼æ•°æ®ï¼Œå¹¶åˆå¹¶åˆ°ä¸€ä¸ª DataFrame ä¸­ã€‚"""
    page = 1
    all_dfs = []
    total_pages = None

    while True:
        url = f"https://fundf10.eastmoney.com/F10DataApi.aspx?type=lsjz&code={code}&page={page}&per={per}"
        response = requests.get(url, timeout=10)
        response.encoding = 'utf-8'

        # æå– content å­—æ®µä¸­çš„ HTML è¡¨æ ¼
        match = re.search(r'content:"(.*?)",records', response.text)
        if not match:
            break
        html_str = unescape(match.group(1))
        # é€šè¿‡ pandas è¯»å– HTML è¡¨æ ¼
        df_list = pd.read_html(StringIO(html_str))
        if df_list:
            all_dfs.append(df_list[0])

        # è·å–æ€»é¡µæ•°
        if total_pages is None:
            pages_match = re.search(r'pages:(\d+)', response.text)
            if pages_match:
                total_pages = int(pages_match.group(1))
            else:
                break

        # å¦‚æœå·²ç»æŠ“å–å®Œå…¨éƒ¨é¡µæ•°ï¼Œåˆ™è·³å‡ºå¾ªç¯
        if page >= total_pages:
            break
        page += 1

    if not all_dfs:
        raise ValueError("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å†å²å‡€å€¼æ•°æ®ï¼Œè¯·æ£€æŸ¥åŸºé‡‘ä»£ç æˆ–ç½‘ç»œè®¾ç½®ã€‚")
    return pd.concat(all_dfs, ignore_index=True)

# æŠ“å–åŸºé‡‘å†å²å‡€å€¼
fund_code = '007844'  # ç›®æ ‡åŸºé‡‘ä»£ç 
print("æ­£åœ¨æŠ“å–åŸºé‡‘å†å²æ•°æ®ï¼Œè¯·ç¨å€™...")
raw_df = fetch_fund_history(fund_code)
print(f"å·²æŠ“å– {len(raw_df)} æ¡è®°å½•ã€‚")

# ä¿å­˜åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
raw_df.to_csv('/workspace/input/' + f'{fund_code}_history_raw.csv', index=False, encoding='utf-8-sig')
print(f"åŸå§‹æ•°æ®å·²ä¿å­˜è‡³ {fund_code}_history_raw.csv")


ç¬¬äºŒéƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š



import pandas as pd

def clean_data(df):
    """å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œè½¬æ¢åˆ—åï¼Œå¹¶å»é™¤æ— ç”¨æ•°æ®åˆ—"""
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½æ­£ç¡®
    if df.empty:
        raise ValueError("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¡¨æ ¼ä¸ºç©ºã€‚")
    
    # å°†ä¸­æ–‡åˆ—åè½¬æ¢ä¸ºè‹±æ–‡
    df.columns = ['date', 'unit_nav', 'cum_nav', 'daily_rate', 'purchase', 'redeem', 'dividend']
    
    # æŸ¥çœ‹åŸå§‹æ•°æ®çš„å‰å‡ è¡Œï¼Œç¡®è®¤æ•°æ®æ ¼å¼
    print("åŸå§‹æ•°æ®å‰å‡ è¡Œï¼š")
    print(df.head())

    # å»é™¤æœ€åä¸¤åˆ—ï¼Œå› ä¸ºå…¶å€¼ç›¸åŒ
    if 'purchase' in df.columns and 'redeem' in df.columns:
        df = df.drop(columns=['purchase', 'redeem'])
    
    # åˆ é™¤â€œå•ä½å‡€å€¼â€å’Œâ€œç´¯è®¡å‡€å€¼â€å…¶ä¸­ä¸€åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å€¼æ˜¯ç›¸åŒçš„
    if 'cum_nav' in df.columns:
        df = df.drop(columns=['cum_nav'])

    # è¿›è¡Œæ—¥æœŸè½¬æ¢ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦å­˜åœ¨è½¬æ¢å¤±è´¥çš„æ—¥æœŸ
    df['date'] = pd.to_datetime(df['date'], errors='coerce')  # é”™è¯¯æ•°æ®ä¼šå˜æˆ NaT
    print(f"è½¬æ¢æ—¥æœŸåï¼ŒNaT æ•°æ®æ•°é‡ï¼š{df['date'].isna().sum()}")

    # å°†â€œæ—¥å¢é•¿ç‡â€åˆ—è½¬åŒ–ä¸ºå°æ•°å½¢å¼ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰éæ•°å­—å­—ç¬¦
    df['daily_rate'] = df['daily_rate'].str.strip('%')
    df['daily_rate'] = pd.to_numeric(df['daily_rate'], errors='coerce')
    print(f"è½¬æ¢æ—¥å¢é•¿ç‡åï¼ŒNaN æ•°æ®æ•°é‡ï¼š{df['daily_rate'].isna().sum()}")

    # å°†â€œå•ä½å‡€å€¼â€åˆ—è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œå¤„ç†é”™è¯¯å€¼
    df['unit_nav'] = pd.to_numeric(df['unit_nav'], errors='coerce')
    print(f"è½¬æ¢å•ä½å‡€å€¼åï¼ŒNaN æ•°æ®æ•°é‡ï¼š{df['unit_nav'].isna().sum()}")

    # è®¡ç®—æ”¶ç›Šç‡å’Œæ¶¨è·Œæ–¹å‘
    df['return'] = df['unit_nav'].pct_change()
    df['direction'] = (df['return'] > 0).astype(int)  # æ¶¨ä¸º1ï¼Œè·Œä¸º0
    
    # æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('date').reset_index(drop=True)
    
    # å»é™¤ NaN å’Œç©ºå€¼ï¼Œåªåˆ é™¤å«æœ‰ NaN çš„è¡Œï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†
    df = df.dropna(subset=['unit_nav', 'daily_rate'])

    # æ£€æŸ¥æ¸…æ´—åçš„æ•°æ®æ˜¯å¦ä¸ºç©º
    if df.empty:
        raise ValueError("æ•°æ®æ¸…æ´—åä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
    
    return df


# è¯»å–æ•°æ®ï¼ˆå‡è®¾æ–‡ä»¶å·²ç»ä¸Šä¼ ï¼‰
file_path = '/workspace/input/007844_history_raw.csv'  # æ›´æ–°ä¸ºæ­£ç¡®çš„è·¯å¾„
df = pd.read_csv(file_path)

# æ¸…æ´—æ•°æ®
cleaned_df = clean_data(df)

# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
cleaned_file_path = '/workspace/input/007844_history_cleaned.csv'
cleaned_df.to_csv(cleaned_file_path, index=False, encoding='utf-8-sig')

print(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜è‡³ï¼š{cleaned_file_path}")



ç¬¬ä¸‰éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š


# ç¬¬ä¸€æ­¥ï¼šè½½å…¥æ¸…æ´—åçš„æ•°æ®ï¼Œå¹¶åšåŸºç¡€ç‰¹å¾å·¥ç¨‹ï¼ˆé€‚é…å°æ ·æœ¬ï¼‰

import pandas as pd
import numpy as np

# è¯»å–æ¸…æ´—åçš„æ•°æ®
file_path = '/workspace/input/007844_history_cleaned.csv'
df = pd.read_csv(file_path, parse_dates=['date'])

# æŒ‰æ—¶é—´æ’åºï¼ˆç¡®ä¿æ—¶åºæ­£ç¡®ï¼‰
df = df.sort_values('date').reset_index(drop=True)

# æ‰“å°åŸå§‹æ•°æ®è¡Œæ•°
print(f"åŸå§‹æ•°æ®è¡Œæ•°ï¼š{len(df)}")

# åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡ï¼ˆé€‚é…è¾ƒå°æ•°æ®é‡ï¼‰
df['sma_3'] = df['unit_nav'].rolling(window=3, min_periods=1).mean()
df['sma_5'] = df['unit_nav'].rolling(window=5, min_periods=1).mean()
df['sma_7'] = df['unit_nav'].rolling(window=7, min_periods=1).mean()
df['volatility_3'] = df['unit_nav'].rolling(window=3, min_periods=1).std()

# è®¡ç®—æ¶¨è·Œæ–¹å‘ï¼ˆå¦‚æœæœªåŒ…å« directionï¼Œå¯ä»¥æ‰‹åŠ¨è¡¥ï¼‰
if 'direction' not in df.columns:
    df['return'] = df['unit_nav'].pct_change()
    df['direction'] = (df['return'] > 0).astype(int)

# å»é™¤æœ€å‰é¢è®¡ç®—ä¸å‡ºæ ‡å‡†å·®çš„é‚£å‡ è¡Œï¼ˆåªå¯¹æ³¢åŠ¨ç‡ä¸ºç©ºçš„è¡Œåšå¤„ç†ï¼‰
df = df.dropna(subset=['volatility_3']).reset_index(drop=True)

# å†æ¬¡æ‰“å°ä¿ç•™çš„æ•°æ®è¡Œæ•°
print(f"ä¿ç•™çš„æ•°æ®è¡Œæ•°ï¼š{len(df)}")

# æŸ¥çœ‹ç»“æœ
df.head()



ç¬¬å››éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š


from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ç¡®è®¤ç‰¹å¾åˆ—
features = ['sma_3', 'sma_5', 'sma_7', 'volatility_3']
X = df[features]
y = df['direction']

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä¸æ‰“ä¹±æ—¶é—´é¡ºåºï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, shuffle=False
)

# åˆå§‹åŒ–å¹¶è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# æ¨¡å‹é¢„æµ‹
y_pred = clf.predict(X_test)

# è¾“å‡ºè¯„ä¼°æŒ‡æ ‡
print("ğŸ¯ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
print("ğŸ“Š æ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

ä¸Šè¿°ä»£ç çš„ç»“æœå¦‚ä¸‹ï¼š

ğŸ¯ åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support

           0       0.54      0.72      0.62       140
           1       0.54      0.35      0.42       133

    accuracy                           0.54       273
   macro avg       0.54      0.53      0.52       273
weighted avg       0.54      0.54      0.52       273


ç¬¬å…­éƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š

# å›å¡«é¢„æµ‹ç»“æœåˆ°åŸå§‹ dataframeï¼ˆæ³¨æ„å¯¹é½ï¼‰
df_eval = df.iloc[-len(y_test):].copy()  # å¯¹åº”æµ‹è¯•é›†éƒ¨åˆ†
df_eval['predicted_direction'] = y_pred

# æ ¹æ®é¢„æµ‹æ–¹å‘ç”Ÿæˆæ“ä½œä¿¡å·
df_eval['signal'] = df_eval['predicted_direction'].map({1: 'buy', 0: 'sell'})

# ä¿å­˜ç»“æœåˆ°æœ¬åœ° CSV
output_path = '/workspace/input/007844_with_signals.csv'
df_eval.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"âœ… ç­–ç•¥ä¿¡å·å·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ï¼š{output_path}")
df_eval[['date', 'unit_nav', 'predicted_direction', 'signal']].head(10)


ä¸Šè¿°ä»£ç çš„ç»“æœå¦‚ä¸‹ï¼š

âœ… ç­–ç•¥ä¿¡å·å·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ï¼š/workspace/input/007844_with_signals.csv
date	unit_nav	predicted_direction	signal
1092	2024-07-19	0.7939	1	buy
1093	2024-07-22	0.7909	1	buy
1094	2024-07-23	0.7788	0	sell
1095	2024-07-24	0.7743	0	sell
1096	2024-07-25	0.7832	0	sell
1097	2024-07-26	0.7864	0	sell
1098	2024-07-29	0.7776	0	sell
1099	2024-07-30	0.7877	0	sell
1100	2024-07-31	0.7934	0	sell
1101	2024-08-01	0.7707	1	buy



ç¬¬ä¸ƒéƒ¨åˆ†çš„ä»£ç å¦‚ä¸‹ï¼š


from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

# âœ… ä½¿ç”¨ä½ å®é™…ç”Ÿæˆè¿‡çš„ç‰¹å¾åˆ—
features = ['sma_3', 'sma_5', 'sma_7', 'volatility_3']

# âœ… æ‰“å°æ¯ä¸€åˆ—ç¼ºå¤±å€¼æ•°é‡ï¼Œæ–¹ä¾¿è°ƒè¯•
print("å„ç‰¹å¾åˆ—ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
print(df[features].isna().sum())

# âœ… ä¸¢æ‰åŒ…å« NaN çš„è¡Œï¼ˆåªè€ƒè™‘ç‰¹å¾å’Œæ ‡ç­¾åˆ—ï¼‰
df_features = df.dropna(subset=features + ['direction']).copy()

# âœ… å†æ¬¡ç¡®è®¤å‰©ä½™è¡Œæ•°
print(f"å¯ç”¨äºè®­ç»ƒçš„æ•°æ®è¡Œæ•°ï¼š{len(df_features)}")
if df_features.empty:
    raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç‰¹å¾åˆ—æˆ–ç¼ºå¤±å¤„ç†é€»è¾‘ã€‚")

# âœ… ç‰¹å¾ä¸æ ‡ç­¾æå–
X_all = df_features[features]
y_all = df_features['direction']

# âœ… æ ‡å‡†åŒ–
scaler = StandardScaler()
X_all_scaled = scaler.fit_transform(X_all)

# âœ… æ¨¡å‹è®­ç»ƒ
clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X_all_scaled, y_all)

# âœ… ä½¿ç”¨æœ€æ–°ä¸€è¡Œæ•°æ®è¿›è¡Œæœªæ¥é¢„æµ‹
latest_row = df_features.iloc[[-1]]
X_future = latest_row[features]
X_future_scaled = scaler.transform(X_future)
future_pred = clf.predict(X_future_scaled)[0]
future_signal = 'buy' if future_pred == 1 else 'sell'

print(f"\nğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸï¼š{latest_row['date'].values[0]}")
print(f"ğŸ“ˆ æ¨¡å‹é¢„æµ‹æœªæ¥ä¿¡å·ï¼š{future_signal.upper()}ï¼ˆé¢„æµ‹æ–¹å‘ï¼š{future_pred}ï¼‰")



ä¸Šè¿°ä»£ç çš„ç»“æœå¦‚ä¸‹ï¼š

å„ç‰¹å¾åˆ—ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š
sma_3           0
sma_5           0
sma_7           0
volatility_3    0
dtype: int64
å¯ç”¨äºè®­ç»ƒçš„æ•°æ®è¡Œæ•°ï¼š1365

ğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸï¼š2025-09-01T00:00:00.000000000
ğŸ“ˆ æ¨¡å‹é¢„æµ‹æœªæ¥ä¿¡å·ï¼šSELLï¼ˆé¢„æµ‹æ–¹å‘ï¼š0ï¼‰

```

ç°åœ¨è¯·æ‚¨æŒ‰ç…§æˆ‘çš„è¦æ±‚ä¼˜åŒ–ä¸Šè¿°ä»£ç ï¼Œä½ åªéœ€è¦è¾“å‡ºpythonä»£ç å³å¯ï¼Œä¸éœ€è¦ä½ è¿›è¡Œè¿è¡Œï¼Œæˆ‘ä¼šåœ¨æˆ‘çš„ç”µè„‘ä¸Šè¿›è¡Œè¿è¡Œï¼Œä½ åªéœ€è¦ä¿è¯ä»£ç é€»è¾‘æ­£ç¡®ã€‚ä½ éœ€è¦åˆ†éƒ¨è¾“å‡ºä»£ç 




## 0103

ä¸‹é¢ç»™å‡º**é€‚åˆåœ¨ Jupyter Notebook é€æ ¼è¿è¡Œ**çš„å®Œæ•´ä»£ç ï¼ˆå·²æŒ‰ä½ çš„éœ€æ±‚é‡æ„ä¸ä¼˜åŒ–ï¼‰ã€‚
æ¯ä¸ª Part æ˜¯ä¸€ä¸ªå•ç‹¬ä»£ç å•å…ƒï¼Œ**ç›´æ¥å¤åˆ¶åˆ°ä¸åŒçš„ Notebook å•å…ƒ**æŒ‰é¡ºåºè¿è¡Œå³å¯ã€‚

---

### Part 1 â€” æŠ“å–å¹¶ä¿å­˜åŸå§‹å†å²å‡€å€¼ï¼ˆä¸œè´¢ï¼‰

```python
# Part 1 â€” æŠ“å–å¹¶ä¿å­˜åŸå§‹å†å²å‡€å€¼ï¼ˆä¸œè´¢ï¼‰
import re
import time
import requests
import pandas as pd
from html import unescape
from io import StringIO
from pathlib import Path

# å…¨å±€é…ç½®ï¼ˆå¯ä¿®æ”¹ï¼‰
FUND_CODE = "007844"
DATA_DIR = "/workspace/input"  # è‹¥ä¸å­˜åœ¨æ­¤ç›®å½•ï¼Œå¯æ”¹ä¸º "."
Path(DATA_DIR).mkdir(parents=True, exist_ok=True)

def fetch_fund_history(code: str, per: int = 50, sleep_s: float = 0.6) -> pd.DataFrame:
    page, total_pages = 1, None
    all_dfs = []
    while True:
        url = f"https://fundf10.eastmoney.com/F10DataApi.aspx?type=lsjz&code={code}&page={page}&per={per}"
        r = requests.get(url, timeout=10)
        r.encoding = "utf-8"

        m = re.search(r'content:"(.*?)",records', r.text)
        if not m:
            break
        html_str = unescape(m.group(1))
        dfs = pd.read_html(StringIO(html_str))
        if dfs:
            all_dfs.append(dfs[0])

        if total_pages is None:
            pm = re.search(r'pages:(\d+)', r.text)
            total_pages = int(pm.group(1)) if pm else page

        if page >= total_pages:
            break
        page += 1
        time.sleep(sleep_s)

    if not all_dfs:
        raise ValueError("æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥åŸºé‡‘ä»£ç æˆ–ç½‘ç»œã€‚")

    df = pd.concat(all_dfs, ignore_index=True)
    df.columns = ['date', 'unit_nav', 'cum_nav', 'daily_rate', 'purchase', 'redeem', 'dividend']
    return df

RAW_SAVE = f"{DATA_DIR}/{FUND_CODE}_history_raw.csv"
raw_df = fetch_fund_history(FUND_CODE, per=50)
raw_df.to_csv(RAW_SAVE, index=False, encoding="utf-8-sig")
print(f"[OK] æŠ“å–å®Œæˆï¼Œä¿å­˜ï¼š{RAW_SAVE}ï¼ˆ{len(raw_df)} è¡Œï¼‰")
raw_df.head()
```

---

### Part 2 â€” æ¸…æ´—ä¸åŸºç¡€æ ¡éªŒ

```python
# Part 2 â€” æ¸…æ´—ä¸åŸºç¡€æ ¡éªŒ
import pandas as pd

RAW_PATH = f"{DATA_DIR}/{FUND_CODE}_history_raw.csv"
CLEAN_PATH = f"{DATA_DIR}/{FUND_CODE}_history_cleaned.csv"

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = ['date', 'unit_nav', 'cum_nav', 'daily_rate', 'purchase', 'redeem', 'dividend']

    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df['unit_nav'] = pd.to_numeric(df['unit_nav'], errors='coerce')
    df['cum_nav'] = pd.to_numeric(df['cum_nav'], errors='coerce')
    df['daily_rate'] = pd.to_numeric(df['daily_rate'].astype(str).str.strip('%'), errors='coerce') / 100.0

    # æ’åºå»é‡ä¸æ— æ•ˆåˆ—
    df = df.dropna(subset=['date']).sort_values('date').drop_duplicates(subset=['date'])
    df = df.drop(columns=['purchase', 'redeem', 'dividend'], errors='ignore')
    if 'cum_nav' in df.columns:
        df = df.drop(columns=['cum_nav'])

    df = df.dropna(subset=['unit_nav']).reset_index(drop=True)
    return df

df_raw = pd.read_csv(RAW_PATH)
df_clean = clean_data(df_raw)
df_clean.to_csv(CLEAN_PATH, index=False, encoding="utf-8-sig")
print(f"[OK] æ¸…æ´—å®Œæˆï¼š{CLEAN_PATH}ï¼ˆ{len(df_clean)} è¡Œï¼‰")
df_clean.head()
```

---

### Part 3 â€” ç‰¹å¾å·¥ç¨‹ä¸æ ‡ç­¾æ„å»ºï¼ˆé¿å…æ³„æ¼ï¼‰

```python
# Part 3 â€” ç‰¹å¾å·¥ç¨‹ä¸æ ‡ç­¾æ„å»ºï¼ˆé¿å…æ³„æ¼ï¼‰
import numpy as np
import pandas as pd

CLEAN_PATH = f"{DATA_DIR}/{FUND_CODE}_history_cleaned.csv"
FEAT_PATH = f"{DATA_DIR}/{FUND_CODE}_features.csv"

def add_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['ret1'] = df['unit_nav'].pct_change()

    # å‡çº¿ä¸æ³¢åŠ¨ç‡ï¼ˆä»…ç”¨å†å²ï¼‰
    for w in [3, 5, 7, 10, 20]:
        df[f'sma_{w}'] = df['unit_nav'].rolling(w, min_periods=1).mean()
    for w in [3, 5, 10, 20]:
        df[f'vol_{w}'] = df['ret1'].rolling(w, min_periods=2).std()

    # åŠ¨é‡/RSI
    df['mom_5'] = df['unit_nav'].pct_change(5)
    delta = df['unit_nav'].diff()
    gain = (delta.where(delta > 0, 0.0)).rolling(14, min_periods=14).mean()
    loss = (-delta.where(delta < 0, 0.0)).rolling(14, min_periods=14).mean()
    rs = gain / (loss.replace(0, np.nan))
    df['rsi_14'] = 100 - (100 / (1 + rs))

    # MACD
    ema12 = df['unit_nav'].ewm(span=12, adjust=False).mean()
    ema26 = df['unit_nav'].ewm(span=26, adjust=False).mean()
    macd = ema12 - ema26
    signal = macd.ewm(span=9, adjust=False).mean()
    df['macd'] = macd
    df['macd_sig'] = signal
    df['macd_hist'] = df['macd'] - df['macd_sig']

    # å¸ƒæ—å¸¦
    ma20 = df['unit_nav'].rolling(20, min_periods=20).mean()
    std20 = df['unit_nav'].rolling(20, min_periods=20).std()
    df['bb_upper'] = ma20 + 2 * std20
    df['bb_lower'] = ma20 - 2 * std20
    df['bb_pct']   = (df['unit_nav'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

    # æ ‡ç­¾ï¼št+1æ–¹å‘ & æœªæ¥5æ—¥æ€»ä½“æ–¹å‘
    df['target_1d'] = (df['ret1'].shift(-1) > 0).astype(int)
    df['ret_fwd_5'] = df['unit_nav'].shift(-5) / df['unit_nav'] - 1.0
    df['target_5d'] = (df['ret_fwd_5'] > 0).astype(int)

    df = df.dropna().reset_index(drop=True)
    return df

base = pd.read_csv(CLEAN_PATH, parse_dates=['date'])
feat = add_features(base)
feat.to_csv(FEAT_PATH, index=False, encoding="utf-8-sig")
print(f"[OK] ç‰¹å¾å®Œæˆï¼š{FEAT_PATH}ï¼ˆ{len(feat)} è¡Œ, åˆ—æ•°={feat.shape[1]}ï¼‰")
feat.head()
```

---

### Part 4 â€” æ—¶åºéªŒè¯ + æ¨¡å‹é€‰æ‹© + æµ‹è¯•é›†è¯„ä¼°ä¸å›¾

```python
# Part 4 â€” æ—¶åºéªŒè¯ + æ¨¡å‹é€‰æ‹© + æµ‹è¯•é›†è¯„ä¼°ä¸å›¾
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (classification_report, confusion_matrix,
                             ConfusionMatrixDisplay, roc_auc_score, roc_curve,
                             balanced_accuracy_score)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

FEAT_PATH = f"{DATA_DIR}/{FUND_CODE}_features.csv"

FEATURES = [
    'sma_3','sma_5','sma_7','sma_10','sma_20',
    'vol_3','vol_5','vol_10','vol_20',
    'mom_5','rsi_14','macd','macd_sig','macd_hist',
    'bb_pct'
]
TARGET = 'target_1d'   # ä¹Ÿå¯ä»¥æ”¹ä¸º 'target_5d' è¯„ä¼°ä¸€å‘¨è¶‹åŠ¿

df = pd.read_csv(FEAT_PATH, parse_dates=['date']).sort_values('date').reset_index(drop=True)

# ç•™å‡ºæ³•ï¼šæœ€å 20% ä½œä¸ºæœ€ç»ˆæµ‹è¯•é›†
test_size = max(200, int(len(df) * 0.2))
train_df, test_df = df.iloc[:-test_size], df.iloc[-test_size:]

X_train, y_train = train_df[FEATURES], train_df[TARGET]
X_test,  y_test  = test_df[FEATURES],  test_df[TARGET]

# ä¸¤ç±»æ¨¡å‹å¯¹æ¯”
pipelines = {
    "logreg": Pipeline([("scaler", StandardScaler()),
                        ("clf", LogisticRegression(max_iter=500, class_weight='balanced'))]),
    "rf": Pipeline([("scaler", StandardScaler(with_mean=False)),
                    ("clf", RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))])
}
param_grid = {
    "logreg": {"clf__C": [0.1, 0.5, 1.0, 2.0]},
    "rf": {"clf__n_estimators": [200, 400],
           "clf__max_depth": [None, 5, 8, 12],
           "clf__min_samples_leaf": [1, 3, 5]}
}

tscv = TimeSeriesSplit(n_splits=5)
best_model, best_name, best_score = None, None, -np.inf
for name, pipe in pipelines.items():
    gscv = GridSearchCV(pipe, param_grid[name], scoring="balanced_accuracy", cv=tscv, n_jobs=-1, verbose=0)
    gscv.fit(X_train, y_train)
    print(f"[CV] {name} best score={gscv.best_score_:.4f}, params={gscv.best_params_}")
    if gscv.best_score_ > best_score:
        best_model, best_name, best_score = gscv.best_estimator_, name, gscv.best_score_

print(f"[SELECT] é€‰æ‹©æ¨¡å‹ï¼š{best_name}ï¼ˆCV balanced_acc={best_score:.4f}ï¼‰")

# æµ‹è¯•é›†è¯„ä¼°
y_pred = best_model.predict(X_test)
if hasattr(best_model, "predict_proba"):
    y_proba = best_model.predict_proba(X_test)[:, 1]
else:
    dec = best_model.decision_function(X_test)
    y_proba = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)

bal_acc = balanced_accuracy_score(y_test, y_pred)
print(f"\n[TEST] balanced_accuracy={bal_acc:.4f}\n")
print(classification_report(y_test, y_pred, digits=4))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred, labels=[0,1])
disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])
fig, ax = plt.subplots(figsize=(5,4))
disp.plot(ax=ax, cmap="Blues", values_format='d')
plt.title("Confusion Matrix (Holdout)")
plt.tight_layout()
plt.show()

# ROC
try:
    auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.figure(figsize=(5,4))
    plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
    plt.plot([0,1], [0,1], linestyle="--")
    plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("ROC (Holdout)")
    plt.legend()
    plt.tight_layout()
    plt.show()
except Exception as e:
    print("[WARN] æ— æ³•è®¡ç®—ROCï¼š", e)

# ä¿å­˜æµ‹è¯•é›†é¢„æµ‹
pred_out = test_df[['date','unit_nav']].copy()
pred_out['y_true'] = y_test.values
pred_out['y_pred'] = y_pred
pred_out['proba_up'] = y_proba
pred_out['signal'] = np.where(pred_out['y_pred']==1, 'buy', 'sell')
SAVE_PRED = f"{DATA_DIR}/{FUND_CODE}_holdout_predictions.csv"
pred_out.to_csv(SAVE_PRED, index=False, encoding="utf-8-sig")
print(f"[OK] æµ‹è¯•é›†é¢„æµ‹å·²ä¿å­˜ï¼š{SAVE_PRED}")
```

---

### Part 5 â€” è®­ç»ƒå…¨é‡æ¨¡å‹å¹¶è¾“å‡ºâ€œä¸‹ä¸€æ—¥â€ä¸â€œä¸€å‘¨è¶‹åŠ¿â€é¢„æµ‹ + ç»˜å›¾

```python
# Part 5 â€” è®­ç»ƒå…¨é‡æ¨¡å‹å¹¶è¾“å‡ºâ€œä¸‹ä¸€æ—¥â€ä¸â€œä¸€å‘¨è¶‹åŠ¿â€é¢„æµ‹ + ç»˜å›¾
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

FEAT_PATH = f"{DATA_DIR}/{FUND_CODE}_features.csv"
OUT_SIG = f"{DATA_DIR}/{FUND_CODE}_future_signals.csv"

FEATURES = [
    'sma_3','sma_5','sma_7','sma_10','sma_20',
    'vol_3','vol_5','vol_10','vol_20',
    'mom_5','rsi_14','macd','macd_sig','macd_hist',
    'bb_pct'
]

def train_best(X, y):
    pipelines = {
        "logreg": Pipeline([("scaler", StandardScaler()),
                            ("clf", LogisticRegression(max_iter=500, class_weight='balanced'))]),
        "rf": Pipeline([("scaler", StandardScaler(with_mean=False)),
                        ("clf", RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))])
    }
    grid = {
        "logreg": {"clf__C":[0.1,0.5,1.0,2.0]},
        "rf": {"clf__n_estimators":[300,500],
               "clf__max_depth":[None,8,12],
               "clf__min_samples_leaf":[1,3,5]}
    }
    tscv = TimeSeriesSplit(n_splits=5)
    best_est, best_score = None, -np.inf
    for name, pipe in pipelines.items():
        g = GridSearchCV(pipe, grid[name], scoring="balanced_accuracy", cv=tscv, n_jobs=-1, verbose=0)
        g.fit(X, y)
        if g.best_score_ > best_score:
            best_score, best_est = g.best_score_, g.best_estimator_
    return best_est, best_score

df = pd.read_csv(FEAT_PATH, parse_dates=['date']).sort_values('date').reset_index(drop=True)

# â€”â€” ä¸‹ä¸€æ—¥æ–¹å‘ï¼ˆtarget_1dï¼‰
df1 = df.dropna(subset=FEATURES + ['target_1d']).copy()
X1, y1 = df1[FEATURES], df1['target_1d']
model_1d, cv1 = train_best(X1, y1)
last_row = df1.iloc[[-1]][FEATURES]
p1 = model_1d.predict_proba(last_row)[:,1][0] if hasattr(model_1d,"predict_proba") else None
pred1 = model_1d.predict(last_row)[0]
sig1 = 'BUY' if pred1==1 else 'SELL'
print(f"[NEXT-DAY] {sig1} | up_prob={p1:.3f}" if p1 is not None else f"[NEXT-DAY] {sig1}")

# â€”â€” æœªæ¥5æ—¥æ€»ä½“è¶‹åŠ¿ï¼ˆtarget_5dï¼‰
df5 = df.dropna(subset=FEATURES + ['target_5d']).copy()
X5, y5 = df5[FEATURES], df5['target_5d']
model_5d, cv5 = train_best(X5, y5)
last_row5 = df5.iloc[[-1]][FEATURES]
p5 = model_5d.predict_proba(last_row5)[:,1][0] if hasattr(model_5d,"predict_proba") else None
pred5 = model_5d.predict(last_row5)[0]
sig5 = 'UP_5D' if pred5==1 else 'DOWN_5D'
print(f"[WEEK] {sig5} | up_prob={p5:.3f}" if p5 is not None else f"[WEEK] {sig5}")

# â€”â€” ç»˜å›¾ï¼šæœ€è¿‘200ä¸ªäº¤æ˜“æ—¥ + æœªæ¥ä¿¡å·æ ‡æ³¨
tail_n = 200
plot_df = df.tail(tail_n).copy()
plt.figure(figsize=(10,4))
plt.plot(plot_df['date'], plot_df['unit_nav'], label='Unit NAV')
plt.title(f"{FUND_CODE} â€” æœ€è¿‘{tail_n}æ—¥å‡€å€¼ä¸æœªæ¥ä¿¡å·")
plt.xlabel("Date"); plt.ylabel("Unit NAV")

x_last = plot_df['date'].iloc[-1]; y_last = plot_df['unit_nav'].iloc[-1]
txt = f"Next: {sig1}"
if p1 is not None: txt += f" ({p1:.2f})"
txt += f" | Week: {sig5}"
if p5 is not None: txt += f" ({p5:.2f})"
plt.scatter([x_last],[y_last], s=60)
plt.annotate(txt, (x_last, y_last), xytext=(15, 15), textcoords="offset points",
             bbox=dict(boxstyle="round,pad=0.3", fc="w", ec="gray"))
plt.tight_layout()
plt.show()

# â€”â€” ä¿å­˜æœªæ¥ä¿¡å·
out = {
    "next_day_signal": sig1,
    "next_day_prob_up": float(p1) if p1 is not None else None,
    "week_trend_signal": sig5,
    "week_prob_up": float(p5) if p5 is not None else None,
    "asof": str(df['date'].iloc[-1].date())
}
pd.DataFrame([out]).to_csv(OUT_SIG, index=False, encoding="utf-8-sig")
print(f"[OK] æœªæ¥ä¿¡å·å·²ä¿å­˜ï¼š{OUT_SIG}")
```

---

### Part 6 â€” å¯é€‰ï¼šæ»šåŠ¨å›æµ‹ï¼ˆWalk-Forwardï¼‰ä¸ Buy\&Hold å¯¹æ¯”

```python
# Part 6 â€” å¯é€‰ï¼šæ»šåŠ¨å›æµ‹ï¼ˆWalk-Forwardï¼‰ä¸ Buy&Hold å¯¹æ¯”
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.base import clone
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

FEAT_PATH = f"{DATA_DIR}/{FUND_CODE}_features.csv"
OUT_BT = f"{DATA_DIR}/{FUND_CODE}_walkforward_backtest.csv"

FEATURES = [
    'sma_3','sma_5','sma_7','sma_10','sma_20',
    'vol_3','vol_5','vol_10','vol_20',
    'mom_5','rsi_14','macd','macd_sig','macd_hist',
    'bb_pct'
]

df = pd.read_csv(FEAT_PATH, parse_dates=['date']).sort_values('date').reset_index(drop=True)
df = df.dropna(subset=FEATURES + ['target_1d','ret1']).copy()

# åŸºæ¨¡å‹ï¼ˆä¸ Part 5 é£æ ¼ä¸€è‡´ï¼‰
base_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=500, class_weight='balanced'))
])

# Walk-forwardï¼šå‰ i å¤©è®­ç»ƒ -> é¢„æµ‹ i+1 å¤©ï¼ˆä¸¥æ ¼åªç”¨å†å²ï¼‰
preds = []
min_train = 250  # çº¦ä¸€å¹´èµ·æ­¥
for i in range(min_train, len(df)-1):
    train = df.iloc[:i]
    test_row = df.iloc[[i]]

    model = clone(base_pipeline)
    model.fit(train[FEATURES], train['target_1d'])
    proba_up = model.predict_proba(test_row[FEATURES])[:,1][0]
    pred = 1 if proba_up >= 0.5 else 0
    preds.append((df.loc[i,'date'], pred, proba_up))

wf = pd.DataFrame(preds, columns=['date','pred','proba_up'])
merged = df.merge(wf, on='date', how='left')

# ç®€å•ç­–ç•¥ï¼špred==1 æŒæœ‰ï¼Œå¦åˆ™æŒå¸ï¼ˆæœªè®¡äº¤æ˜“æˆæœ¬/æ»‘ç‚¹ï¼‰
merged['ret_strategy'] = merged['ret1'] * merged['pred'].shift(1)
merged['ret_bh'] = merged['ret1']
merged[['cum_strategy','cum_bh']] = (1+merged[['ret_strategy','ret_bh']]).fillna(1.0).cumprod()

# ç”»ç´¯è®¡æ”¶ç›Š
plt.figure(figsize=(10,4))
plt.plot(merged['date'], merged['cum_bh'], label='Buy & Hold')
plt.plot(merged['date'], merged['cum_strategy'], label='Strategy (WF)')
plt.title(f"{FUND_CODE} ç­–ç•¥å›æµ‹ï¼ˆæ»šåŠ¨è®­ç»ƒï¼Œæœªè®¡è´¹ï¼‰")
plt.xlabel("Date"); plt.ylabel("Cumulative Return (Ã—)")
plt.legend()
plt.tight_layout()
plt.show()

# ä¿å­˜
merged[['date','unit_nav','ret1','pred','proba_up','ret_strategy','ret_bh','cum_strategy','cum_bh']].to_csv(
    OUT_BT, index=False, encoding="utf-8-sig"
)
print(f"[OK] æ»šåŠ¨å›æµ‹ç»“æœå·²ä¿å­˜ï¼š{OUT_BT}")
```

---

> å¯é€‰å°æç¤ºï¼š
>
> * è‹¥ä½ çš„ Notebook æ²¡æœ‰ `/workspace/input` è¿™ä¸ªç›®å½•ï¼ŒæŠŠ `DATA_DIR` æ”¹æˆ `"."` å³å¯ã€‚
> * å¦‚æœä½ å¸Œæœ›**åªç”¨ä½ æœ€å¼€å§‹çš„ 4 ä¸ªç‰¹å¾**ï¼ˆ`sma_3,sma_5,sma_7,volatility_3`ï¼‰ï¼ŒæŠŠ FEATURES åˆ—è¡¨æ”¹æˆé‚£ 4 ä¸ªå³å¯ï¼ˆ`volatility_3` å¯¹åº”è¿™é‡Œçš„ `vol_3`ï¼‰ã€‚
> * æœ¬å¥—ä»£ç å·²ç»ï¼šé¿å…äº†æ•°æ®æ³„æ¼ã€ä½¿ç”¨ `TimeSeriesSplit` åšæ—¶åºäº¤å‰éªŒè¯ã€ä»¥ `balanced_accuracy` ä½œä¸ºè¯„åˆ†ã€æ›´ç¨³å¥åœ°è¯„ä¼°äº†æ¨¡å‹ï¼Œå¹¶é™„å¸¦â€œä¸‹ä¸€æ—¥ + ä¸€å‘¨è¶‹åŠ¿â€é¢„æµ‹ä¸å›¾å½¢ã€‚


# 0401GPT4o

è¯·é—®æ ¹æ®å†å²æˆäº¤ä»·é¢„æµ‹æœªæ¥åŸºé‡‘èµ°å‘è¶‹åŠ¿çš„æ¨¡å‹æœ‰å“ªäº›ï¼Ÿ

æˆ‘æœ‰ä¸€ä»½æ•°æ®ï¼Œå‚¨å­˜çš„è·¯å¾„ä¸ºï¼šfile_path = '/workspace/input/007844_history_raw.csv'

ç°åœ¨è¯·ä½ ç”¨è¿™ä»½æ•°æ®æ„å»ºä¸Šè¿°æ¨¡å‹ï¼Œè¦æ±‚è¿”å›jupyter notebookä»£ç ï¼Œä¸éœ€è¦ä½ è¿›è¡Œè¿è¡Œï¼Œåªéœ€è¦ç»™å‡ºä»£ç å³å¯


å¥½çš„ï¼Œä¸‹é¢æ˜¯å®Œæ•´çš„ Jupyter Notebook ä»£ç ï¼ˆMarkdown + Python cell æ··åˆæ ¼å¼ï¼‰ï¼Œå®ç°ä»¥ä¸‹å†…å®¹ï¼š

* åŠ è½½å¹¶åˆæ­¥æ¢ç´¢åŸºé‡‘æ•°æ®ï¼›
* é¢„å¤„ç†åŸºé‡‘å†å²æ•°æ®ï¼›
* æ„å»ºå¤šä¸ªæ¨¡å‹è¿›è¡ŒåŸºé‡‘ä»·æ ¼é¢„æµ‹ï¼ˆåŸºäºå†å²å‡€å€¼ï¼‰ï¼š

  * ARIMA
  * LSTM
  * XGBoost
  * Transformerï¼ˆPatchTST ç®€åŒ–å®ç°ï¼‰

---

### ğŸ§  Notebook æ ‡é¢˜ï¼šåŸºäºå†å²å‡€å€¼çš„åŸºé‡‘è¶‹åŠ¿é¢„æµ‹æ¨¡å‹å¯¹æ¯”åˆ†æ

---

```markdown
## åŸºé‡‘ä»·æ ¼é¢„æµ‹æ¨¡å‹æ¯”è¾ƒåˆ†æ
æ•°æ®æ¥æºï¼š007844 åŸºé‡‘å†å²å‡€å€¼æ•°æ®  
é¢„æµ‹ç›®æ ‡ï¼šæœªæ¥5æ—¥åŸºé‡‘å•ä½å‡€å€¼ï¼ˆUnit Net Valueï¼‰

ä½œè€…ï¼šChatGPT + ç”¨æˆ·  
æ—¥æœŸï¼š2025-09
```

---

```python
# ğŸ“¦ åŸºç¡€ä¾èµ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

# ç»Ÿè®¡å»ºæ¨¡
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error

# MLæ¨¡å‹
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
import xgboost as xgb

# LSTM
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Transformer
from einops import rearrange
```

---

### 1ï¸âƒ£ æ•°æ®åŠ è½½ä¸å¯è§†åŒ–

```python
# è·¯å¾„
file_path = '/workspace/input/007844_history_raw.csv'

# åŠ è½½
df = pd.read_csv(file_path)
df.head()
```

---

```python
# å¦‚æœæ˜¯ä¸­æ–‡å­—æ®µï¼Œå…ˆæ”¹åˆ—åä¸ºè‹±æ–‡
df.columns = ['date', 'unit_nav', 'acc_nav', 'daily_growth', 'buy_status', 'sell_status']

# è½¬æ¢æ—¥æœŸ
df['date'] = pd.to_datetime(df['date'])

# ä¿ç•™æ‰€éœ€åˆ—å¹¶æŒ‰æ—¶é—´æ’åº
df = df[['date', 'unit_nav']].sort_values('date')

# å¯è§†åŒ–
plt.figure(figsize=(12,5))
plt.plot(df['date'], df['unit_nav'], label='Unit NAV')
plt.title("Fund Net Value Over Time")
plt.xlabel("Date")
plt.ylabel("Net Value")
plt.legend()
plt.grid(True)
plt.show()
```

---

### 2ï¸âƒ£ é¢„æµ‹ä»»åŠ¡å®šä¹‰ä¸æ•°æ®å‡†å¤‡

```python
# è®¾ç½®é¢„æµ‹å¤©æ•°
forecast_horizon = 5

# æ„å»ºç›‘ç£å­¦ä¹ æ ¼å¼ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
def create_supervised_data(data, window_size=20, pred_horizon=5):
    X, y = [], []
    for i in range(len(data) - window_size - pred_horizon + 1):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size:i+window_size+pred_horizon])
    return np.array(X), np.array(y)

# æå–ç›®æ ‡åºåˆ—
nav_series = df['unit_nav'].values
X, y = create_supervised_data(nav_series, window_size=30, pred_horizon=forecast_horizon)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
split = int(0.8 * len(X))
X_train, y_train = X[:split], y[:split]
X_test, y_test = X[split:], y[split:]

print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")
```

---

### 3ï¸âƒ£ æ¨¡å‹ä¸€ï¼šARIMAï¼ˆå•æ­¥é¢„æµ‹ï¼‰

```python
from tqdm import tqdm

history = list(nav_series[:split+30])
preds = []

for t in tqdm(range(forecast_horizon)):
    model = ARIMA(history, order=(5,1,0))  # ç®€å•è®¾ç½®
    model_fit = model.fit()
    yhat = model_fit.forecast()[0]
    preds.append(yhat)
    history.append(yhat)

plt.plot(range(len(nav_series)), nav_series, label='True')
plt.plot(range(len(nav_series), len(nav_series)+forecast_horizon), preds, label='ARIMA Forecast')
plt.legend()
plt.title('ARIMA Forecast')
plt.show()
```

---

### 4ï¸âƒ£ æ¨¡å‹äºŒï¼šLSTMï¼ˆå¤šæ­¥é¢„æµ‹ï¼‰

```python
# æ„å»ºDatasetç±»
class FundDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

train_loader = DataLoader(FundDataset(X_train, y_train), batch_size=32, shuffle=True)
test_loader = DataLoader(FundDataset(X_test, y_test), batch_size=32, shuffle=False)

# å®šä¹‰æ¨¡å‹
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, forecast_horizon)

    def forward(self, x):
        _, (hn, _) = self.lstm(x)
        out = self.fc(hn[-1])
        return out

model = LSTMModel(input_dim=1).cuda()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    model.train()
    for Xb, yb in train_loader:
        Xb, yb = Xb.unsqueeze(-1).cuda(), yb.cuda()
        preds = model(Xb)
        loss = criterion(preds, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

### 5ï¸âƒ£ æ¨¡å‹ä¸‰ï¼šXGBoostï¼ˆå¤šè¾“å‡ºï¼‰

```python
# æ¯ä¸€ç»´ y åˆ†å¼€å»ºæ¨¡
models = []
preds = []

for i in range(forecast_horizon):
    model = xgb.XGBRegressor(n_estimators=100)
    model.fit(X_train, y_train[:, i])
    models.append(model)
    pred = model.predict(X_test)
    preds.append(pred)

preds = np.stack(preds, axis=1)

# å¯è§†åŒ–
plt.plot(y_test[0], label='True')
plt.plot(preds[0], label='XGBoost Predicted')
plt.legend()
plt.title('XGBoost Multi-step Prediction')
plt.show()
```

---

### 6ï¸âƒ£ æ¨¡å‹å››ï¼šTransformerï¼ˆPatchTST ç®€åŒ–ç‰ˆï¼‰

```python
class SimpleTransformer(nn.Module):
    def __init__(self, input_len=30, d_model=64, nhead=4):
        super().__init__()
        self.embedding = nn.Linear(1, d_model)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=2)
        self.fc = nn.Linear(d_model * input_len, forecast_horizon)

    def forward(self, x):  # x: (B, T, 1)
        x = self.embedding(x)  # (B, T, D)
        x = self.transformer(x)  # (B, T, D)
        x = x.flatten(1)  # (B, T*D)
        return self.fc(x)

model = SimpleTransformer(input_len=30).cuda()
```

---

### ğŸ“Š æ¨¡å‹è¯„ä¼°æ¨¡æ¿

```python
def evaluate(true, pred):
    mse = mean_squared_error(true, pred)
    mae = mean_absolute_error(true, pred)
    return {'MAE': mae, 'MSE': mse, 'RMSE': np.sqrt(mse)}

# ç¤ºä¾‹ï¼šXGBoostè¯„ä¼°
evaluate(y_test, preds)
```

---

å¦‚éœ€æˆ‘å¸®ä½ å•ç‹¬å®Œæˆæ¯ä¸€ä¸ªæ¨¡å‹è®­ç»ƒ + æ¨ç† + è¯„ä¼° + å›¾è¡¨è¾“å‡º + å¤šæ¬¡æ»‘åŠ¨é¢„æµ‹å¾ªç¯ï¼Œè¯·ç»§ç»­å‘Šè¯‰æˆ‘ä½ æœ€å…³å¿ƒå“ªä¸ªæ¨¡å‹æˆ–ç»“æœç»´åº¦ï¼ˆå¦‚ï¼šé•¿æœŸèµ°åŠ¿ vs æ³¢åŠ¨æ€§ vs æ‹ç‚¹é¢„æµ‹ç­‰ï¼‰ï¼Œæˆ‘å¯ä»¥ç»§ç»­æ‰©å±•è¿™ä¸ª notebookã€‚

# 0402æ—¶åºæ¨¡å‹GPT4o

éå¸¸å¥½ï¼æˆ‘ä»¬ç°åœ¨å¼€å§‹æ„å»ºå®Œæ•´çš„ä»£ç éƒ¨åˆ†ï¼ˆPart A1ï¼‰ï¼Œå†…å®¹åŒ…æ‹¬ï¼š

---

# âœ… Part A1ï¼šæ•°æ®è¯»å–ä¸é¢„å¤„ç†ï¼ˆä¸º Prophetã€LSTMã€XGBoost åšå‡†å¤‡ï¼‰

```python
# âœ… å¿…è¦ä¾èµ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… è¯»å–æ•°æ®
file_path = '/workspace/input/007844_history_raw.csv'
df = pd.read_csv(file_path)

# âœ… æŸ¥çœ‹åŸå§‹åˆ—å
print("åŸå§‹åˆ—åï¼š", df.columns.tolist())
```

---

## âœ… Step 1ï¼šæ•´ç†å­—æ®µï¼ˆé€‚é… Prophet + å…¶ä»–æ¨¡å‹ï¼‰

```python
# æ ‡å‡†åŒ–åˆ—åï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# ä¸€èˆ¬åŒ…æ‹¬ï¼šæ—¥æœŸã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€æ—¥å¢é•¿ç‡ã€ä¹°å…¥çŠ¶æ€ã€å–å‡ºçŠ¶æ€
df.columns = ['date', 'unit_nav', 'acc_nav', 'daily_growth', 'buy_status', 'sell_status']

# ä¿ç•™éœ€è¦çš„å­—æ®µï¼Œå¹¶è½¬æ¢æ—¥æœŸæ ¼å¼
df['date'] = pd.to_datetime(df['date'])
df = df[['date', 'unit_nav']].dropna().sort_values('date').reset_index(drop=True)

# å¯è§†åŒ–æ•°æ®æ¦‚å†µ
plt.figure(figsize=(12, 5))
plt.plot(df['date'], df['unit_nav'], label='Unit NAV')
plt.title("Historical Fund Net Value")
plt.xlabel("Date")
plt.ylabel("Unit NAV")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

---

## âœ… Step 2ï¼šä¸º Prophet æ¨¡å‹å‡†å¤‡æ•°æ®æ ¼å¼

```python
# Prophet éœ€è¦ç‰¹å®šæ ¼å¼ï¼š'ds' ä½œä¸ºæ—¥æœŸï¼Œ'y' ä½œä¸ºç›®æ ‡å€¼
df_prophet = df.rename(columns={'date': 'ds', 'unit_nav': 'y'})
```

---

## âœ… Step 3ï¼šä¸º LSTM / XGBoost æ¨¡å‹æ„å»ºæ»‘åŠ¨çª—å£

```python
# æ„é€ ç›‘ç£å­¦ä¹ åºåˆ—æ•°æ®ï¼ˆç”¨äº LSTM å’Œ XGBoostï¼‰
def create_supervised_data(data, window_size=30, forecast_horizon=1):
    X, y = [], []
    for i in range(len(data) - window_size - forecast_horizon + 1):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size:i+window_size+forecast_horizon])
    return np.array(X), np.array(y)

# æå–ç›®æ ‡åºåˆ—
nav_series = df['unit_nav'].values

# æ„é€  X, y
window_size = 30
forecast_horizon = 1   # æ¯æ­¥é¢„æµ‹1å¤©ï¼Œåé¢ä¼šå¾ªç¯å¤šæ¬¡é€’æ¨
X, y = create_supervised_data(nav_series, window_size=window_size, forecast_horizon=forecast_horizon)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
```

---

ğŸ¯ **ä¸‹ä¸€æ­¥ Part A2** å°†åŒ…æ‹¬ï¼š

* Prophet æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
* LSTM æ¨¡å‹æ„å»ºä¸è®­ç»ƒ
* XGBoost æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹

æ˜¯å¦ç»§ç»­è¾“å‡º **Part A2**ï¼Ÿåªéœ€å›å¤â€œç»§ç»­â€ã€‚


å¾ˆå¥½ï¼Œæˆ‘ä»¬ç»§ç»­è¾“å‡ºï¼š

---

# âœ… Part A2ï¼šä¸‰ç§æ¨¡å‹çš„è®­ç»ƒä¸é¢„æµ‹ï¼ˆProphet, LSTM, XGBoostï¼‰

---

## âœ… Step 1: Prophet æ¨¡å‹è®­ç»ƒä¸æœªæ¥ 1 ä¸ªæœˆé¢„æµ‹

```python
from prophet import Prophet

# åˆå§‹åŒ– Prophet æ¨¡å‹
prophet_model = Prophet(daily_seasonality=True)
prophet_model.fit(df_prophet)

# æ„å»ºæœªæ¥ 20 ä¸ªäº¤æ˜“æ—¥ï¼ˆ1 ä¸ªæœˆï¼‰é¢„æµ‹æ—¥æœŸ
future = prophet_model.make_future_dataframe(periods=20, freq='B')  # 'B' = Business Day
forecast = prophet_model.predict(future)

# æå–é¢„æµ‹ç»“æœï¼ˆæœªæ¥éƒ¨åˆ†ï¼‰
forecast_future = forecast[forecast['ds'] > df_prophet['ds'].max()]
```

---

## âœ… Step 2: æ„å»ºå¹¶è®­ç»ƒ LSTM æ¨¡å‹

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# è®¾å¤‡é€‰æ‹©ï¼ˆå…¼å®¹ CPU/GPUï¼‰
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)

# è‡ªå®šä¹‰ Dataset
class FundDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

# DataLoader
train_loader = DataLoader(FundDataset(X_train, y_train), batch_size=32, shuffle=True)
test_loader = DataLoader(FundDataset(X_test, y_test), batch_size=32, shuffle=False)

# LSTM æ¨¡å‹
class LSTMModel(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, forecast_horizon)

    def forward(self, x):  # x: (B, T, 1)
        _, (hn, _) = self.lstm(x)
        out = self.fc(hn[-1])
        return out

# å®ä¾‹åŒ–å¹¶è®­ç»ƒ
lstm_model = LSTMModel().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)

# LSTM è®­ç»ƒ
for epoch in range(10):
    lstm_model.train()
    total_loss = 0
    for Xb, yb in train_loader:
        Xb, yb = Xb.unsqueeze(-1).to(device), yb.to(device)
        pred = lstm_model(Xb)
        loss = criterion(pred, yb)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")
```

---

## âœ… Step 3: LSTM é¢„æµ‹æœªæ¥ 20 æ—¥ï¼ˆé€’æ¨ï¼‰

```python
# ä½¿ç”¨æœ€è¿‘ 30 å¤©æ•°æ®ä½œä¸ºè¾“å…¥
input_seq = torch.tensor(nav_series[-window_size:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)
lstm_preds = []

lstm_model.eval()
with torch.no_grad():
    for _ in range(20):
        pred = lstm_model(input_seq)
        next_val = pred[0, 0].item()
        lstm_preds.append(next_val)
        input_seq = torch.cat([input_seq[:, 1:, :], torch.tensor([[[next_val]]], device=device)], dim=1)
```

---

## âœ… Step 4: XGBoost è®­ç»ƒä¸é¢„æµ‹

```python
import xgboost as xgb

# åˆå§‹åŒ–æ¨¡å‹
xgb_model = xgb.XGBRegressor(n_estimators=100)
xgb_model.fit(X_train, y_train.ravel())

# é€’æ¨é¢„æµ‹æœªæ¥ 20 å¤©
xgb_input = list(nav_series[-window_size:])  # åˆå§‹åŒ–çª—å£
xgb_preds = []

for _ in range(20):
    features = np.array(xgb_input[-window_size:]).reshape(1, -1)
    next_val = xgb_model.predict(features)[0]
    xgb_preds.append(next_val)
    xgb_input.append(next_val)
```

---

ğŸ¯ **ä¸‹ä¸€æ­¥ Part B** å°†åŒ…æ‹¬ï¼š

* å¯è§†åŒ–ï¼šå±•ç¤ºå†å²æ•°æ® + ä¸‰ç§æ¨¡å‹é¢„æµ‹æ›²çº¿ï¼ˆçº¢çº¿ã€ç»¿çº¿ã€è“çº¿ï¼‰
* å›¾ä¾‹æ ‡æ³¨ã€ç½®ä¿¡åŒºé—´å±•ç¤º
* å¤šæ¨¡å‹å¯¹æ¯”å›¾ï¼ˆæ”¯æŒä¿å­˜å¯¼å‡ºï¼‰

æ˜¯å¦ç»§ç»­è¾“å‡º **Part Bï¼ˆå¯è§†åŒ–éƒ¨åˆ†ï¼‰**ï¼Ÿåªéœ€å›å¤â€œç»§ç»­â€ã€‚


å¥½çš„ï¼Œç»§ç»­è¾“å‡ºï¼š

---

# âœ… Part Bï¼šä¸‰ç§æ¨¡å‹é¢„æµ‹ç»“æœå¯è§†åŒ–å¯¹æ¯”å›¾

æˆ‘ä»¬å°†ç»˜åˆ¶ä¸€ä¸ªå›¾ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

* âœ… å†å²æˆäº¤ä»·ï¼ˆé»‘è‰²ç‚¹ï¼‰
* âœ… Prophet é¢„æµ‹ï¼šçº¢çº¿ + çº¢ç‚¹ + æµ…è“è‰²ç½®ä¿¡åŒºé—´
* âœ… LSTM é¢„æµ‹ï¼šç»¿è‰²çº¿ + ç»¿è‰²åœ†ç‚¹
* âœ… XGBoost é¢„æµ‹ï¼šè“è‰²è™šçº¿ + è“è‰²æ–¹å—ç‚¹

---

## âœ… å¯è§†åŒ–ä»£ç ï¼ˆProphet vs LSTM vs XGBoostï¼‰

```python
import matplotlib.pyplot as plt
import pandas as pd

# --------- Step 1: è®¾ç½®æ—¶é—´çª—å£ ---------
end_date = forecast['ds'].max()
start_date = end_date - pd.DateOffset(months=4)  # æ˜¾ç¤ºè¿‘3ä¸ªæœˆå†å² + 1ä¸ªæœˆé¢„æµ‹

# --------- Step 2: ç­›é€‰å†å²æ•°æ®å’Œ Prophet ç»“æœ ---------
forecast_plot = forecast[(forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)]
history_plot = df_prophet[(df_prophet['ds'] >= start_date) & (df_prophet['ds'] <= end_date)]

# æå– Prophet çš„é¢„æµ‹æ®µï¼ˆæœªæ¥ 20 å¤©ï¼‰
future_start = df_prophet['ds'].max()
forecast_future = forecast_plot[forecast_plot['ds'] > future_start]

# --------- Step 3: æ„å»ºç»Ÿä¸€çš„æœªæ¥æ—¥æœŸç´¢å¼•ï¼ˆç”¨äºå¯¹é½ LSTM/XGBoostï¼‰ ---------
future_dates = forecast_future['ds'].reset_index(drop=True)  # Series

# --------- Step 4: ç»˜å›¾ ---------
plt.figure(figsize=(14, 6))

# ğŸ”µ Prophetç½®ä¿¡åŒºé—´
plt.fill_between(forecast_plot['ds'],
                 forecast_plot['yhat_lower'],
                 forecast_plot['yhat_upper'],
                 color='blue', alpha=0.15, label='Prophet CI')

# âš« å†å²çœŸå®å€¼
plt.scatter(history_plot['ds'], history_plot['y'], color='black', s=15, label='Historical NAV')

# ğŸ”´ Propheté¢„æµ‹ï¼šçº¢çº¿ + çº¢ç‚¹
plt.plot(forecast_plot['ds'], forecast_plot['yhat'], color='red', linewidth=2, label='Prophet Forecast')
plt.scatter(forecast_future['ds'], forecast_future['yhat'], color='red', s=30)

# ğŸŸ¢ LSTMé¢„æµ‹ï¼šç»¿çº¿ + ç»¿ç‚¹
plt.plot(future_dates, lstm_preds, color='green', linestyle='-', marker='o', label='LSTM Forecast')

# ğŸ”· XGBoosté¢„æµ‹ï¼šè“è™šçº¿ + è“æ–¹å—ç‚¹
plt.plot(future_dates, xgb_preds, color='blue', linestyle='--', marker='s', label='XGBoost Forecast')

# --------- Step 5: å›¾å½¢ç¾åŒ– ---------
plt.title('Fund Net Value Forecast: Prophet vs LSTM vs XGBoost', fontsize=14)
plt.xlabel('Date')
plt.ylabel('Unit NAV')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

---

## âœ… æ•ˆæœè¯´æ˜ï¼š

| æ¨¡å‹      | æ›²çº¿é¢œè‰²   | ç‚¹å½¢çŠ¶     | ç‰¹å¾         |
| ------- | ------ | ------- | ---------- |
| Prophet | ğŸ”´ çº¢çº¿  | ğŸ”´ çº¢åœ†ç‚¹  | å¸¦ç½®ä¿¡åŒºé—´çš„ä¸­å€¼é¢„æµ‹ |
| LSTM    | ğŸŸ¢ ç»¿çº¿  | ğŸŸ¢ ç»¿åœ†ç‚¹  | é€’æ¨é¢„æµ‹       |
| XGBoost | ğŸ”· è“è™šçº¿ | ğŸ”· è“æ–¹å—ç‚¹ | é€’æ¨é¢„æµ‹       |
| å†å²æ•°æ®    | âš« é»‘ç‚¹   | -       | çœŸå®å€¼        |

---

## ğŸ“ï¼ˆå¯é€‰ï¼‰å¯¼å‡ºé¢„æµ‹æ•°æ®ä¸º CSV

```python
future_df = pd.DataFrame({
    'date': future_dates,
    'prophet': forecast_future['yhat'].values,
    'lstm': lstm_preds,
    'xgboost': xgb_preds
})
future_df.to_csv('future_forecast_comparison.csv', index=False)
```

---

## âœ… ç»“æŸè¯­

ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„æµç¨‹ï¼š

1. æ•°æ®æ¸…æ´—ä¸æ»‘åŠ¨çª—å£æ„å»ºï¼›
2. ä¸‰ç§æ¨¡å‹ï¼ˆProphet, LSTM, XGBoostï¼‰å»ºæ¨¡ï¼›
3. å¤šæ¨¡å‹å¯¹æ¯”å›¾æ¸…æ™°å±•ç¤ºé¢„æµ‹èƒ½åŠ›ï¼›
4. å¯å¯¼å‡ºç»“æœç”¨äºæŠ¥å‘Šæˆ–åç»­ç­–ç•¥å¼€å‘ã€‚

---

å¦‚ä½ å¸Œæœ›æˆ‘ï¼š

* ä¸ºè¿™ä¸ª Notebook æ·»åŠ äº¤äº’æ§ä»¶ï¼ˆé¢„æµ‹å¤©æ•°ã€çª—å£é•¿åº¦ï¼‰
* æŠŠ Notebook å¯¼å‡ºä¸º PDF / HTML æŠ¥å‘Šæ¨¡æ¿
* æ·»åŠ åŸºäºé¢„æµ‹ç»“æœçš„ç®€å•ä¹°å–ä¿¡å·ç­–ç•¥

éƒ½å¯ä»¥ç»§ç»­å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ æ‰©å±•è¿™ä¸ª Notebookã€‚æ˜¯å¦éœ€è¦ï¼Ÿ



# å°è£…

éå¸¸å¥½ï¼ä¸‹é¢æˆ‘å°†ä¸ºä½ ç¼–å†™ä¸€ä¸ª**å®Œæ•´çš„â€œæ¯æ—¥è¿½è¸ªé¢„æµ‹ç³»ç»Ÿâ€æ¨¡å—**ï¼Œç”¨äºï¼š

---

## âœ… åŠŸèƒ½ç›®æ ‡

| æ¨¡å—           | åŠŸèƒ½æè¿°                                   |
| ------------ | -------------------------------------- |
| 1ï¸âƒ£ è‡ªåŠ¨ç”Ÿæˆé¢„æµ‹   | æ¯æ¬¡è¿è¡Œè‡ªåŠ¨è¿›è¡Œ Prophet/LSTM/XGBoost é¢„æµ‹       |
| 2ï¸âƒ£ è‡ªåŠ¨è®°å½•é¢„æµ‹æ—¥æœŸ | æ¯ä¸€æ¡é¢„æµ‹è®°å½•éƒ½æ ‡è®° `generated_on` æ—¶é—´æˆ³          |
| 3ï¸âƒ£ è‡ªåŠ¨ä¿å­˜æ—¥å¿—   | å°†é¢„æµ‹ç»“æœ**è¿½åŠ **åˆ°ç»Ÿä¸€çš„ `forecast_log.csv` æ–‡ä»¶ä¸­ |
| 4ï¸âƒ£ é¿å…é‡å¤å†™å…¥   | è‡ªåŠ¨æ£€æŸ¥æ˜¯å¦å½“å¤©å·²ç»å†™å…¥ï¼Œé˜²æ­¢é‡å¤æ’å…¥                    |

---

## âœ… æ–‡ä»¶ç»“æ„å»ºè®®

```
ğŸ“ your_project/
â”œâ”€â”€ forecast_log.csv               â† æ¯æ—¥ç´¯è®¡é¢„æµ‹ç»“æœï¼ˆæŒç»­å¢é•¿ï¼‰
â”œâ”€â”€ 007844_history_raw.csv        â† åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆæ¯æ—¥æ›´æ–°ï¼‰
â””â”€â”€ run_daily_forecast.py         â† æ¯æ—¥è¿è¡Œä¸€æ¬¡å³å¯
```

---

## âœ… âœ¨ å…¨æµç¨‹ä»£ç ï¼ˆå°è£…ä¸º daily\_forecast.py è„šæœ¬ï¼‰

```python
import pandas as pd
import numpy as np
import os
from prophet import Prophet
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import xgboost as xgb
from datetime import datetime

# ---------- Step 1: åŠ è½½æ•°æ® ----------
data_path = '007844_history_raw.csv'
df = pd.read_csv(data_path)
df.columns = ['date', 'unit_nav', 'acc_nav', 'daily_growth', 'buy_status', 'sell_status']
df['date'] = pd.to_datetime(df['date'])
df = df[['date', 'unit_nav']].dropna().sort_values('date').reset_index(drop=True)

# ---------- Step 2: é¢„æµ‹ç”Ÿæˆæ—¥ ----------
generated_on = df['date'].max()

# ---------- Step 3: Prophet æ¨¡å‹ ----------
df_prophet = df.rename(columns={'date': 'ds', 'unit_nav': 'y'})
prophet_model = Prophet(daily_seasonality=True)
prophet_model.fit(df_prophet)
future = prophet_model.make_future_dataframe(periods=20, freq='B')
forecast = prophet_model.predict(future)
forecast_future = forecast[forecast['ds'] > df_prophet['ds'].max()]
future_dates = forecast_future['ds'].reset_index(drop=True)

# ---------- Step 4: LSTM æ¨¡å‹ ----------
def create_supervised_data(data, window_size=30, forecast_horizon=1):
    X, y = [], []
    for i in range(len(data) - window_size - forecast_horizon + 1):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size:i+window_size+forecast_horizon])
    return np.array(X), np.array(y)

nav_series = df['unit_nav'].values
window_size = 30
forecast_horizon = 1
X, y = create_supervised_data(nav_series, window_size, forecast_horizon)
split_idx = int(len(X) * 0.8)
X_train, y_train = X[:split_idx], y[:split_idx]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FundDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

train_loader = DataLoader(FundDataset(X_train, y_train), batch_size=32, shuffle=True)

class LSTMModel(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, forecast_horizon)
    def forward(self, x):
        _, (hn, _) = self.lstm(x)
        return self.fc(hn[-1])

lstm_model = LSTMModel().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)

for epoch in range(5):
    lstm_model.train()
    for Xb, yb in train_loader:
        Xb, yb = Xb.unsqueeze(-1).to(device), yb.to(device)
        pred = lstm_model(Xb)
        loss = criterion(pred, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

lstm_model.eval()
input_seq = torch.tensor(nav_series[-window_size:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)
lstm_preds = []
with torch.no_grad():
    for _ in range(20):
        pred = lstm_model(input_seq)
        next_val = pred[0, 0].item()
        lstm_preds.append(next_val)
        input_seq = torch.cat([input_seq[:, 1:, :], torch.tensor([[[next_val]]], device=device)], dim=1)

# ---------- Step 5: XGBoost æ¨¡å‹ ----------
xgb_model = xgb.XGBRegressor(n_estimators=100)
xgb_model.fit(X_train, y_train.ravel())

xgb_input = list(nav_series[-window_size:])
xgb_preds = []
for _ in range(20):
    features = np.array(xgb_input[-window_size:]).reshape(1, -1)
    next_val = xgb_model.predict(features)[0]
    xgb_preds.append(next_val)
    xgb_input.append(next_val)

# ---------- Step 6: æ„å»ºæœ€ç»ˆ DataFrame ----------
result_df = pd.DataFrame({
    'date': future_dates,
    'prophet': forecast_future['yhat'].values,
    'lstm': lstm_preds,
    'xgboost': xgb_preds,
    'generated_on': generated_on
})

# ---------- Step 7: è‡ªåŠ¨ä¿å­˜è‡³æ—¥å¿—æ–‡ä»¶ ----------
log_file = 'forecast_log.csv'
if not os.path.exists(log_file):
    result_df.to_csv(log_file, index=False)
else:
    # æ£€æŸ¥æ˜¯å¦å·²ç»é¢„æµ‹è¿‡å½“å¤©
    existing = pd.read_csv(log_file)
    existing['generated_on'] = pd.to_datetime(existing['generated_on'])
    if generated_on not in existing['generated_on'].unique():
        result_df.to_csv(log_file, mode='a', header=False, index=False)
    else:
        print(f"ğŸ“Œ ä»Šå¤©ï¼ˆ{generated_on.date()}ï¼‰çš„é¢„æµ‹å·²å­˜åœ¨ï¼Œæœªé‡å¤å†™å…¥ã€‚")
```

---

## âœ… æ¯æ—¥ä½¿ç”¨æ–¹æ³•

1. ç¡®ä¿ä½ æ¯å¤©éƒ½æœ‰æ›´æ–°çš„ `007844_history_raw.csv` æ–‡ä»¶ï¼›
2. æ¯å¤©è¿è¡Œä¸€æ¬¡è¯¥è„šæœ¬ï¼ˆå¯ç”¨å®šæ—¶å™¨ã€crontabã€Jupyter è°ƒåº¦ï¼‰ï¼›
3. æ‰€æœ‰ç»“æœä¼šç´¯è®¡ä¿å­˜åœ¨ `forecast_log.csv` æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿åæœŸè¯„ä¼°ã€‚

---

## âœ… åç»­æ‰©å±•å»ºè®®

| åŠŸèƒ½       | è¯´æ˜                                           |
| -------- | -------------------------------------------- |
| ğŸ“ˆ è‡ªåŠ¨ç”»å›¾  | æ¯å¤©è¿è¡Œåè¾“å‡ºå½“å¤©é¢„æµ‹æ›²çº¿å›¾                               |
| ğŸ“Š å‡†ç¡®æ€§è¯„ä¼° | æ¯è¿‡å‡ å¤©å¯¹æ¯” `generated_on` ä¸çœŸå® `unit_nav`ï¼Œè¯„ä¼° RMSE |
| ğŸ“¤ æ•°æ®ä¸Šä¼   | è‡ªåŠ¨æ¨é€ç»“æœåˆ°é‚®ç®±ã€æ•°æ®åº“æˆ–å¯è§†åŒ–å¹³å°                          |

---

å¦‚ä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ„å»º **é¢„æµ‹å‡†ç¡®ç‡è¯„ä¼°å™¨** æˆ– **å®Œæ•´çš„ Streamlit Web UI** æ¥å±•ç¤ºè¿™å¥—ç³»ç»Ÿã€‚æ˜¯å¦éœ€è¦ç»§ç»­ï¼Ÿ
