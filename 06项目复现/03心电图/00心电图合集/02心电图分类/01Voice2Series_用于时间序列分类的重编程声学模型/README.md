# Voice2Series: Reprogramming Acoustic Models for Time Series Classification

Voice2Series: 用于时间序列分类的重编程声学模型

[Voice2Series: Reprogramming Acoustic Models for Time Series Classification](https://paperswithcode.com/paper/voice2series-reprogramming-acoustic-models)



> Chao-Han Huck Yang1 Yun-Yun Tsai 2 Pin-Yu Chen3
> 2022

## 摘要
在有限数据的情况下学习对时间序列进行分类是一个实用但具有挑战性的问题。当前的方法主要基于手工设计的特征提取规则或特定领域的数据增强。受深度语音处理模型的进步以及语音数据是单变量时间信号的事实启发，本文提出了一种新颖的端到端方法Voice2Series（V2S），该方法通过输入转换学习和输出标签映射，重新编程声学模型以用于时间序列分类。利用大规模预训练语音处理模型的表示学习能力，在30个不同的时间序列任务上，我们展示了V2S在19个时间序列分类任务上取得了有竞争力的结果。我们进一步通过证明V2S的总体风险是源风险的上界和一个通过重新编程实现特征对齐的Wasserstein距离来为V2S提供理论证明。我们的结果为时间序列分类提供了新的有效手段。我们的代码可在以下链接获取：https://github.com/huckiyang/Voice2Series-Reprogramming。

## 1. 引言
机器学习在时间序列数据上的应用在多个领域都有丰富的应用，包括医疗诊断（例如，生理信号如心电图（ECG）(Kampouraki等人，2008)）、金融/天气预测，以及工业测量（例如，传感器和物联网（IoT)）。值得注意的是，阻止时间序列学习任务使用现代大规模深度学习模型的一个常见实际挑战是数据稀缺。尽管已经做出了许多努力（Fawaz等人，2018; Ye & Dai, 2018; Kashiparekh等人，2019）来推进时间序列分类的迁移学习和模型适应，但缺乏一种原则性的方法，其性能可能无法与传统的统计学习基准（Langkvist等人，2014）相媲美。

为了弥补这一差距，我们提出了一种名为voice to series（V2S）的新方法，通过重新编程预训练的声学模型（例如，口语术语识别模型）来进行时间序列分类。与一般的时间序列任务不同，现代的声学模型是在大规模人类语音数据集上训练的，并被认为是一种成熟的技术，广泛部署在智能电子设备中。V2S的基本原理在于，语音数据可以被视为单变量时间信号，因此一个训练良好的声学模型很可能被重新编程为一个强大的特征提取器，用于解决时间序列分类任务。图1展示了所提出的V2S框架的示意图，包括（a）可训练的重新编程层，（b）预训练的声学模型（AM），以及（c）源（人类语音）和目标（时间序列）标签之间的特定标签映射函数。

模型重新编程最初在（Elsayed等人，2019）中提出。作者展示了如何学习一个通用的输入转换函数，以重新编程预训练的ImageNet模型（不改变模型权重），以高准确度解决MNIST/CIFAR-10图像分类和简单的基于视觉的计数任务。它可以被视为一种有限数据的高效迁移学习方法，并在生物医学图像分类任务上取得了最先进的（SOTA）结果（Tsai等人，2020）。然而，尽管有实证成功，关于重新编程如何以及为什么能够成功，人们知之甚少。

与现有工作不同，本文旨在解决以下三个基本问题：（i）声学模型能否被重新编程用于时间序列分类？（ii）V2S能否超越SOTA时间序列分类结果？（iii）重新编程为何有效，有没有理论证明？

我们的主要贡献在本文中提供了对上述基本问题的肯定回答，总结如下。

1. 我们提出了V2S，这是一种新颖且统一的方法，用于重新编程大规模预训练的声学模型以执行不同的时间序列分类任务。据我们所知，V2S是第一个实现时间序列任务重新编程的框架。

2. 在标准UCR时间序列分类基准（Dau等人，2019）上进行测试，V2S在30个数据集中的19个上表现出竞争力，这表明V2S可能是时间序列分类的一种潜在有效方法。

3. 在第4节中，我们开发了一种理论风险分析，以通过源风险和表示对齐损失来描述重新编程在目标任务上的性能。在第5节中，我们还展示了如何使用我们的理论结果来评估重新编程的性能。此外，我们通过听觉神经显著性图和嵌入可视化来解释V2S。

## 2. 相关工作
2.1. 时间序列分类
学习对时间序列进行分类是机器学习和信号处理中的一个标准研究课题。一个主要的研究分支使用设计好的特征，然后是传统的分类器，例如与支持向量机（SVM）（Kampouraki等人，2008）一起使用的数字滤波器设计，决策树（Geurts，2001）或基于核的方法（Zhang等人，2010；Lines等人，2012）。最近，深度学习模型已被用于时间序列（Fawaz等人，2019），并展示了改进的性能。这些方法从完全端到端的分类器（Zhang等人，2010；Wang等人，2017）到结合了特征工程和深度学习的混合模型（Hong等人，2019）。值得注意的是，特征工程方法（Wang等人，2017）在一些时间序列分类任务上仍然可以获得有竞争力的结果，特别是当训练数据数量较少时。

2.2. 模型重新编程
尽管在原始论文（Elsayed等人，2019）中，模型重新编程被称为“对抗性”重新编程，但它不仅限于对抗性设置，也不涉及任何对抗性训练。Elsayed等人（2019）展示了预训练的ImageNet模型可以被重新编程用于分类其他图像数据集和解决基于视觉的计数任务。Tsai等人（2020）展示了重新编程在标签有限数据（如生物医学图像分类）上的优势，并使用零阶优化（Liu等人，2020）来实现黑盒机器学习系统的重新编程。

除了图像数据，模型重新编程已在自然语言处理（NLP）（Neekhara等人，2019；Hambardzumyan等人，2020）中使用，例如机器翻译和情感分类。Vinod等人（2020）进一步展示了NLP模型可以被重新编程用于生物化学中的分子学习任务。

对于时间序列的重新编程，目前还不清楚应该使用哪个源域和预训练模型。当前关于重新编程的研究工作没有扩展到时间序列，因为图像和文本数据与时间序列在特征特性上有根本的不同。我们的主要贡献之一是提出在丰富的人类语音数据上重新编程预训练的声学模型以进行时间序列分类。据我们所知，我们的V2S是第一个用于时间序列的重新编程框架。

2.3. 深度声学建模
最近的深度学习模型在从声学信息预测标签方面展示了令人印象深刻的结果。核心思想是使用大量的频谱特征（例如，Mel-spectrogram或log-power spectrum）作为训练输入，以捕捉重要特征。从声学模型（AM）学习的一些潜在特征基于生理听觉实验（例如，皮层响应（Kaya & Elhilali，2017））或神经显著性方法是可解释的。已经投入了大量努力设计大型和深度神经网络，以从人类语音数据集中提取特征。其中，残差神经网络（ResNet）（He等人，2016；Saon等人，2017）和VGGish（Hershey等人，2017）模型是声学模型任务（如口语术语识别（Yang等人，2021a；de Andrade等人，2018）和语音增强（Xu等人，2014；Yang等人，2020））的流行骨干网络。值得注意的是，通过参数微调的标准迁移学习对于数据有限的时间序列任务并不理想，因为声学数据和时间序列数据在规模上通常非常不同。我们的V2S通过学习输入转换函数同时固定预训练的AM来解决这个问题。

## 3. Voice2Series (V2S)

### 3.1 数学符号

表1总结了本文中用于V2S重新编程的主要数学符号。在本文中，我们将预训练在语音数据上的K路声学分类模型称为源模型，并使用术语目标数据来表示单变量时间序列数据。

表1. 重新编程的数学符号
- 符号 (Symbol) 含义 (Meaning)
- S / T 源域/目标域 (source/target domain)
- XS / XT 源数据/目标数据样本的空间 (space of source/target data samples)
- YS / YT 源数据/目标数据标签的空间 (space of source/target data labels)
- DS ⊆ XS × YS / DT ⊆ XT × YT 源数据/目标数据分布 (source/target data distribution)
- (x, y) ∼ D 从分布D中抽取的数据样本和独热编码标签 (data sample and one-hot encoded label drawn from distribution D)
- K 源标签的数量 (number of source labels)
- fS : Rd → [0, 1]K 预训练的K路源分类模型 (pre-trained K-way source classification model)
- η : RK → [0, 1]K 神经网络中的softmax函数 (softmax function in neural network)
- z(·) ∈ RK logit（预softmax）表示 (logit (pre-softmax) representation)
- ℓ(x, y) ≜ ∥f(x) − y∥2 基于分类器f的(x, y)的风险函数 (risk function of (x, y) based on classifier f)
- ED[ℓ(x, y)] ≜ E(x,y) ∼D[ℓ(x, y)] = ED∥f(x) − y∥2 基于分类器f的总体风险 (population risk based on classifier f)
- δ, θ 目标数据的可加输入转换，由θ参数化 (additive input transformation on target data, parameterized by θ)
- P 保留用于表示概率函数。其余符号将在适用时介绍 (The notation P is reserved for denoting a probability function. The remaining notations will be introduced when applicable.)

### 3.2 V2S在数据输入上的重新编程

在这里，我们制定了V2S在数据输入上重新编程的问题。设xt ∈ XT ⊆ RdT 表示来自目标域的单变量时间序列输入，其中dT 表示时间特征。我们的目标是找到一个可训练的输入转换函数H，它是通用于所有目标数据输入的，其目的是将xt重新编程到源数据空间XS ⊆ RdS中，其中dT < dS。具体来说，重新编程的样本x′t被制定为：

x′t = H(xt; θ) := Pad(xt) + M ⊙ θ

(1)

其中Pad(xt)是一个零填充函数，输出维度为dS的零填充时间序列。xt在x′t中的位置是一个设计参数，我们将在第5.2节中讨论。M ∈ {0, 1}dS 是一个二进制掩码，指示xt在其零填充输入Pad(xt)中的位置，其中M的第i个条目为0表示xt存在（表示该条目不可重新编程），否则为1（表示该条目未被占用，因此可以重新编程）。⊙ 表示逐元素乘积。最后，θ ∈ RdS 是一组可训练参数，用于对齐源域和目标域数据分布。可以考虑在我们的重新编程函数中使用更复杂的函数W(θ)。但在实践中，与简单函数θ相比，我们没有观察到显著的增益。在接下来的内容中，我们将使用术语δ ≜ M ⊙ θ来表示V2S重新编程的可训练加性输入转换。此外，为了表示方便，我们将省略填充符号，简单地使用xt + δ来表示重新编程的目标数据，将“+”操作视为零填充广播函数。

### 3.3 V2S在声学模型（AMs）上的重新编程

我们选择一个预训练的深度声学分类模型作为源模型（fS）进行模型重新编程。我们假设源模型的最后层是softmax，并为每个源标签输出非负置信度得分（预测概率）。通过转换后的数据输入H(xt; θ)（如(1)所述），我们可以在重新编程的目标数据样本xt上获得源模型fS的类预测，表示为：

P(ys|fS(H(xt; θ))), 对于所有ys ∈ YS (2)

接下来，如图1所示，我们分配一个（多对一）标签映射函数h，将源标签映射到目标标签。对于目标标签yt ∈ YT，其类预测将是分配给它的源标签集合上的平均类预测。我们使用术语P(h(YS)|fS(H(xt; θ)))来表示与关联的真实目标标签yt = h(YS)相关的目标任务的预测概率。最后，我们通过优化以下目标来学习数据输入重新编程的最优参数θ*：

θ* = arg min θ − log P(h(YS)|fS(H(xt; θ))) 

V2S loss ≜ L

(3)

其中h (YS) = yt。优化将通过在所有目标域训练数据对{xt, yt}上评估的经验损失（V2S loss L）来实现，以求解θ*。

在实践中，我们发现多对一标签映射可以提高重新编程的准确性，与一对一标签映射相比，这与(Tsai等人，2020)的发现相似。下面我们通过一个具体的例子来说明如何在V2S重新编程中使用多对一标签映射。考虑将口语术语AM重新编程为ECG分类的情况。我们可以选择将源任务中的多个（但不重叠）类别（例如，'yes'、'no'、'up'、'down'在AM类别中）映射到目标任务的每个类别（例如，ECG类别中的'Normal'或'Ischemia'），从而得到一个特定的映射函数h。让B ⊂ YS表示映射到目标标签yt ∈ YT 的源标签集合。然后，基于V2S重新编程的yt的类预测是分配给源标签的聚合预测，定义为：

P(yt|fS(H(xt; θ))) = 1/|B| ∑ys∈B P(ys|fS(H(xt; θ)))

(4)

其中|B|表示B中标签的数量。在我们的实现中，我们使用随机（但不重叠）的多对一映射在源和目标标签之间。每个目标标签被分配相同数量的源标签。

### 3.4 V2S算法

算法1总结了我们提出的V2S重新编程算法的训练过程。该算法使用ADAM优化器（Kingma & Ba，2015）来找到最优的重新编程参数θ*，以最小化定义在(3)中的V2S损失L，该损失在所有目标域训练数据上评估。在我们的算法1实现中，我们使用小批量的随机优化。

算法1 Voice to Series (V2S) 重新编程

1: 输入：预训练的声学模型fS，V2S损失L在(3)中，目标域训练数据{(x(i) t , y(i) t }n i=1，掩码函数M，多标签映射函数h(·)，最大迭代次数T，初始学习率α
2: 输出：最优的重新编程参数θ*
3: 随机初始化θ；设置t = 0
4: # 生成重新编程的数据输入 H(x(i) t ; θ) = Pad(x(i) t ) + M ⊙ θ，对于所有 i = {1, 2, . . . , n}
5: # 计算V2S损失 L 从方程(3)中 L(θ) = − 1/n ∑i=1n log P(y(i) t |fS(H(x(i) t ); θ))
6: # 解决重新编程参数 使用ADAM优化器基于L(θ)求解θ*

## 4. 通过重新编程的总体风险

为了提供V2S有效性的理论证明，我们在接下来的内容中建立了正式的总体风险分析，并证明了基于V2S，目标任务的总体风险由源总体风险和源数据与重新编程目标数据的logit表示之间的Wasserstein-1距离的和上界。我们的分析符合直觉，即具有高准确度（低总体风险）的源模型与更好的源-目标数据对齐（小Wasserstein-1距离）应该表现出更好的重新编程性能。

在第5.4节中，我们展示了我们推导出的总体风险界限可以用来评估不同源模型和目标任务的V2S重新编程性能。我们还注意到，我们的理论分析不仅限于V2S重新编程。它适用于通用分类任务。

使用表1中总结的数学符号，源模型是一个预训练的K路神经网络分类器fS(·) = η(zS(·))，其中softmax层η(·)作为最终模型输出。在我们的分析中省略了模型参数的符号，因为重新编程不会改变预训练模型参数。符号(x, y)用来描述数据样本x及其独热编码标签y。我们将使用下标s/t来表示源/目标数据（适用时）。为了分析的目的，给定一个神经网络分类器f，我们考虑均方根误差（RMSE）表示为∥f(x) − y∥2。

为了提出我们的分析，我们基于重新编程框架做出以下假设：

1. 源风险是ϵS，即EDS[ℓ(xs, ys)] = ϵS。
2. 源-目标标签空间有一个特定的双射一对一标签映射函数ht，对于每个目标标签t，使得∀yt ∈ YT，yt = ht(YS) ≜ ys ∈ YS，且ht ≠ ht′如果t ≠ t′。
3. 基于重新编程，目标损失函数ℓT可以通过加性输入转换函数δ表示为ℓT (xt + δ, yt) (a) = ℓT (xt + δ, ys) (b) = ℓS(xt + δ, ys)，其中(a)是由标签映射（假设2）引起的，(b)是由目标数据重新编程源损失引起的。
4. 重新编程的学习输入转换函数由δ* ≜ arg minδ EDT [ℓS(xt + δ, ys)]表示，它是目标总体风险与重新编程损失目标的最小化器。
5. 源和目标数据的域独立抽取：让ΦS(·)和ΦT (·)分别表示源数据和目标数据分布在XS和XT上的概率密度函数。联合概率密度函数是它们边缘的乘积，即ΦS,T (xs, xt) = ΦS(xs) · ΦT (xt)。

对于给定的神经网络分类器，以下引理将模型在两个不同域上的预测的预期RMSE与它们在logit表示上相应概率测度之间的Wasserstein-1距离联系起来，这将在表征重新编程的总体风险中发挥关键作用。Wasserstein距离是两个概率测度µ和µ′之间的统计距离，它已被广泛用于研究最优运输问题（Peyr´e & Cuturi, 2018）。具体地，对于任何p ≥ 1，Wasserstein-p距离定义为：

Wp(µ, µ′) = inf π∈Π(µ,µ′) ∥x − x′∥pdπ(x, x′) 1/p，

其中Π(µ, µ′)表示所有具有边缘µ和µ′的联合分布π。

引理1：给定一个K路神经网络分类器f(·) = η(z(·))。让µz和µ′z是来自两个数据域D和D′的logit表示{z(x)}和{z(x′)}的概率测度，其中x ∼ D和x′ ∼ D′。假设x和x′的独立抽取，即ΦD,D′(x, x′) = ΦD(x) · ΦD′(x′)。

Ex∼D, x′∼D′∥f(x) − f(x′)∥2 ≤ 2 √K · W1(µz, µ′z)，

其中W1(µz, µ′z)是µz和µ′z之间的Wasserstein-1距离。证明：请参见附录A。

有了引理1，我们现在陈述关于重新编程的总体风险上界的主要定理。

定理1：让δ*表示重新编程的学习加性输入转换（假设4）。通过重新编程一个K路源神经网络分类器fS(·) = η(zS(·))，目标任务的总体风险，表示为EDT [ℓT (xt + δ*, yt)]，由以下上界给出：

EDT [ℓT (xt + δ*, yt)] ≤ ϵS + 2 √K · W1(µ(zS(xt + δ*)), µ(zS(xs)))xt∼DT , xs∼DS ，

其中W1(µz, µ′z)是µz和µ′z之间的Wasserstein-1距离。证明：请参见附录B。

定理1表明，通过重新编程的目标总体风险由两部分组成：(i) 源总体风险ϵS，和(ii) 基于相同源神经网络分类器fS(·) = η(zS(·))的源数据zS(xs)和重新编程目标数据zS(xt + δ*)之间的表示对齐损失，通过它们的Wasserstein-1距离来衡量。结果表明，当源模型具有较低的源损失和较小的表示对齐损失时，重新编程可以达到更好的性能（更低的风险）。

在极端情况下，如果源和目标表示可以完全对齐，Wasserstein-1距离将变为0，因此通过重新编程的目标任务可以像源任务一样表现。另一方面，如果表示对齐损失很大，则可能会主导源风险并阻碍目标任务的性能。在下一节中，我们将研究表示对齐损失如何为V2S的重新编程性能提供信息。我们还想最后评论一下，我们的风险分析可以扩展到加性输入转换设置之外，通过考虑更复杂的函数输入转换函数g(xt)（例如，仿射转换）。然而，在实践中，我们在V2S中观察到这样做的收益很小，因此专注于加性输入转换设置。

## 5. 性能评估

### 5.1 声学模型（AMs）和源数据集

我们首先介绍源模型和它们训练的数据集。预训练的源模型将在我们的V2S实验中使用。

有限词汇量语音命令数据集：为了为我们的实验创建一个大规模（约100k训练样本）的预训练声学模型，我们选择了Google Speech Commands V2（Warden，2018）（称为GSCv2）数据集，其中包含来自2,618名录制者的105,829个35个单词的语音，采样率为16 kHz。我们还提供了一些关于在其他声学数据集（例如，AudioSet（Gemmeke等人，2017））上训练的模型的讨论，见附录D。总的来说，我们发现在相同的网络架构下，使用语音命令数据集训练的AMs在V2S性能上优于其他数据集，这可能归因于其与短长度（一秒或更短）时间序列数据的相似性。

基于注意力的AMs：为了训练源模型，我们使用了一种流行的基于transformer的单头注意力架构（de Andrade等人，2018）进行V2S重新编程，称为V2Sa（图2（a））。我们还训练了一个类似的架构，带有U-Net（Long等人，2015），称为V2Su（图2（b）），旨在增强声学任务中的特征提取（Yang等人，2021a）。预训练的V2Sa和V2Su模型具有可比的模型参数数量（约0.2M/0.3M）和测试准确率（96.90%/96.92%）。它们的数据输入维度是d ∼ 16k，因此重新编程函数θ有约16k个可训练参数。关于基于注意力的模型和其他流行神经网络架构的详细信息和比较，请参见附录C。

### 5.2 V2S实现和基线

V2S实现：我们使用TensorFlow（Abadi等人，2016）（v2.2）来实现我们的V2S框架，遵循算法1。为了使端到端V2S训练成为可能，我们使用Kapre工具包（Choi等人，2017）在GPU上集成了一个音频预处理层，如图2所示。对于算法1中的V2S参数，我们使用α = 0.05和32的小批量大小，T = 100个训练周期。我们使用最大多对一随机标签映射，为每个目标标签分配⌊ |YS| / |YT| ⌋个不重叠的源标签，其中|Y|是标签集Y的大小，⌊z⌋是不超过z的最大整数。为了稳定训练过程，我们在V2S损失中添加了权重衰减作为正则化项，并将正则化系数设置为0.04。我们在https://github.com/huckiyang/Voice2Series-Reprogramming上提供了V2S重新编程层的实现。

对于模型调优，我们在重新编程参数θ的训练中使用dropout。此外，在输入重新编程期间，我们还将目标信号xt复制成m个段，并从重新编程输入的开始处以相同的间隔放置它们（见图4（a）中m = 3的示例）。对于每个任务，我们在一组超参数中报告V2S的最佳结果，dropout率 ∈ {0, 0.1, 0.2, 0.3, 0.4}，目标信号复制次数m ∈ {1, 2, ..., 10}。我们使用训练数据的10折分割来选择基于验证损失表现最佳的模型，并报告测试数据的平均准确率，平均运行10次，这遵循了（Cabello等人，2020）中使用的类似实验设置。

迁移学习基线（TFa）：为了展示重新编程的有效性，我们还提供了一个使用相同V2Sa预训练模型的迁移学习基线。与V2S不同，这个基线（命名为TFa）不使用输入重新编程，而是允许使用零填充的目标数据微调预训练模型参数。为了训练，还包括了一个额外的任务依赖分类的密集层。

### 5.3 UCR时间序列分类基准

UCR档案（Dau等人，2019）是一个著名的基准，包含了大量时间序列分类数据集，具有默认的训练和测试数据分割。当前在测试准确率上的最佳结果（SOTA）是通过以下方法获得的：（i）深度神经网络，包括全卷积网络（FCN）和深度残差神经网络，如（Wang等人，2017）中报告的；（ii）基于特征框架（Schäfer，2015；Cabello等人，2020）；（iii）基于集成的框架（Hills等人，2014；Bagnall等人，2015；Lines等人，2018）；（iv）时间扭曲框架（Ratanamahatana & Keogh，2005）。

为了确保每个目标标签至少分配了3个独特的源标签，我们选择了UCR档案中的30个时间序列数据集进行V2S实验，这些数据集的目标标签数量≤ 10。我们注意到，对于每个数据集，实现当前结果的算法可能会有所不同，因此与V2S的比较可能是不公平的。

除了比较每个数据集的标准测试准确率以及所有数据集的平均和中位数准确率外，我们还报告了（Wang等人，2017）提出的平均每个类别错误（MPCE），这是一个用于多个数据集性能评估的单一指标。MPCE是J个数据集的每个类别错误（PCE）的总和，定义为MPCE = ∑j∈[J] PCEj = ej/cj，其中包括每个数据集的错误率（ej）和类别数量（cj）。

重新编程性能：表2总结了30个数据集上每种方法的性能。值得注意的是，我们重新编程的V2Sa模型在30个时间序列数据集中的19个上取得了更好或等效的结果，这表明V2S作为一种单一方法是时间序列分类的有竞争力和有前景的方法。迁移学习基线TFa的性能较差，这可以归因于有限的训练数据。V2Sa的平均/中位数准确率（准确率提高了1.84/2.63%）和MPCE（相对误差降低了约2.87%）优于当前结果，证明了V2S的有效性。对于大多数数据集，V2Sa的性能优于V2Su，这可以通过定理1通过较低的经验目标风险上界（见第5.4节）来解释。

### 5.4 表示对齐损失

根据定理1，目标风险由固定源风险和源数据与重新编程目标数据之间的表示对齐损失的和上界。后者通过它们的logit表示的Wasserstein-1距离来衡量。我们使用以下实验来实证验证V2S训练期间的表示对齐损失，并激励其用于重新编程性能评估。具体来说，为了计算效率，我们使用切片Wasserstein-2距离（SWD）（Kolouri等人，2018）来近似定理1中的Wasserstein-1距离。SWD通过调用1D最优传输（OT），使用一维（1D）随机投影（我们使用1,000次运行）来计算切片Wasserstein-2距离，与高维OT问题相比具有计算效率（Peyr´e & Cuturi，2018）。此外，Wasserstein-1距离由Wasserstein-2距离上界（Peyr´e & Cuturi，2018），因此SWD将作为确切表示对齐损失的良好近似。

训练期间的Wasserstein距离：使用表2中的DistalPhalanxTW（Davis，2013）数据集和V2Sa，图3显示了V2S训练期间的验证（测试）准确率、验证（测试）损失和SWD。可以观察到测试损失和SWD之间的相似趋势，这表明V2S确实通过逐渐使目标数据表示更接近源数据分布来学习重新编程目标数据，如定理1所示。

模型选择：基于定理1，可以利用我们推导出的风险界限进行V2S模型选择。比较V2Sa和V2Su，表3显示了源任务（GSCv2语音数据集（Warden，2018））的验证损失和表2中所有30个训练集的平均/中位数SWD。我们发现V2Sa确实具有较低的源损失和SWD之和，这解释了其在表2中的改进性能。

### 5.5 V2S解释的额外分析

为了进一步了解V2S，我们研究了它的声学显著性图和嵌入可视化。

注意力和类激活映射：为了解释V2S所做的预测，我们通过类激活映射（CAM）（Zhou等人，2016）提供了对重新编程特征的频谱图的神经显著性分析，使用了Worms数据集（Bagnall等人，2015）和V2Sa。激活和注意力映射方法（Wu & Lee，2019）已被用于听觉分析（Fritz等人，2007），并在神经生理学研究中调查了其与音频信号和大脑皮层激活之间的关系（Kaya & Elhilali，2017；Veale等人，2017）。有趣的是，如图4所示，预训练AM的相应注意力头（b）（在V2S过程中不可训练）仍然能够识别（a）中重新编程输入信号的原始时间模式。我们还展示了重新编程输入的Mel频谱图（c），表明了与加权输出预测相对应的激活的空间-时间声学特征。此外，参考图2中介绍的V2S架构，我们选择了第一和第二卷积层进行CAM可视化（d）和（e）。

（此处省略了图4的描述，因为它涉及图像内容，无法在文本中呈现。）

从分析中，我们观察到这两个层的不同功能。第一卷积层倾向于关注目标信号段本身以及重新编程输入的Mel频谱图（c）中的低频声学特征，而第二卷积层则更关注重新编程输入中的高频成分。

嵌入可视化：我们使用t分布随机邻域嵌入（t-SNE）（Van der Maaten & Hinton，2008）来可视化Strawberry训练集（Holland等人，1998）的logit表示，分别在重新编程前后，以及迁移学习基线（TFa）。如图5所示，重新编程后的tSNE结果显示不同目标类别的嵌入之间有清晰的分离，这表明V2S确实学习了有意义的、具有区分性的数据表示，以重新编程预训练的声学模型进行时间序列分类。另一方面，迁移学习的嵌入可视化显示了低类别间可分性。

（此处省略了图5的描述，因为它涉及图像内容，无法在文本中呈现。）

### 5.6 额外讨论

接下来，我们提供了关于V2S重新编程的额外讨论和见解。

#### 多对一标签映射
多对一映射可以被视为单类输出的集成平均，这是一种实用的技术，可以提高V2S重新编程的分类性能。我们目前的结果表明，在控制实验中，多对一标签映射表现更好。

#### 显著性测试
在第5节中，我们的结果表明，作为一种单一方法，V2S可以在30个数据集的19个上实现或超过竞争性能。我们进一步在两个层面上运行显著性测试，基于表2中的30个数据集的准确率——（i）V2Sa与SOTA数字：p值=0.0017；（ii）V2S与FCN（Wang等人，2017）：p值=0.0011，这表明我们的结果具有显著性。

#### 源数据集的影响
V2S的源数据集影响可以通过定理1中的源风险ϵS以及通过SWD的表示对齐损失来捕捉。我们首先评估不同的声学数据集以计算它们的源测试错误，其中GSCv2获得最小的测试错误。在{GSCv2, TAU, AudioSet, ESC}上的源测试错误分别为{0.1709, 0.1822, 0.1839, 0.1765}，它们的SWD在附录表5中报告。我们的定理告知了V2S在不同源数据集上的性能。如果其他数据集可以比GSCv2有更小的ϵS和SWD，我们期望它有更好的V2S性能。

#### 未来工作
我们的未来工作包括在不同的声学和语音模型（例如，与词汇信息相关的模型）上进行更广泛的性能评估，用于V2S重新编程，模型重新编程用于低资源语音处理，以及扩展到多变量时间序列任务。所提出的理论也将为分析视觉和语言处理领域中对抗性重新编程的成功提供见解。

## 6. 结论

在这项工作中，我们提出了V2S，这是一种新颖的方法，用于重新编程预训练的声学模型进行时间序列分类。我们还开发了一种理论风险分析来表征重新编程性能。在UCR基准上的实验结果表明，V2S的性能优越，通过在30个数据集中的19个上实现新的（或相等的）最先进的准确率。我们还通过表示对齐、声学显著性图和嵌入可视化深入研究了V2S的成功。V2S未来还可以结合不同的先进数据增强技术进行研究。

## 致谢

作者感谢在双盲审稿过程中匿名审稿人的评论和讨论。

## 参考文献

（此处省略了参考文献列表，因为它涉及大量文献，无法在文本中完整呈现。）

## 附录

### A. 引理1的证明

（此处省略了引理1的证明，因为它涉及数学推导，无法在文本中完整呈现。）

### B. 定理1的证明

（此处省略了定理1的证明，因为它涉及数学推导，无法在文本中完整呈现。）

### C. 预训练模型研究

我们提供了关于不同预训练声学架构和相关时间序列分类性能的高级研究。特别是，我们选择了以下模型，它们在Google Speech Commands版本2数据集（Warden，2018）上测试时获得了有竞争力的性能，或者在声学场景（VGGish（Hershey等人，2017））和时间序列分类（TCN（Lea等人，2016））中展示了尖端性能。这些模型将与主文中使用的V2Sa（循环注意力（de Andrade等人，2018））和V2Su（通过U-Net增强的V2Sa（Yang等人，2021a））进行比较。

（此处省略了预训练模型研究的详细内容，因为它涉及大量技术细节，无法在文本中完整呈现。）

### D. 额外的消融研究

基于C节的讨论，我们进一步选择了三种具有不同模型容量（0.2M/1M/4.7M）的高效V2S模型，包括V2Sa、V2Sr和V2St，来研究在不同训练设置下的平均目标准确率，并提供有关有效设计V2S模型的一些见解。

（此处省略了额外消融研究的详细内容，因为它涉及大量技术细节，无法在文本中完整呈现。）

### E. 更多tSNE可视化

我们提供了更多的tSNE可视化，以更好地理解第5.5节中讨论的V2S模型的嵌入结果。在图6（a）到（e）中，重新编程的表示（最右侧）在2D和3D tSNE图中显示出更好的解耦结果。

（此处省略了图6的描述，因为它涉及图像内容，无法在文本中呈现。）

### F. 硬件设置和能源成本讨论

我们使用Nvidia GPU（2080-Ti和V100）进行实验，使用CUDA版本10.1。为了进行主文中表2所示的结果，大约需要40分钟来运行每个时间序列预测数据集的100个周期（最多），批量大小为32，考虑到第5.2节中描述的超参数调整（例如，dropout率）。总的来说，本文中展示的实验（30个数据集及其消融研究）在300W电源下大约需要120个计算小时。作为另一个优势，V2S重新编程技术冻结了预训练的神经模型，并且只用于训练新任务的重新编程层。所提出的方法可能有助于回收训练良好的模型用于额外的任务，以减轻部署负责任的ML系统的额外能源成本。

（此处省略了硬件设置和能源成本讨论的详细内容，因为它涉及技术细节，无法在文本中完整呈现。）

