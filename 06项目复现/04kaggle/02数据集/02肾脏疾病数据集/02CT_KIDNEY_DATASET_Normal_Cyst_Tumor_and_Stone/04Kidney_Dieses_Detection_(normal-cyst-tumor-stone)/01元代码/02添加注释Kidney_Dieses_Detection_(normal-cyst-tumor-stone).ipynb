{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7739a025",
   "metadata": {},
   "source": [
    "# Kidney Dieses Detection (normal-cyst-tumor-stone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0e79d",
   "metadata": {},
   "source": [
    "# Kidney Dieses Classification in 4 classes using 3 different model (VGG, MOBILENET, RESNET)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada48dc9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入Pandas库，这是一个用于数据分析和操作的库，常用于处理表格数据。\n",
    "import pandas as pd\n",
    "\n",
    "# 导入NumPy库，这是一个用于科学计算的库，提供了多维数组对象以及一系列操作这些数组的函数。\n",
    "import numpy as np\n",
    "\n",
    "# 导入matplotlib的pyplot模块，这是一个用于创建静态、交互式和动画可视化的库。\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 导入OpenCV库，这是一个功能强大的计算机视觉和图像处理库。\n",
    "import cv2\n",
    "\n",
    "# 导入TensorFlow库，这是一个开源的机器学习库，广泛用于深度学习模型的构建和训练。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 导入os库，这是一个用于与操作系统交互的库，可以用来操作文件和目录。\n",
    "import os \n",
    "\n",
    "# 从tensorflow库中导入keras子库，Keras是一个高层神经网络API，能够运行在TensorFlow之上。\n",
    "from tensorflow import keras\n",
    "\n",
    "# 从tensorflow.keras库中导入层（Layer）的概念，这些层可以堆叠起来构建深度学习模型。\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 从tensorflow.keras.layers库中导入全连接层（Dense），展平层（Flatten），这些都是构建神经网络常用的层类型。\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# 从tensorflow.keras.models库中导入Sequential模型，这是一种线性堆叠的层结构，用于快速构建模型。\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 从tensorflow.keras.optimizers库中导入Adam优化器，这是一种常用的优化算法，用于模型训练中的参数更新。\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 再次从tensorflow.keras.layers库中导入几个层类型，包括Flatten层，Dense层，Dropout层和批量归一化层（BatchNormalization）。\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# 从tensorflow.keras.models库中导入Model类和Sequential类，Model类是用于构建任意结构的模型，而Sequential是其一个特殊子类，用于构建线性堆叠的模型。\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# 从tensorflow.keras.layers库中导入二维卷积层（Conv2D）和二维最大池化层（MaxPooling2D），这些是构建卷积神经网络（CNN）的基本层。\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# 从tensorflow.keras.layers库中导入全局平均池化层（GlobalAveragePooling2D），这是一种池化层，用于将卷积层的输出平均聚合为单个数值。\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "```\n",
    "\n",
    "这些注释提供了对代码中每个导入语句的详细解释，说明了每个库或模块的用途和它们在机器学习、数据处理和图像处理中的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24478e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7274000",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 设置batch_size的值为32。在机器学习中，batch_size是每次训练迭代中用于更新模型权重的样本数量。较小的batch_size可以提供更多的随机性，而较大的batch_size可以加快训练速度，但可能需要更多的内存。\n",
    "batch_size = 32\n",
    "\n",
    "# 设置img_height的值为224。这通常用于指定图像处理或卷积神经网络中图像的高度。在许多计算机视觉任务中，图像会被调整到这个尺寸以适应模型的输入要求。\n",
    "img_height = 224\n",
    "\n",
    "# 设置img_width的值为224。这通常用于指定图像处理或卷积神经网络中图像的宽度。与img_height一样，这个尺寸确保了图像可以被模型正确处理。\n",
    "img_width = 224\n",
    "```\n",
    "\n",
    "这些注释提供了对代码中每个变量赋值语句的详细解释，说明了这些变量在机器学习和图像处理中的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone'\n",
    "\n",
    "train=tf.keras.utils.image_dataset_from_directory(data_dir,image_size=(img_height,img_width),\n",
    "                                                validation_split=0.1,\n",
    "                                                subset='training',seed=123)\n",
    "val=tf.keras.utils.image_dataset_from_directory(data_dir,image_size=(img_height,img_width),\n",
    "                                                validation_split=0.2,\n",
    "                                                subset='validation',seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4136f",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 定义一个字符串变量data_dir，它指定了包含图像数据的目录路径。\n",
    "# 这个路径指向Kaggle输入数据集中的特定文件夹，该文件夹包含了肾脏CT扫描图像数据集，用于区分正常的肾脏、囊肿、肿瘤和结石。\n",
    "data_dir='/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone'\n",
    "\n",
    "# 使用tf.keras.utils.image_dataset_from_directory函数从指定的目录创建一个图像数据集。\n",
    "# 该函数将根据目录结构自动为图像分配标签，子目录名称将作为标签。\n",
    "# image_size参数指定了图像将被调整到的尺寸，即高度和宽度都为224像素。\n",
    "# validation_split参数指定了从训练数据中分割出来作为验证集的比例，这里设置为10%。\n",
    "# subset参数设置为'training'，表示这是训练数据集。\n",
    "# seed参数设置为123，用于确保数据分割的随机性是可重复的。\n",
    "train=tf.keras.utils.image_dataset_from_directory(data_dir,image_size=(img_height,img_width),\n",
    "                                                validation_split=0.1,\n",
    "                                                subset='training',seed=123)\n",
    "\n",
    "# 再次使用tf.keras.utils.image_dataset_from_directory函数从相同的目录创建另一个图像数据集。\n",
    "# 与创建训练数据集类似，但这次validation_split参数设置为0.2，意味着20%的数据将用作验证集。\n",
    "# subset参数设置为'validation'，表示这是验证数据集。\n",
    "# 同样，seed参数设置为123，以确保数据分割的一致性。\n",
    "val=tf.keras.utils.image_dataset_from_directory(data_dir,image_size=(img_height,img_width),\n",
    "                                                validation_split=0.2,\n",
    "                                                subset='validation',seed=123)\n",
    "```\n",
    "\n",
    "这些注释提供了对代码中每个函数调用的详细解释，说明了如何从特定目录加载图像数据集，并如何分割训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7707a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_class_name = dict(zip(range(len(train.class_names)), train.class_names))\n",
    "label_to_class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f089f",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 创建一个名为label_to_class_name的字典，该字典将用于将标签索引映射到对应的类别名称。\n",
    "# 使用zip函数将train对象的类名列表（train.class_names）与一个从0开始的整数序列进行配对。这里的len(train.class_names)将生成一个与类名列表长度相同的整数序列。\n",
    "# 然后，使用dict函数将zip函数返回的配对转换成一个字典。\n",
    "label_to_class_name = dict(zip(range(len(train.class_names)), train.class_names))\n",
    "\n",
    "# 打印label_to_class_name字典，这将展示映射关系，其中键是标签索引，值是对应的类别名称。\n",
    "# 这个映射对于理解数据集中每个标签代表什么类别非常有用，尤其是在可视化或解释模型预测时。\n",
    "label_to_class_name\n",
    "```\n",
    "\n",
    "这段代码通过创建一个映射字典，为后续处理提供了方便，使得可以从数字标签快速转换到人类可读的类别名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c14d4d",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 开始一个for循环，该循环遍历由tf.keras.utils.image_dataset_from_directory创建的train数据集对象。\n",
    "# 在每次迭代中，循环提供两样东西：当前批次的图像（image_batch）和对应的标签（labels_batch）。\n",
    "for image_batch, labels_batch in train:\n",
    "\n",
    "  # 打印当前图像批次的形状。图像批次是一个四维数组，其形状通常是（批次大小，图像高度，图像宽度，颜色通道数）。\n",
    "  # 例如，如果batch_size是32，而图像尺寸是224x224，且图像是三通道的RGB图像，那么形状将是（32, 224, 224, 3）。\n",
    "  print(image_batch.shape)\n",
    "  \n",
    "  # 打印当前标签批次的形状。标签批次是一个一维数组，其形状通常是（批次大小，）。\n",
    "  # 这是因为每个图像都有一个对应的标签，所以标签的数量与图像批次中的图像数量相同。\n",
    "  print(labels_batch.shape)\n",
    "  \n",
    "  # 使用break语句提前退出for循环。这意味着循环只会执行一次，即只处理训练数据集中的第一个批次。\n",
    "  # 这通常用于调试目的，以快速检查数据集的结构而无需加载整个数据集。\n",
    "  break\n",
    "```\n",
    "\n",
    "这段代码通过迭代训练数据集的第一个批次，展示了如何访问和检查图像及其对应标签的形状。这对于理解数据的维度和结构非常有用，尤其是在开始模型训练之前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = train.as_numpy_iterator()\n",
    "batch = data_iterator.next()\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "for i in range(4):  \n",
    "    for j in range(4):  \n",
    "        index = i * 4 + j  \n",
    "        ax[i, j].imshow(batch[0][index].astype(int))\n",
    "        ax[i, j].set_title(label_to_class_name[batch[1][index]])\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b536e",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 将train数据集转换为一个Numpy迭代器。这个迭代器可以让我们按批次访问数据集中的图像和标签。\n",
    "data_iterator = train.as_numpy_iterator()\n",
    "\n",
    "# 使用迭代器的next()方法获取第一个批次的数据。这个批次包含图像和对应的标签。\n",
    "batch = data_iterator.next()\n",
    "\n",
    "# 使用matplotlib的subplots函数创建一个图形和子图的网格。nrows=4和ncols=4表示网格有4行4列，共16个子图。\n",
    "# figsize=(10, 10)设置整个图形的大小为10x10英寸。\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "# 外层循环遍历4行。\n",
    "for i in range(4):\n",
    "    # 内层循环遍历4列。\n",
    "    for j in range(4):\n",
    "        # 计算当前子图的索引（从0开始）。\n",
    "        index = i * 4 + j\n",
    "        \n",
    "        # 在当前子图上显示当前批次中的图像。astype(int)将图像数据类型转换为整数，这通常用于图像显示。\n",
    "        ax[i, j].imshow(batch[0][index].astype(int))\n",
    "        \n",
    "        # 设置当前子图的标题为对应的类别名称。batch[1][index]是当前图像的标签索引，label_to_class_name字典用于将标签索引转换为类别名称。\n",
    "        ax[i, j].set_title(label_to_class_name[batch[1][index]])\n",
    "\n",
    "# 使用subplots_adjust函数调整子图之间的空白。wspace和hspace分别设置子图水平和垂直方向的空白。\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# 显示整个图形。这将弹出一个窗口，其中包含16个子图，每个子图显示一个图像及其类别名称。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过创建一个4x4的图像网格，展示了如何使用matplotlib来可视化数据集中的一个批次的图像，并将每个图像的类别名称作为标题显示在相应的子图上。这对于快速浏览数据集并了解其内容非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.map(lambda x,y:(x/255,y))\n",
    "val=val.map(lambda x,y:(x/255,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84751686",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用map函数对训练数据集train进行操作。map函数接受一个函数，该函数会被应用到数据集中的每个批次。\n",
    "# lambda是一个匿名函数，这里它接受两个参数x和y，分别代表图像数据和对应的标签。\n",
    "# x/255是对图像数据进行归一化的操作，将图像像素值从0-255的范围缩放到0-1之间，这通常有助于模型训练的稳定性和收敛速度。\n",
    "# 最终，归一化后的图像数据x和原始标签y被作为元组返回。\n",
    "train = train.map(lambda x, y: (x / 255, y))\n",
    "\n",
    "# 与训练数据集类似，使用map函数对验证数据集val也进行相同的操作。\n",
    "# 同样的lambda函数被用于将图像数据归一化，并将归一化后的图像与原始标签一起返回。\n",
    "val = val.map(lambda x, y: (x / 255, y))\n",
    "```\n",
    "\n",
    "通过这两行代码，训练和验证数据集中的每个图像都被归一化，以提高模型性能。归一化是一种常见的数据预处理步骤，特别是在使用基于梯度的优化算法进行深度学习训练时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd512d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cafe74",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从tensorflow.data模块导入AUTOTUNE常量。AUTOTUNE允许tf.data API动态决定一些参数的最佳值，比如buffer_size，以优化数据加载性能。\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# 对训练数据集train应用cache方法。cache方法将数据集的元素缓存到内存中，这样在每个epoch的迭代中，数据就不需要从磁盘重新读取，从而加快数据读取速度。\n",
    "# 然后调用prefetch方法，它允许数据集在模型训练的同时预取下一批数据。这可以减少CPU和GPU之间的空闲时间，因为当模型在训练一个批次时，下一个批次的数据已经在准备中了。\n",
    "# prefetch方法的buffer_size参数设置为AUTOTUNE，这样tf.data API会动态决定最佳的缓冲区大小。\n",
    "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 对验证数据集val也进行相同的操作，使用cache和prefetch方法来优化数据读取和预取，提高验证过程的效率。\n",
    "# 同样，buffer_size参数使用AUTOTUNE，让API自动调整。\n",
    "val = val.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "```\n",
    "\n",
    "这段代码通过使用`cache`和`prefetch`方法，优化了数据集的加载过程，使得数据集在训练和验证过程中可以更快地被访问。这对于大型数据集或需要从较慢的存储介质（如硬盘）读取数据的情况特别有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59c726",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model = Sequential()\n",
    "\n",
    "pretrained_model= tf.keras.applications.VGG16(include_top=False, input_shape=(150,150,3), pooling='max',classes=4, weights='imagenet')\n",
    "\n",
    "\n",
    "VGG_model.add(pretrained_model)\n",
    "VGG_model.add(Flatten())\n",
    "VGG_model.add(Dense(512, activation='relu'))\n",
    "VGG_model.add(BatchNormalization())  # Batch Normalization layer\n",
    "VGG_model.add(Dropout(0.5))\n",
    "\n",
    "VGG_model.add(Dense(4, activation='softmax'))\n",
    "pretrained_model.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c8def",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 首先，创建一个Sequential模型的实例，这是一个线性堆叠的神经网络模型。\n",
    "VGG_model = Sequential()\n",
    "\n",
    "# 使用tf.keras.applications模块中的VGG16函数创建一个预训练的VGG16模型。\n",
    "# include_top=False表示不包含网络顶部的全连接层，因为我们将添加自己的全连接层。\n",
    "# input_shape=(150,150,3)指定了输入图像的尺寸和颜色通道数，即150x150像素的彩色图像。\n",
    "# pooling='max'指示在最后一个卷积层后面接一个最大池化层，而不是全连接层。\n",
    "# classes=4表示模型的输出将有4个神经元，用于4个类别的分类。\n",
    "# weights='imagenet'表示使用在ImageNet数据集上预训练的权重来初始化模型。\n",
    "pretrained_model = tf.keras.applications.VGG16(include_top=False, input_shape=(150,150,3), pooling='max', classes=4, weights='imagenet')\n",
    "\n",
    "# 将预训练的VGG16模型作为第一个层添加到Sequential模型中。\n",
    "VGG_model.add(pretrained_model)\n",
    "\n",
    "# 添加一个Flatten层，该层将VGG16模型输出的二维特征图展平成一维数组，以便于输入到全连接层。\n",
    "VGG_model.add(Flatten())\n",
    "\n",
    "# 添加一个具有512个神经元的全连接层，并使用ReLU激活函数。\n",
    "VGG_model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# 添加批量归一化层（BatchNormalization），这有助于提高训练的稳定性和速度。\n",
    "VGG_model.add(BatchNormalization())\n",
    "\n",
    "# 添加一个Dropout层，丢弃率（rate）为0.5，这意味着在训练过程中有50%的神经元输出将被随机置零，以减少过拟合。\n",
    "VGG_model.add(Dropout(0.5))\n",
    "\n",
    "# 添加最后一个全连接层，具有4个神经元，并使用softmax激活函数，以进行多分类任务。\n",
    "VGG_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# 将预训练的VGG16模型设置为不可训练（冻结），这样在模型训练过程中，VGG16模型的权重将不会更新。\n",
    "pretrained_model.trainable = False\n",
    "```\n",
    "\n",
    "这段代码演示了如何使用预训练的VGG16模型作为特征提取器，并且在此基础上添加了全连接层来完成特定任务的分类。通过冻结预训练模型的权重，可以避免在训练过程中对这些权重进行更新，从而节省训练时间并利用预训练模型的强大特征提取能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78878eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf7229",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用VGG_model对象的compile方法来配置模型的训练参数。\n",
    "# compile方法是准备模型进行训练的第一步，它设置了模型的优化器、损失函数和评估指标。\n",
    "VGG_model.compile(\n",
    "    # 指定优化器为'adam'，这是一种广泛使用的随机梯度下降优化算法，以其良好的性能和快速收敛而闻名。\n",
    "    optimizer='adam',\n",
    "    # 指定损失函数为'sparse_categorical_crossentropy'，这是一种适用于多分类问题并且标签是整数的情况的损失函数。\n",
    "    # 它计算的是实际输出概率分布和目标概率分布之间的交叉熵。\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # 指定在训练和验证过程中要计算的评估指标，这里选择了['accuracy']，即准确率。\n",
    "    # 准确率是正确分类的样本数占总样本数的比例，是衡量分类模型性能的一个常用指标。\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "这段代码通过编译模型，为接下来的模型训练做好了准备。选择的优化器、损失函数和评估指标对于模型能否有效学习以及如何评估学习效果至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81789db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c96d9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 调用VGG_model对象的summary方法。\n",
    "# summary方法会打印出模型的详细结构，包括模型中每一层的名称、类型、输出形状、参数数量等信息。\n",
    "# 这个方法对于可视化和理解模型的架构非常有帮助，特别是在模型较为复杂时。\n",
    "VGG_model.summary()\n",
    "```\n",
    "\n",
    "通过执行这一行代码，你将能够获得一个关于VGG模型每一层的详细报告，包括每层的参数数量，这有助于你了解整个模型的复杂度和潜在的过拟合风险。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aff86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = VGG_model.fit(\n",
    "  train,\n",
    "  validation_data=val,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cbce1",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 定义一个变量epochs，设置其值为10。这个变量代表训练模型时将要进行的迭代次数，也就是训练周期(epoch)的数量。\n",
    "epochs = 10\n",
    "\n",
    "# 使用VGG_model对象的fit方法开始训练模型。\n",
    "# fit方法是用于训练模型的函数，它将数据输入模型并根据提供的优化器和损失函数调整模型的权重。\n",
    "history = VGG_model.fit(\n",
    "    # 将train数据集作为训练数据输入模型。\n",
    "    train,\n",
    "    # validation_data参数指定了验证集，这里使用的是之前定义的val数据集。在每个epoch结束后，模型将在验证集上进行评估。\n",
    "    validation_data=val,\n",
    "    # epochs参数设置为之前定义的epochs变量，即训练将进行10个epoch。\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# 训练完成后，history变量将包含训练过程中的所有记录，包括每个epoch的训练和验证损失(loss)以及准确率(accuracy)。这些信息可以用来分析模型的学习过程和性能。\n",
    "```\n",
    "\n",
    "这段代码启动了模型的训练过程，并且每经过一个epoch，都会在训练集和验证集上评估模型的性能。训练结束后，`history`对象包含了整个训练过程中的重要信息，这些信息对于后续分析模型的学习动态和调试模型非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a330c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('VGG_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03583cd2",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用pandas库中的DataFrame函数将history对象转换成一个Pandas DataFrame对象。history对象是模型的fit方法返回的，它包含了模型训练和验证过程中的损失和准确率等信息。\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# 使用DataFrame对象的to_csv方法将数据保存到CSV文件中。'VGG_training.csv'是指定的文件名，这个文件将包含模型训练的历史记录。\n",
    "# index=False参数表示在保存时不包括行索引，因为行索引通常是自动生成的，不一定需要保存。\n",
    "history_df.to_csv('VGG_training.csv', index=False)\n",
    "```\n",
    "\n",
    "这段代码的作用是将模型训练过程中收集的历史数据转换成一个结构化的表格，然后将其保存到CSV文件中。CSV文件是一种常见的数据存储格式，可以方便地被多种数据分析工具和软件读取和处理。这样做有助于后续对模型训练过程的回顾和分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model.save('VGG_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefc3e0",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用VGG_model对象的save方法将整个模型保存到指定的文件中。\n",
    "# 'VGG_model.h5'是文件的保存路径及名称。.h5是HDF5格式的文件扩展名，这是一种用于存储和组织大量数据的文件格式，常用于保存深度学习模型。\n",
    "# 保存模型后，可以重新加载该模型，包括其架构、权重和优化器状态等，以便进行后续的推理或继续训练。\n",
    "VGG_model.save('VGG_model.h5')\n",
    "```\n",
    "\n",
    "这段代码的作用是将训练好的VGG模型保存到本地磁盘上的一个文件中。使用HDF5格式保存模型是一种常见的做法，因为这种格式能够高效地存储模型的复杂结构和参数。保存后的模型文件可以用于模型的部署、共享或将来的进一步训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7791f7c",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从历史记录history.history中提取训练损失，存储在变量loss中。\n",
    "# history.history是一个字典，包含了训练过程中的各种指标，'loss'键对应的值是训练损失的列表。\n",
    "loss = history.history['loss']\n",
    "\n",
    "# 从历史记录history.history中提取验证损失，存储在变量val_loss中。\n",
    "# 'val_loss'键对应的值是验证损失的列表，它是在每个epoch结束后在验证集上计算得到的。\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# 使用range函数生成一个与损失列表长度相同的整数序列，作为横坐标的epochs。\n",
    "# 因为range的上限是排他性的，所以使用len(loss)+1来确保包含所有epoch。\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制训练损失和验证损失曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数为'Training loss'设置训练损失曲线的图例。\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "\n",
    "# 绘制验证损失曲线，线条颜色为红色，图例为'Validation loss'。\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Loss'。\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Loss'。\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 调用legend函数显示图例。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数展示整个图表。\n",
    "# 这将弹出一个窗口，显示了训练损失和验证损失随epoch变化的曲线图，用于分析模型训练过程中损失的变化趋势。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练损失和验证损失随训练周期变化的曲线图，提供了一种直观的方式来观察模型在训练过程中的性能变化。这对于理解模型的学习能力、诊断过拟合或欠拟合问题以及决定何时停止训练都是非常有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44817ccb",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从历史记录history.history中提取训练准确率，存储在变量acc中。\n",
    "# 'accuracy'键对应的值是训练准确率的列表，它记录了每个epoch后模型在训练集上的准确率。\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "# 从历史记录history.history中提取验证准确率，存储在变量val_acc中。\n",
    "# 'val_accuracy'键对应的值是验证准确率的列表，记录了每个epoch后模型在验证集上的准确率。\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制训练准确率和验证准确率曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数为'Training acc'设置训练准确率曲线的图例。\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "\n",
    "# 绘制验证准确率曲线，线条颜色为红色，图例为'Validation acc'。\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Accuracy'。\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Accuracy'。\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# 调用legend函数显示图例。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数展示整个图表。\n",
    "# 这将弹出一个窗口，显示了训练准确率和验证准确率随epoch变化的曲线图，用于分析模型训练过程中准确率的变化趋势和模型的泛化能力。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练准确率和验证准确率随训练周期变化的曲线图，提供了一种直观的方式来观察模型在训练过程中的准确率变化。这对于评估模型的性能、理解模型在训练集和验证集上的表现差异是非常有用的。图例、标题、坐标轴标签以及图例都增强了图表的可读性和信息的清晰度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f180835",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入NumPy库，这是一个适用于科学计算的库，提供了多维数组对象以及一系列操作这些数组的函数。\n",
    "import numpy as np\n",
    "\n",
    "# 导入matplotlib的pyplot模块，这是一个用于创建静态、交互式和动画可视化的库。\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从sklearn.metrics模块导入混淆矩阵(confusion_matrix)和混淆矩阵展示工具(ConfusionMatrixDisplay)。\n",
    "# 混淆矩阵是一个重要的工具，用于可视化分类模型的性能，特别是在多分类问题中。\n",
    "# ConfusionMatrixDisplay是用于显示混淆矩阵的类，它提供了一个图形化界面来更好地理解模型预测的结果。\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 导入TensorFlow库，这是一个开源的机器学习库，广泛用于深度学习模型的构建、训练和部署。\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "这段代码导入了进行机器学习和深度学习任务时常用的几个库。NumPy用于数值计算，matplotlib用于数据可视化，sklearn用于计算混淆矩阵并展示，TensorFlow用于构建和训练深度学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb184393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "predictions = VGG_model.predict(val)\n",
    "\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "with open('confusion_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(cm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484501bd",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个Python中用于序列化和反序列化的对象结构，可以用于存储和检索数据结构。\n",
    "import pickle\n",
    "\n",
    "# 从sklearn.metrics模块导入混淆矩阵(confusion_matrix)函数，该函数用于生成混淆矩阵，是评估分类模型性能的一个重要工具。\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 导入TensorFlow库，这是一个用于机器学习和深度神经网络的开源库，广泛用于构建和训练深度学习模型。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 使用训练好的VGG_model模型对验证集val进行预测，得到预测结果。\n",
    "predictions = VGG_model.predict(val)\n",
    "\n",
    "# 从验证集val中提取真实的标签。val是一个tf.data.Dataset对象，它由多组数据(x, y)组成。\n",
    "# 使用列表推导式和np.concatenate合并所有真实的标签到一个NumPy数组中，以便与预测结果进行比较。\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "# 使用混淆矩阵函数计算模型预测结果和真实标签之间的混淆矩阵。np.argmax函数用于从预测结果中选择概率最高的类别作为预测类别。\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "# 使用with语句打开文件'confusion_matrix.pkl'，以二进制写入模式('wb')。\n",
    "# with语句可以确保文件正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('confusion_matrix.pkl', 'wb') as f:\n",
    "    # 使用pickle.dump函数将混淆矩阵cm序列化并保存到文件中。这允许以后可以重新加载混淆矩阵进行进一步分析。\n",
    "    pickle.dump(cm, f)\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的模型对验证集进行预测，计算混淆矩阵，并使用pickle将混淆矩阵序列化并保存到文件中。混淆矩阵是一个重要的工具，可以帮助我们理解模型在各个类别上的表现，特别是在多分类问题中。通过序列化混淆矩阵，我们可以方便地将其存储和传输，以便后续分析或报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "with open('confusion_matrix.pkl', 'rb') as f:\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']  \n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a5edd",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个用于序列化和反序列化Python对象的库，可以用于从文件中读取和写入数据结构。\n",
    "import pickle\n",
    "\n",
    "# 导入NumPy库，这是一个适用于科学计算的库，提供了多维数组对象以及一系列操作这些数组的函数。\n",
    "import numpy as np\n",
    "\n",
    "# 导入matplotlib的pyplot模块，这是一个用于创建静态、交互式和动画可视化的库。\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从sklearn.metrics模块导入ConfusionMatrixDisplay类，这是一个用于图形化展示混淆矩阵的工具。\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# 使用with语句打开文件'confusion_matrix.pkl'，以二进制读取模式('rb')。\n",
    "# with语句可以确保文件在使用后正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('confusion_matrix.pkl', 'rb') as f:\n",
    "    # 使用pickle.load函数从文件中读取之前保存的混淆矩阵cm。\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "# 定义一个列表class_names，包含模型分类任务中所有类别的名称。这将用于在混淆矩阵的轴上显示类别名称。\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "\n",
    "# 创建一个ConfusionMatrixDisplay对象disp，传入混淆矩阵cm和类别名称class_names。\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# 调用disp对象的plot方法来绘制混淆矩阵。\n",
    "# cmap参数设置为'viridis'，这指定了颜色映射，用于根据不同的值显示不同的颜色。\n",
    "# xticks_rotation参数设置为'vertical'，这指定了x轴上的刻度标签的旋转方式，'vertical'表示垂直旋转，使得标签更易读。\n",
    "# values_format参数设置为'd'，这指定了混淆矩阵中的数值显示格式，'d'表示以整数形式显示。\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "# 调用plt.show()函数来展示混淆矩阵图形。这将弹出一个窗口，显示了混淆矩阵的图形化表示。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过加载之前保存的混淆矩阵，并使用`ConfusionMatrixDisplay`和`matplotlib`来生成并展示一个图形化的混淆矩阵。混淆矩阵是一个重要的工具，用于可视化分类模型的性能，尤其是在多分类问题中。通过图形化的展示，我们可以更直观地看到模型在各个类别上的表现，以及可能存在的混淆情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "resize = tf.image.resize(img, (150,150))\n",
    "yhat = VGG_model.predict(np.expand_dims(resize/255, 0))\n",
    "max_index = np.argmax(yhat)\n",
    "label_to_class_name[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcea9b9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用OpenCV库的imread函数读取指定路径下的图像文件。\n",
    "# 这里读取的是一个名为'Cyst- (1).jpg'的图像，它位于Kaggle输入数据集的'Cyst'子目录中。\n",
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "\n",
    "# 使用matplotlib的pyplot模块展示读取到的图像。\n",
    "# imshow函数将图像显示在屏幕上。\n",
    "plt.imshow(img)\n",
    "\n",
    "# 调用show函数来展示图像。\n",
    "# 这将弹出一个窗口，显示了加载的图像。\n",
    "plt.show()\n",
    "\n",
    "# 使用TensorFlow的image.resize函数将图像大小调整为150x150像素。\n",
    "# 这个尺寸与之前模型训练时使用的输入尺寸一致。\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "\n",
    "# 将调整大小后的图像通过VGG_model进行预测。\n",
    "# 由于模型预期的输入是一个批次的图像，这里使用np.expand_dims将图像数组扩展到一个新的维度，形成一个新的轴0，从而满足模型输入的批次要求。\n",
    "# 同时，图像数据被归一化到0-1范围内，这是因为模型是在归一化后的数据上训练的。\n",
    "yhat = VGG_model.predict(np.expand_dims(resize / 255, 0))\n",
    "\n",
    "# 使用NumPy的argmax函数找出预测概率最高的类别索引。\n",
    "# 这将给出一个整数，表示在模型预测中哪个类别的概率最高。\n",
    "max_index = np.argmax(yhat)\n",
    "\n",
    "# 使用之前定义的label_to_class_name字典，通过预测得到的类别索引获取对应的类别名称。\n",
    "# 这将给出一个字符串，表示预测结果中概率最高的类别的名称。\n",
    "label_to_class_name[max_index]\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的VGG模型对单个图像进行预测，并输出预测的类别名称。首先，它读取一个图像文件，然后调整图像大小以匹配模型的输入要求，接着归一化图像数据并进行预测。最后，它通过索引获取预测结果中概率最高的类别名称。这个过程是深度学习模型用于图像分类任务中的典型应用流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dc676",
   "metadata": {},
   "source": [
    "## MOBILENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.regularizers import l2\n",
    "input_shape = (224, 224, 3)\n",
    "MobileNetV2_model = MobileNetV2(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "\n",
    "for _ in range(8):\n",
    "    MobileNetV2_model.layers.pop()\n",
    "for layer in MobileNetV2_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "x = GlobalAveragePooling2D()(MobileNetV2_model.output)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "mobile_net = Model(inputs=MobileNetV2_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ecd45",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从tensorflow.keras.applications模块导入MobileNetV2类，这是一个预训练的深度学习模型，适用于图像分类任务。\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# 从tensorflow.keras.regularizers模块导入l2正则化器，这是一种用于防止神经网络过拟合的技术。\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# 定义输入图像的形状，这里设置为224x224像素的彩色图像，即3个颜色通道。\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# 创建MobileNetV2模型的实例，使用ImageNet预训练权重，但不包括网络顶部的全连接层（因为我们要添加自己的全连接层）。\n",
    "MobileNetV2_model = MobileNetV2(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "\n",
    "# 通过循环移除MobileNetV2模型顶部的8个层，通常是一些全连接层或者上采样层，因为我们将用自己的层替换它们。\n",
    "for _ in range(8):\n",
    "    MobileNetV2_model.layers.pop()\n",
    "\n",
    "# 将MobileNetV2模型中所有层设置为可训练。通常，预训练模型的某些层在开始时是冻结的（不可训练），但在这里我们希望它们能够进行训练。\n",
    "for layer in MobileNetV2_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 使用GlobalAveragePooling2D层将MobileNetV2模型的输出进行全局平均池化，这将转换为一个固定大小的向量。\n",
    "x = GlobalAveragePooling2D()(MobileNetV2_model.output)\n",
    "\n",
    "# 添加一个具有512个神经元的全连接层，使用ReLU激活函数，并应用L2正则化器，正则化因子为1e-4。\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "# 再次添加一个具有256个神经元的全连接层，同样使用ReLU激活函数和L2正则化器。\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "# 添加输出层，这是一个具有4个神经元的全连接层，使用softmax激活函数，用于多分类问题。\n",
    "output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# 使用Model类创建一个新的模型，将MobileNetV2模型的输入和新构建的全连接层的输出作为输入和输出。\n",
    "mobile_net = Model(inputs=MobileNetV2_model.input, outputs=output)\n",
    "```\n",
    "\n",
    "这段代码演示了如何使用预训练的MobileNetV2模型作为特征提取器，并通过移除其顶部的层，然后添加自定义的全连接层来构建一个新的分类模型。通过这种方式，我们可以使用预训练模型的强大特征提取能力，并通过微调（fine-tuning）来适应特定的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0339d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369127aa",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 对mobile_net模型调用compile方法来配置模型的训练参数。\n",
    "# compile方法是准备模型进行训练的第一步，它设置了模型的优化器、损失函数和评估指标。\n",
    "mobile_net.compile(\n",
    "    # optimizer参数指定优化器为'adam'，这是一种自适应学习率的优化算法，常用于深度学习任务中。\n",
    "    optimizer='adam',\n",
    "    # loss参数指定损失函数为'sparse_categorical_crossentropy'，适用于多分类问题，其中标签是整数形式。\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # metrics参数指定在训练过程中要监控的指标是准确率（'accuracy'），这是衡量分类模型性能的一个常用指标。\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "这段代码通过编译模型，为接下来的模型训练做好了准备。选择的优化器、损失函数和评估指标对于模型能否有效学习以及如何评估学习效果至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobile_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6b943",
   "metadata": {},
   "source": [
    "代码只有一行注释，我将对其进行解释并提供相应的中文注释：\n",
    "\n",
    "```python\n",
    "# 这一行是Python中的单行注释，使用了#符号。它说明了接下来的代码行是不可执行的注释内容。\n",
    "# mobile_net.summary() 是一个被注释掉的代码行，它调用了mobile_net模型的summary方法。\n",
    "# 如果取消注释（即删除#符号），该行代码将会执行，打印出mobile_net模型的详细结构，包括每一层的名称、类型、输出形状、参数数量等信息。\n",
    "# 这个方法对于可视化和理解模型的架构非常有帮助，尤其是在模型较为复杂时。\n",
    "# mobile_net.summary()\n",
    "```\n",
    "\n",
    "在实际使用中，取消注释（即删除`#`符号）后，`mobile_net.summary()` 将非常有用，尤其是当你需要查看模型的每一层以及整体架构时。在模型构建阶段和训练之前，这是一个非常有用的实践，有助于确保模型结构符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "history = mobile_net.fit(\n",
    "  train,\n",
    "  validation_data=val,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5440f",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 定义变量epochs并设置其值为15，这个变量代表训练模型时将要进行的迭代次数，也就是训练周期(epoch)的数量。\n",
    "epochs = 15\n",
    "\n",
    "# 使用mobile_net模型的fit方法开始训练模型。\n",
    "# fit方法是用于训练模型的函数，它将数据输入模型并根据提供的优化器和损失函数调整模型的权重。\n",
    "history = mobile_net.fit(\n",
    "    # 将train数据集作为训练数据输入模型。\n",
    "    train,\n",
    "    # validation_data参数指定了验证集，这里使用的是之前定义的val数据集。\n",
    "    # 在每个epoch结束后，模型将在验证集上进行评估，以监控模型在未见数据上的表现。\n",
    "    validation_data=val,\n",
    "    # epochs参数设置为之前定义的epochs变量，即训练将进行15个epoch。\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# 训练完成后，history变量将包含训练过程中的所有记录，包括每个epoch的训练和验证损失(loss)以及准确率(accuracy)。\n",
    "# 这些信息可以用来分析模型的学习过程和性能。\n",
    "```\n",
    "\n",
    "这段代码启动了模型的训练过程，并且每经过一个epoch，都会在训练集和验证集上评估模型的性能。训练结束后，`history`对象包含了整个训练过程中的重要信息，这些信息对于后续分析模型的学习动态和调试模型非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165009cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('MobileNet_training.csv', index=False)\n",
    "mobile_net.save('MobileNet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8cbd0",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用pandas库中的DataFrame函数将history对象转换成一个Pandas DataFrame对象。\n",
    "# history对象是模型的fit方法返回的，它包含了模型训练和验证过程中的损失和准确率等信息。\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# 使用DataFrame对象的to_csv方法将数据保存到CSV文件中。\n",
    "# 'MobileNet_training.csv'是指定的文件名，这个文件将包含模型训练的历史记录。\n",
    "# index=False参数表示在保存时不包括行索引，因为行索引通常是自动生成的，不一定需要保存。\n",
    "history_df.to_csv('MobileNet_training.csv', index=False)\n",
    "\n",
    "# 使用mobile_net模型的save方法将整个模型保存到指定的文件中。\n",
    "# 'MobileNet_model.h5'是文件的保存路径及名称。.h5是HDF5格式的文件扩展名，这是一种用于存储和组织大量数据的文件格式，常用于保存深度学习模型。\n",
    "# 保存模型后，可以重新加载该模型，包括其架构、权重和优化器状态等，以便进行后续的推理或继续训练。\n",
    "mobile_net.save('MobileNet_model.h5')\n",
    "```\n",
    "\n",
    "这段代码首先将模型训练过程中收集的历史数据转换成一个结构化的表格，并将其保存到CSV文件中。然后，它将整个模型保存到一个HDF5文件中。这样做有助于后续对模型训练过程的回顾和分析，同时也方便了模型的共享和部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87521c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8ae84",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从历史记录history.history中提取训练损失，存储在变量loss中。\n",
    "# history.history是一个字典，包含了训练过程中的各种指标，'loss'键对应的值是训练损失的列表。\n",
    "loss = history.history['loss']\n",
    "\n",
    "# 从历史记录history.history中提取验证损失，存储在变量val_loss中。\n",
    "# 'val_loss'键对应的值是验证损失的列表，它是在每个epoch结束后在验证集上计算得到的。\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# 使用range函数生成一个与损失列表长度相同的整数序列，作为横坐标的epochs。\n",
    "# 因为range的上限是排他性的，所以使用len(loss)+1来确保包含所有epoch。\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制训练损失和验证损失曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数为'Training loss'设置训练损失曲线的图例。\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "\n",
    "# 绘制验证损失曲线，线条颜色为红色，图例为'Validation loss'。\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Loss'。\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Loss'。\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 调用legend函数显示图例。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数展示整个图表。\n",
    "# 这将弹出一个窗口，显示了训练损失和验证损失随epoch变化的曲线图，用于分析模型训练过程中损失的变化趋势。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练损失和验证损失随训练周期变化的曲线图，提供了一种直观的方式来观察模型在训练过程中的性能变化。这对于理解模型的学习能力、诊断过拟合或欠拟合问题以及决定何时停止训练都是非常有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9177464",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a3798",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从训练历史记录history.history中提取训练集的准确率数据，并将其存储在变量acc中。\n",
    "# 'accuracy'键对应的值是模型在训练集上每个epoch后的准确率。\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "# 从训练历史记录history.history中提取验证集的准确率数据，并将其存储在变量val_acc中。\n",
    "# 'val_accuracy'键对应的值是模型在验证集上每个epoch后的准确率。\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制模型在训练集上的准确率变化曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数设置为'Training acc'，为训练准确率曲线设置图例。\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制模型在验证集上的准确率变化曲线。\n",
    "# 'r'表示线条颜色为红色，'label'参数设置为'Validation acc'，为验证准确率曲线设置图例。\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Accuracy'，表示正在展示的是训练集和验证集的准确率。\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'，表示横坐标代表的是训练的周期数。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Accuracy'，表示纵坐标代表的是准确率的值。\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# 调用legend函数来显示图例，图例中将包含'Training acc'和'Validation acc'两个条目，分别对应两条曲线。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数来展示整个图表。这将弹出一个窗口，显示了训练集和验证集的准确率随epoch变化的曲线图。\n",
    "# 通过这个曲线图，可以直观地观察模型训练过程中准确率的变化趋势，评估模型的学习和泛化能力。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练集和验证集的准确率曲线，提供了一种直观的方式来观察模型在训练过程中的性能变化。这对于评估模型的性能、理解模型在训练集和验证集上的表现差异是非常有用的。图例、标题、坐标轴标签都增强了图表的可读性和信息的清晰度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "predictions = mobile_net.predict(val)\n",
    "\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "with open('MobileNet_confusion_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(cm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dd9ca",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个Python中用于序列化和反序列化的对象结构，可以用于存储和检索数据结构。\n",
    "import pickle\n",
    "\n",
    "# 从sklearn.metrics模块导入混淆矩阵(confusion_matrix)函数，该函数用于生成混淆矩阵，是评估分类模型性能的一个重要工具。\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 导入TensorFlow库，这是一个用于机器学习和深度神经网络的开源库，广泛用于构建和训练深度学习模型。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 使用训练好的mobile_net模型对验证集val进行预测，得到预测结果。\n",
    "predictions = mobile_net.predict(val)\n",
    "\n",
    "# 从验证集val中提取真实的标签。val是一个tf.data.Dataset对象，它由多组数据(x, y)组成。\n",
    "# 使用列表推导式和np.concatenate合并所有真实的标签到一个NumPy数组中，以便与预测结果进行比较。\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "# 使用混淆矩阵函数计算模型预测结果和真实标签之间的混淆矩阵。\n",
    "# np.argmax函数用于从预测结果中选择概率最高的类别作为预测类别。\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "# 使用with语句打开文件'MobileNet_confusion_matrix.pkl'，以二进制写入模式('wb')。\n",
    "# with语句可以确保文件正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('MobileNet_confusion_matrix.pkl', 'wb') as f:\n",
    "    # 使用pickle.dump函数将混淆矩阵cm序列化并保存到文件中。这允许以后可以重新加载混淆矩阵进行进一步分析。\n",
    "    pickle.dump(cm, f)\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的模型对验证集进行预测，计算混淆矩阵，并使用pickle将混淆矩阵序列化并保存到文件中。混淆矩阵是一个重要的工具，可以帮助我们理解模型在各个类别上的表现，尤其是在多分类问题中。通过序列化混淆矩阵，我们可以方便地将其存储和传输，以便后续分析或报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "with open('MobileNet_confusion_matrix.pkl', 'rb') as f:\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']  \n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf08ab",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个用于序列化和反序列化Python对象的库，可以用于从文件中读取和写入数据结构。\n",
    "import pickle\n",
    "\n",
    "# 导入NumPy库，这是一个适用于科学计算的库，提供了多维数组对象以及一系列操作这些数组的函数。\n",
    "import numpy as np\n",
    "\n",
    "# 导入matplotlib的pyplot模块，这是一个用于创建静态、交互式和动画可视化的库。\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从sklearn.metrics模块导入ConfusionMatrixDisplay类，这是一个用于图形化展示混淆矩阵的工具。\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# 使用with语句打开文件'MobileNet_confusion_matrix.pkl'，以二进制读取模式('rb')。\n",
    "# with语句可以确保文件在使用后正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('MobileNet_confusion_matrix.pkl', 'rb') as f:\n",
    "    # 使用pickle.load函数从文件中读取之前保存的混淆矩阵cm。\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "# 定义一个列表class_names，包含模型分类任务中所有类别的名称。这将用于在混淆矩阵的轴上显示类别名称。\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "\n",
    "# 创建一个ConfusionMatrixDisplay对象disp，传入混淆矩阵cm和类别名称class_names。\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# 调用disp对象的plot方法来绘制混淆矩阵。\n",
    "# cmap参数设置为'viridis'，这指定了颜色映射，用于根据不同的值显示不同的颜色。\n",
    "# xticks_rotation参数设置为'vertical'，这指定了x轴上的刻度标签的旋转方式，'vertical'表示垂直旋转，使得标签更易读。\n",
    "# values_format参数设置为'd'，这指定了混淆矩阵中的数值显示格式，'d'表示以整数形式显示。\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "# 调用plt.show()函数来展示混淆矩阵图形。这将弹出一个窗口，显示了混淆矩阵的图形化表示。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过加载之前保存的混淆矩阵，并使用`ConfusionMatrixDisplay`和`matplotlib`来生成并展示一个图形化的混淆矩阵。混淆矩阵是一个重要的工具，用于可视化分类模型的性能，尤其是在多分类问题中。通过图形化的展示，我们可以更直观地看到模型在各个类别上的表现，以及可能存在的混淆情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5597377",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "resize = tf.image.resize(img, (150,150))\n",
    "yhat = mobile_net.predict(np.expand_dims(resize/255, 0))\n",
    "max_index = np.argmax(yhat)\n",
    "label_to_class_name[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41577d",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 使用OpenCV库的imread函数读取指定路径下的图像文件。\n",
    "# 这里读取的是一个名为'Cyst- (1).jpg'的图像，它位于Kaggle输入数据集的'Cyst'子目录中。\n",
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "\n",
    "# 使用matplotlib的pyplot模块展示读取到的图像。\n",
    "# imshow函数将图像显示在屏幕上。\n",
    "plt.imshow(img)\n",
    "\n",
    "# 调用show函数来展示图像。\n",
    "# 这将弹出一个窗口，显示了加载的图像。\n",
    "plt.show()\n",
    "\n",
    "# 使用TensorFlow的image.resize函数将图像大小调整为150x150像素。\n",
    "# 这个尺寸与之前模型训练时使用的输入尺寸一致。\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "\n",
    "# 将调整大小后的图像数据通过mobile_net模型进行预测。\n",
    "# 由于模型预期的输入是一个批次的图像，这里使用np.expand_dims将图像数组扩展到一个新的维度，形成一个新的轴0，从而满足模型输入的批次要求。\n",
    "# 同时，图像数据被归一化到0-1范围内，这是因为模型是在归一化后的数据上训练的。\n",
    "yhat = mobile_net.predict(np.expand_dims(resize / 255, 0))\n",
    "\n",
    "# 使用NumPy的argmax函数找出预测概率最高的类别索引。\n",
    "# 这将给出一个整数，表示在模型预测中哪个类别的概率最高。\n",
    "max_index = np.argmax(yhat)\n",
    "\n",
    "# 使用之前定义的label_to_class_name字典，通过预测得到的类别索引获取对应的类别名称。\n",
    "# 这将给出一个字符串，表示预测结果中概率最高的类别的名称。\n",
    "label_to_class_name[max_index]\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的MobileNet模型对单个图像进行预测，并输出预测的类别名称。首先，它读取一个图像文件，然后调整图像大小以匹配模型的输入要求，接着归一化图像数据并进行预测。最后，它通过索引获取预测结果中概率最高的类别名称。这个过程是深度学习模型用于图像分类任务中的典型应用流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e12a8",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "resnet_model = ResNet50(input_shape=input_shape, weights=weights, include_top=False)\n",
    "        \n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "x = GlobalAveragePooling2D()(resnet_model.output)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "resnet_net = Model(inputs=resnet_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116fe70",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入TensorFlow库，这是一个用于机器学习和深度神经网络的开源库。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 从tensorflow.keras.layers模块导入Dense（全连接层）、Flatten（展平层）、BatchNormalization（批量归一化层）和Dropout（丢弃层）。\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "# 从tensorflow.keras.applications模块导入预训练的深度学习模型ResNet50。\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# 从tensorflow.keras.models模块导入Model类，用于创建自定义的模型。\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 初始化ResNet50模型，使用ImageNet预训练权重但不包括网络顶部的全连接层，因为将添加自定义的顶层。\n",
    "# input_shape是输入图像的尺寸，weights参数指定使用预训练权重（通常是'imagenet'），include_top=False表示不包含模型顶部的分类层。\n",
    "resnet_model = ResNet50(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "\n",
    "# 遍历ResNet50模型中的所有层，并将它们设置为可训练的。\n",
    "# 这允许在微调过程中更新所有层的权重。\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 使用GlobalAveragePooling2D层将ResNet50模型的输出进行全局平均池化，将多维特征图转换为一维特征向量。\n",
    "x = GlobalAveragePooling2D()(resnet_model.output)\n",
    "\n",
    "# 添加一个具有512个神经元的全连接层，使用ReLU激活函数，并应用L2正则化器，正则化因子为1e-4，以减少过拟合。\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "\n",
    "# 添加一个Dropout层，丢弃率为0.5，即在训练过程中随机丢弃50%的神经元输出，以进一步减少过拟合。\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# 添加输出层，其神经元数量应与分类问题中的类别数相同（由变量num_classes指定），使用softmax激活函数进行多分类。\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# 使用Model类创建自定义模型resnet_net，输入为ResNet50模型的输入，输出为新添加的全连接层的输出。\n",
    "resnet_net = Model(inputs=resnet_model.input, outputs=output)\n",
    "```\n",
    "\n",
    "这段代码演示了如何使用预训练的ResNet50模型作为特征提取器，并通过添加全局平均池化层和全连接层来构建一个新的分类模型。通过这种方式，可以利用预训练模型的强大特征提取能力，并通过微调（fine-tuning）来适应特定的分类任务。代码中未指定的`input_shape`和`num_classes`应根据具体的任务需求进行设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_net.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8de58",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 对resnet_net模型调用compile方法来配置模型的训练参数。\n",
    "# compile方法是准备模型进行训练的第一步，它设置了模型的优化器、损失函数和评估指标。\n",
    "resnet_net.compile(\n",
    "    # optimizer参数指定优化器为'adam'，这是一种自适应学习率的优化算法，常用于深度学习任务中。\n",
    "    optimizer='adam',\n",
    "    # loss参数指定损失函数为'sparse_categorical_crossentropy'，适用于多分类问题，其中标签是整数形式。\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # metrics参数指定在训练过程中要监控的指标是准确率（'accuracy'），这是衡量分类模型性能的一个常用指标。\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "这段代码通过编译模型，为接下来的模型训练做好了准备。选择的优化器、损失函数和评估指标对于模型能否有效学习以及如何评估学习效果至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = resnet_net.fit(\n",
    "  train,\n",
    "  validation_data=val,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdec34",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 定义变量epochs并设置其值为10，这个变量代表训练模型时将要进行的迭代次数，也就是训练周期(epoch)的数量。\n",
    "epochs = 10\n",
    "\n",
    "# 使用resnet_net模型的fit方法开始训练模型。\n",
    "# fit方法是用于训练模型的函数，它将数据输入模型并根据提供的优化器和损失函数调整模型的权重。\n",
    "history = resnet_net.fit(\n",
    "    # 将train数据集作为训练数据输入模型。\n",
    "    train,\n",
    "    # validation_data参数指定了验证集，这里使用的是之前定义的val数据集。\n",
    "    # 在每个epoch结束后，模型将在验证集上进行评估，以监控模型在未见数据上的表现。\n",
    "    validation_data=val,\n",
    "    # epochs参数设置为之前定义的epochs变量，即训练将进行10个epoch。\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# 训练完成后，history变量将包含训练过程中的所有记录，包括每个epoch的训练和验证损失(loss)以及准确率(accuracy)。\n",
    "# 这些信息可以用来分析模型的学习过程和性能。\n",
    "```\n",
    "\n",
    "这段代码启动了模型的训练过程，并且每经过一个epoch，都会在训练集和验证集上评估模型的性能。训练结束后，`history`对象包含了整个训练过程中的重要信息，这些信息对于后续分析模型的学习动态和调试模型非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65beb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('ResNet_training.csv', index=False)\n",
    "resnet_net.save('ResNet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde18ab",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pandas库中的DataFrame函数，将history对象转换成一个Pandas DataFrame对象。\n",
    "# history对象是模型的fit方法返回的，包含了模型训练和验证过程中的损失和准确率等信息。\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# 使用DataFrame对象的to_csv方法将数据保存到CSV文件中。\n",
    "# 'ResNet_training.csv'是指定的文件名，这个文件将包含模型训练的历史记录。\n",
    "# index=False参数表示在保存时不包括行索引，因为行索引通常是自动生成的，不一定需要保存。\n",
    "history_df.to_csv('ResNet_training.csv', index=False)\n",
    "\n",
    "# 使用resnet_net模型的save方法将整个模型保存到指定的文件中。\n",
    "# 'ResNet_model.h5'是文件的保存路径及名称。.h5是HDF5格式的文件扩展名，这是一种用于存储和组织大量数据的文件格式，常用于保存深度学习模型。\n",
    "# 保存模型后，可以重新加载该模型，包括其架构、权重和优化器状态等，以便进行后续的推理或继续训练。\n",
    "resnet_net.save('ResNet_model.h5')\n",
    "```\n",
    "\n",
    "这段代码首先将模型训练过程中收集的历史数据转换成一个结构化的表格，并将其保存到CSV文件中。然后，它将整个模型保存到一个HDF5文件中。这样做有助于后续对模型训练过程的回顾和分析，同时也方便了模型的共享和部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6071279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you have a model 'model' and a test dataset 'test_dataset'\n",
    "\n",
    "# Make predictions using your model\n",
    "predictions = resnet_net.predict(val)\n",
    "\n",
    "# Get the true labels from the test dataset\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "# Save the confusion matrix in pkl format\n",
    "with open('ResNet_confusion_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(cm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a5969",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个用于序列化和反序列化Python对象的库，可以用于存储和检索数据结构。\n",
    "import pickle\n",
    "\n",
    "# 从sklearn.metrics模块导入混淆矩阵(confusion_matrix)函数，该函数用于生成混淆矩阵，是评估分类模型性能的一个重要工具。\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 导入TensorFlow库，这是一个用于机器学习和深度神经网络的开源库，广泛用于构建和训练深度学习模型。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 假设你已经有一个训练好的模型'model'和一个测试数据集'test_dataset'，这里的代码将使用这些预定义的变量。\n",
    "\n",
    "# 使用训练好的resnet_net模型对验证集val进行预测，得到预测结果。\n",
    "predictions = resnet_net.predict(val)\n",
    "\n",
    "# 从验证集val中提取真实的标签。val是一个tf.data.Dataset对象，它由多组数据(x, y)组成。\n",
    "# 使用列表推导式和np.concatenate合并所有真实的标签到一个NumPy数组中，以便与预测结果进行比较。\n",
    "true_labels = np.concatenate([y for x, y in val], axis=0)\n",
    "\n",
    "# 使用混淆矩阵函数计算模型预测结果和真实标签之间的混淆矩阵。\n",
    "# np.argmax函数用于从预测结果中选择概率最高的类别作为预测类别。\n",
    "cm = confusion_matrix(true_labels, np.argmax(predictions, axis=1))\n",
    "\n",
    "# 使用with语句打开文件'ResNet_confusion_matrix.pkl'，以二进制写入模式('wb')。\n",
    "# with语句可以确保文件正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('ResNet_confusion_matrix.pkl', 'wb') as f:\n",
    "    # 使用pickle.dump函数将混淆矩阵cm序列化并保存到文件中。这允许以后可以重新加载混淆矩阵进行进一步分析。\n",
    "    pickle.dump(cm, f)\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的模型对验证集进行预测，计算混淆矩阵，并使用pickle将混淆矩阵序列化并保存到文件中。混淆矩阵是一个重要的工具，可以帮助我们理解模型在各个类别上的表现，尤其是在多分类问题中。通过序列化混淆矩阵，我们可以方便地将其存储和传输，以便后续分析或报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ecba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Load the saved confusion matrix\n",
    "with open('ResNet_confusion_matrix.pkl', 'rb') as f:\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "# Define class names (assuming you have them)\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']   # Replace with your actual class names\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "# Show the confusion matrix plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ec7b9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入pickle库，这是一个用于序列化和反序列化Python对象的库，可以用于从文件中读取和写入数据结构。\n",
    "import pickle\n",
    "\n",
    "# 导入NumPy库，这是一个适用于科学计算的库，提供了多维数组对象以及一系列操作这些数组的函数。\n",
    "import numpy as np\n",
    "\n",
    "# 导入matplotlib的pyplot模块，这是一个用于创建静态、交互式和动画可视化的库。\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从sklearn.metrics模块导入ConfusionMatrixDisplay类，这是一个用于图形化展示混淆矩阵的工具。\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# 使用with语句打开文件'ResNet_confusion_matrix.pkl'，以二进制读取模式('rb')。\n",
    "# with语句可以确保文件在使用后正确关闭，即使在发生异常的情况下也是如此。\n",
    "with open('ResNet_confusion_matrix.pkl', 'rb') as f:\n",
    "    # 使用pickle.load函数从文件中读取之前保存的混淆矩阵cm。\n",
    "    cm = pickle.load(f)\n",
    "\n",
    "# 定义一个列表class_names，包含模型分类任务中所有类别的名称。这将用于在混淆矩阵的轴上显示类别名称。\n",
    "# 这里的类名是示例，应该替换为实际的类别名称。\n",
    "class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "\n",
    "# 创建一个ConfusionMatrixDisplay对象disp，传入混淆矩阵cm和类别名称class_names。\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# 调用disp对象的plot方法来绘制混淆矩阵。\n",
    "# cmap参数设置为'viridis'，这指定了颜色映射，用于根据不同的值显示不同的颜色。\n",
    "# xticks_rotation参数设置为'vertical'，这指定了x轴上的刻度标签的旋转方式，'vertical'表示垂直旋转，使得标签更易读。\n",
    "# values_format参数设置为'd'，这指定了混淆矩阵中的数值显示格式，'d'表示以整数形式显示。\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical', values_format='d')\n",
    "\n",
    "# 调用plt.show()函数来展示混淆矩阵图形。这将弹出一个窗口，显示了混淆矩阵的图形化表示。\n",
    "# 通过这个曲线图，可以直观地观察模型在各个类别上的表现，评估模型的分类性能。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过加载之前保存的混淆矩阵，并使用`ConfusionMatrixDisplay`和`matplotlib`来生成并展示一个图形化的混淆矩阵。混淆矩阵是一个重要的工具，用于可视化分类模型的性能，尤其是在多分类问题中。通过图形化的展示，我们可以更直观地看到模型在各个类别上的表现，以及可能存在的混淆情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eab09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "resize = tf.image.resize(img, (150,150))\n",
    "yhat = resnet_net.predict(np.expand_dims(resize/255, 0))\n",
    "max_index = np.argmax(yhat)\n",
    "label_to_class_name[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643af05",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 导入OpenCV库中的imread函数，用于读取图像文件。\n",
    "img = cv2.imread('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1).jpg')\n",
    "\n",
    "# 使用matplotlib的pyplot模块展示读取到的图像。\n",
    "plt.imshow(img)\n",
    "\n",
    "# 调用show函数来展示图像。\n",
    "# 这将弹出一个窗口，显示了加载的图像。\n",
    "plt.show()\n",
    "\n",
    "# 使用TensorFlow的image.resize函数将图像大小调整为150x150像素。\n",
    "# 这个尺寸与之前模型训练时使用的输入尺寸一致。\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "\n",
    "# 将调整大小后的图像数据通过resnet_net模型进行预测。\n",
    "# 由于模型预期的输入是一个批次的图像，这里使用np.expand_dims将图像数组扩展到一个新的维度，形成一个新的轴0，从而满足模型输入的批次要求。\n",
    "# 同时，图像数据被归一化到0-1范围内，这是因为模型是在归一化后的数据上训练的。\n",
    "yhat = resnet_net.predict(np.expand_dims(resize / 255, 0))\n",
    "\n",
    "# 使用NumPy的argmax函数找出预测概率最高的类别索引。\n",
    "# 这将给出一个整数，表示在模型预测中哪个类别的概率最高。\n",
    "max_index = np.argmax(yhat)\n",
    "\n",
    "# 假设存在一个label_to_class_name映射，它将类别索引映射到对应的类别名称。\n",
    "# 使用该映射通过预测得到的类别索引获取对应的类别名称。\n",
    "# 注意：label_to_class_name映射需要预先定义，并且包含模型能预测的所有类别名称。\n",
    "label_to_class_name[max_index]\n",
    "```\n",
    "\n",
    "这段代码展示了如何使用训练好的ResNet模型对单个图像进行预测，并输出预测的类别名称。首先，它读取一个图像文件，然后调整图像大小以匹配模型的输入要求，接着归一化图像数据并进行预测。最后，它通过一个映射获取预测结果中概率最高的类别名称。这个过程是深度学习模型用于图像分类任务中的典型应用流程。注意，`label_to_class_name`映射需要根据实际情况进行定义，以确保能够正确地将预测的索引转换为对应的类别名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99975eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f1596",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从训练历史对象history.history中提取训练损失值，并将其存储在变量loss中。\n",
    "# history.history是一个字典，其中包含训练过程中的各种指标，'loss'键对应的值是训练损失的列表。\n",
    "loss = history.history['loss']\n",
    "\n",
    "# 从训练历史对象history.history中提取验证损失值，并将其存储在变量val_loss中。\n",
    "# 'val_loss'键对应的值是验证损失的列表，记录了每个epoch后在验证集上的损失。\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# 使用range函数创建一个表示epoch次数的整数序列，从1到训练的epoch数加1。\n",
    "# 因为列表索引是从0开始的，所以使用len(loss)+1确保包括所有的epoch。\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制训练损失和验证损失曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数设置为'Training loss'，为训练损失曲线设置图例。\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "\n",
    "# 绘制验证损失曲线，线条颜色为红色，图例为'Validation loss'。\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Loss'，表示正在展示的是训练和验证的损失。\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'，表示横坐标代表的是训练的周期数。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Loss'，表示纵坐标代表的是损失值。\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 调用legend函数显示图例，图例中将包含'Training loss'和'Validation loss'两个条目，分别对应两条曲线。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数展示整个图表。\n",
    "# 这将弹出一个窗口，显示了训练损失和验证损失随epoch变化的曲线图，用于分析模型训练过程中损失的变化趋势。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练损失和验证损失随训练周期变化的曲线图，提供了一种直观的方式来观察模型在训练过程中的性能变化。这对于理解模型的学习能力、诊断过拟合或欠拟合问题以及决定何时停止训练都是非常有用的。图例、标题、坐标轴标签都增强了图表的可读性和信息的清晰度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490163f",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# 从训练历史对象history.history中提取训练集上的准确率数据，并将其存储在变量acc中。\n",
    "# 'accuracy'键对应的值是模型在训练集上每个epoch后的准确率。\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "# 从训练历史对象history.history中提取验证集上的准确率数据，并将其存储在变量val_acc中。\n",
    "# 'val_accuracy'键对应的值是模型在验证集上每个epoch后的准确率。\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制模型在训练集上的准确率变化曲线。\n",
    "# 'b'表示线条颜色为蓝色，'label'参数设置为'Training acc'，为训练准确率曲线设置图例。\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "\n",
    "# 使用matplotlib的pyplot模块绘制模型在验证集上的准确率变化曲线。\n",
    "# 'r'表示线条颜色为红色，'label'参数设置为'Validation acc'，为验证准确率曲线设置图例。\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "# 设置图表的标题为'Training and Validation Accuracy'，表示正在展示的是训练集和验证集的准确率。\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# 设置x轴的标签为'Epochs'，表示横坐标代表的是训练的周期数。\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# 设置y轴的标签为'Accuracy'，表示纵坐标代表的是准确率的值。\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# 调用legend函数显示图例，图例中将包含'Training acc'和'Validation acc'两个条目，分别对应两条曲线。\n",
    "plt.legend()\n",
    "\n",
    "# 调用show函数展示整个图表。\n",
    "# 这将弹出一个窗口，显示了训练集和验证集的准确率随epoch变化的曲线图，用于分析模型训练过程中准确率的变化趋势和模型的泛化能力。\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "这段代码通过绘制训练集和验证集的准确率曲线，提供了一种直观的方式来观察模型在训练过程中的性能变化。这对于评估模型的性能、理解模型在训练集和验证集上的表现差异是非常有用的。图例、标题、坐标轴标签都增强了图表的可读性和信息的清晰度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
