{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e75e0c2",
   "metadata": {},
   "source": [
    "**01Heartbeat Sound LSTM Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237e251",
   "metadata": {},
   "source": [
    "## Heartbeat sounds\n",
    "\n",
    "Heart sounds are produced from a specific cardiac event such as closure of a valve or tensing of a chordae tendineae.\n",
    "\n",
    "• S1 result from the closing of the mitral and tricuspid valves.\n",
    "\n",
    "• S2 produced by the closure of the aortic and pulmonic valves.\n",
    "\n",
    "In medicine we call the ‘lub’ sound \"S1\" and the ‘dub’ sound \"S2\".\n",
    "\n",
    "You can learn short intro about heart sounds from this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=dBwr2GZCmQM\n",
    "\n",
    "Why using Deep Learning in this ?\n",
    "\n",
    "It is a subset of Neural Networks ( a computing systems which is insprired by biological neural networks ). And by stacking a lot of Neural Networks layers, we will get a Deep Learning model which might contains multiple billions parameters and is capable of handle very complicated problems that no classical algorithm can be effective.\n",
    "\n",
    "Deep Learning is especially powerful with unstructrured data (Image, Text, Sound, Video, …).\n",
    "\n",
    "## About data:\n",
    "Is challenge published in 2012 to classify the heart sound to some categories from ‘AISTATS’ . Data has been gathered from two sources (A) and (B).\n",
    "\n",
    "A) from the general public via the iStethoscope Pro.\n",
    "\n",
    "B) from a clinic trial in hospitals using the digital stethoscope DigiScope.\n",
    "\n",
    "Before we work on this nootebook we handle the data folders and conctante both sources(A&B) to be easy to deal with it.\n",
    "\n",
    "Original Data here: http://www.peterjbentley.com/heartchallenge/\n",
    "\n",
    "## Import main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa #To deal with Audio files\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0565b68",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "import os  # 导入os模块，用于与操作系统交互，如文件和目录操作\n",
    "import glob  # 导入glob模块，用于文件模式匹配，可以找到符合特定模式的文件路径\n",
    "import fnmatch  # 导入fnmatch模块，用于文件名模式匹配，常用于字符串比较\n",
    "import pandas as pd  # 导入pandas库，并使用别名pd，这是一个数据分析和操作的强大工具\n",
    "import numpy as np  # 导入numpy库，并使用别名np，这是一个进行科学计算的基础库\n",
    "import librosa  # 导入librosa库，用于音频文件的处理，如音频信号分析\n",
    "import librosa.display  # 导入librosa的显示模块，用于可视化音频信号\n",
    "import matplotlib.pyplot as plt  # 导入matplotlib的pyplot模块，并使用别名plt，用于绘图\n",
    "import IPython.display as ipd  # 导入IPython的display模块，并使用别名ipd，用于显示多媒体内容\n",
    "import math  # 导入math模块，提供了一系列数学函数，如三角函数、指数、对数等\n",
    "import tensorflow as tf  # 导入tensorflow库，并使用别名tf，这是一个用于机器学习的开源框架\n",
    "```\n",
    "\n",
    "这些注释解释了每行代码导入的模块及其用途，它们为Python程序提供了处理文件、数据分析、音频处理、绘图、数学计算和机器学习等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/heartbeat-sound/Heartbeat_Sound/\"\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f723262",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "data_path = \"../input/heartbeat-sound/Heartbeat_Sound/\"  # 定义一个字符串变量data_path，它包含了一个文件路径。这个路径指向一个名为\"Heartbeat_Sound\"的目录，该目录位于上一级的\"input/heartbeat-sound\"目录中。\n",
    "print(os.listdir(data_path))  # 使用os模块的listdir函数，它接受一个目录路径作为参数，返回该目录下所有文件和子目录的列表。然后将这个列表打印出来，通常用于查看目录内容。\n",
    "```\n",
    "\n",
    "这段代码的作用是列出指定目录下的所有文件和子目录的名称。这对于了解目录结构或进行进一步的文件操作非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f98f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarin_data      = data_path \n",
    "unlabel_data        = data_path  + \"/unlabel/\"\n",
    "\n",
    "normal_data     = tarin_data + '/normal/'\n",
    "murmur_data     = tarin_data + '/murmur/'\n",
    "extrastole_data = tarin_data + '/extrastole/'\n",
    "artifact_data   = tarin_data + '/artifact/'\n",
    "extrahls_data   = tarin_data + \"/extrahls/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d698920",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "tarin_data = data_path  # 将之前定义的data_path变量赋值给tarin_data变量，作为训练数据集的路径。\n",
    "unlabel_data = data_path + \"/unlabel/\"  # 将data_path与子目录\"/unlabel/\"拼接，形成未标记数据集的路径。\n",
    "\n",
    "normal_data = tarin_data + '/normal/'  # 将tarin_data与子目录'/normal/'拼接，形成正常心跳声音数据集的路径。\n",
    "murmur_data = tarin_data + '/murmur/'  # 将tarin_data与子目录'/murmur/'拼接，形成心脏杂音数据集的路径。\n",
    "extrastole_data = tarin_data + '/extrastole/'  # 将tarin_data与子目录'/extrastole/'拼接，形成额外心动过速数据集的路径。\n",
    "artifact_data = tarin_data + '/artifact/'  # 将tarin_data与子目录'/artifact/'拼接，形成伪迹数据集的路径。\n",
    "extrahls_data = tarin_data + \"/extrahls/\"  # 将tarin_data与子目录\"/extrahls/\"拼接，形成额外高保真声音数据集的路径。\n",
    "```\n",
    "\n",
    "这段代码通过拼接字符串来构建不同的数据集路径。这些路径指向存放心跳声音文件的不同分类目录，用于后续的数据加载和处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc50976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal files:\", len(os.listdir(normal_data))) #length of normal training sounds\n",
    "print(\"Murmur files:\",len(os.listdir(murmur_data))) #length of murmur training sounds \n",
    "print(\"Extrastole files\", len(os.listdir(extrastole_data))) #length of extrastole training sounds \n",
    "print(\"Artifact files:\",len(os.listdir(artifact_data))) #length of artifact training sounds \n",
    "print(\"Extrahls files:\",len(os.listdir(extrahls_data))) #length of extrahls training sounds \n",
    "\n",
    "print('TOTAL TRAIN SOUNDS:', len(os.listdir(normal_data)) \n",
    "                              + len(os.listdir(murmur_data))\n",
    "                              + len(os.listdir(extrastole_data))\n",
    "                              + len(os.listdir(artifact_data))\n",
    "                              + len(os.listdir(extrahls_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e48df",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "print(\"Normal files:\", len(os.listdir(normal_data)))  # 打印出正常心跳声音数据集的文件数量。使用os.listdir(normal_data)获取该目录下所有文件和目录的列表，然后计算列表长度。\n",
    "print(\"Murmur files:\", len(os.listdir(murmur_data)))  # 打印出心脏杂音数据集的文件数量。同样，使用os.listdir(murmur_data)获取列表并计算长度。\n",
    "print(\"Extrastole files\", len(os.listdir(extrastole_data)))  # 打印出额外心动过速数据集的文件数量。使用os.listdir(extrastole_data)获取列表并计算长度。\n",
    "print(\"Artifact files:\", len(os.listdir(artifact_data)))  # 打印出伪迹数据集的文件数量。使用os.listdir(artifact_data)获取列表并计算长度。\n",
    "print(\"Extrahls files:\", len(os.listdir(extrahls_data)))  # 打印出额外高保真声音数据集的文件数量。使用os.listdir(extrahls_data)获取列表并计算长度。\n",
    "\n",
    "print('TOTAL TRAIN SOUNDS:', len(os.listdir(normal_data))  # 打印出所有训练声音文件的总数。\n",
    "                              + len(os.listdir(murmur_data))  # 将各个数据集目录的文件数量相加。\n",
    "                              + len(os.listdir(extrastole_data))  # 包括正常心跳、心脏杂音、额外心动过速、伪迹和额外高保真声音数据集。\n",
    "                              + len(os.listdir(artifact_data))  # 计算总和，得到训练集的总文件数量。\n",
    "                              + len(os.listdir(extrahls_data)))  # 完成总文件数量的计算并打印结果。\n",
    "```\n",
    "\n",
    "这段代码的目的是统计并打印出指定目录下各类心跳声音数据集的文件数量，以及所有训练声音文件的总数。这对于了解数据集的规模和准备数据处理非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test sounds: \", len(os.listdir(unlabel_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded751a",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "print(\"Test sounds: \", len(os.listdir(unlabel_data)))  # 打印出未标记测试数据集的文件数量。首先使用os.listdir(unlabel_data)获取unlabel_data路径下所有文件和目录的列表，然后计算该列表的长度，并将结果打印出来。列表的长度即代表了该目录下的文件和子目录的总数。\n",
    "```\n",
    "\n",
    "这段代码的作用是统计并打印出未标记测试数据集（unlabel_data）中的文件数量。这对于了解测试数据集的大小和进行后续的数据处理或分析工作很有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b70fb",
   "metadata": {},
   "source": [
    "## EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58877106",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([len(os.listdir(normal_data)),\n",
    "              len(os.listdir(murmur_data)),\n",
    "              len(os.listdir(extrastole_data)),\n",
    "              len(os.listdir(artifact_data)),\n",
    "              len(os.listdir(extrahls_data))])\n",
    "labels = ['normal', 'murmur', 'extrastole', 'artifact', 'extrahls']\n",
    "plt.pie(x, labels = labels, autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3948e",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "x = np.array([len(os.listdir(normal_data)),  # 使用numpy库创建一个数组x，数组中的元素是各个心跳声音数据子目录下的文件数量。\n",
    "              len(os.listdir(murmur_data)),\n",
    "              len(os.listdir(extrastole_data)),\n",
    "              len(os.listdir(artifact_data)),\n",
    "              len(os.listdir(extrahls_data))])  # 每个子目录的文件数量通过os.listdir()函数获取，并计算其长度。\n",
    "\n",
    "labels = ['normal', 'murmur', 'extrastole', 'artifact', 'extrahls']  # 定义一个列表labels，包含了每个数组元素对应的类别标签。\n",
    "\n",
    "plt.pie(x, labels = labels, autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})  # 使用matplotlib的pyplot模块绘制一个饼图。参数说明：\n",
    "  - x: 饼图的数据来源数组。\n",
    "  - labels: 饼图每个部分对应的标签。\n",
    "  - autopct: 格式化字符串，用于显示每个部分的百分比，'%.0f%%'表示显示整数百分比。\n",
    "  - radius: 饼图的半径大小。\n",
    "  - textprops: 设置文本属性，这里设置字体大小为16。\n",
    "\n",
    "plt.show()  # 显示图表。在交互式环境中，如Jupyter Notebook，这将在输出区域显示图表；在脚本中运行时，会打开一个窗口显示图表。\n",
    "```\n",
    "\n",
    "这段代码的作用是统计不同类别心跳声音数据集的文件数量，并将这些数量以饼图的形式可视化，从而直观地展示每个类别在总数据集中的比例。这对于快速了解数据集的分布情况非常有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad9b37",
   "metadata": {},
   "source": [
    "### Visualizing random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_random_sample(folder_name):\n",
    "  #to hear the audio sample\n",
    "  random_sample             = np.random.randint(0,len(os.listdir(folder_name)))\n",
    "  sample_sound              = os.listdir(folder_name)[random_sample]\n",
    "  sample_address            = folder_name + sample_sound\n",
    "  sample_sound, sample_rate = librosa.load(sample_address)\n",
    "  sample_audio              = ipd.Audio(sample_sound, rate=sample_rate)\n",
    "  return sample_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bdc38a",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "def visulize_random_sample(folder_name):  # 定义一个函数visulize_random_sample，它接受一个参数folder_name，即一个文件夹的路径。\n",
    "  # to hear the audio sample  # 注释：以下代码用于播放随机选取的音频样本。\n",
    "  random_sample = np.random.randint(0, len(os.listdir(folder_name)))  # 从0到folder_name目录下的文件数量（不包括目录）中随机选择一个整数，作为随机样本的索引。\n",
    "  sample_sound = os.listdir(folder_name)[random_sample]  # 使用随机索引从folder_name目录中选取一个文件名。\n",
    "  sample_address = folder_name + sample_sound  # 将选定的文件名与目录路径拼接，形成完整的文件路径。\n",
    "  sample_sound, sample_rate = librosa.load(sample_address)  # 使用librosa.load()函数加载音频文件，返回音频信号数据sample_sound和对应的采样率sample_rate。\n",
    "  sample_audio = ipd.Audio(sample_sound, rate=sample_rate)  # 使用IPython.display.Audio()函数创建一个音频对象，可以用于播放，sample_sound是音频信号数据，sample_rate是采样率。\n",
    "  return sample_audio  # 函数返回创建的音频对象sample_audio，这样在调用该函数的地方就可以播放选定的随机音频样本。\n",
    "```\n",
    "\n",
    "这段代码定义了一个函数，用于随机选择并播放指定文件夹中的一个音频样本。这对于快速预览和听取数据集中的音频文件非常有用，尤其是在进行数据探索或质量检查时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72529e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize_random_sample(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8a304",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "visulize_random_sample(normal_data)  # 调用之前定义的函数visulize_random_sample，并传入参数normal_data。normal_data是一个包含正常心跳声音文件的目录路径。\n",
    "# 函数将执行以下步骤：\n",
    "# 1. 随机选择normal_data目录中的一个音频文件。\n",
    "# 2. 使用librosa库加载这个音频文件，并获取音频信号和采样率。\n",
    "# 3. 使用IPython.display库的Audio函数创建一个可以播放的音频对象。\n",
    "# 4. 返回这个音频对象，调用者可以使用它来播放选中的音频样本。\n",
    "```\n",
    "\n",
    "当你执行这行代码时，它会触发函数`visulize_random_sample`的所有步骤，并最终播放normal_data目录下随机选择的一个正常心跳声音的音频样本。这个操作通常用在数据分析或机器学习项目中，用于验证数据集的质量和内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609aef0c",
   "metadata": {},
   "source": [
    "#### 1. Normal sound\n",
    "Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cf362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from normal folder\n",
    "random_normal= np.random.randint(0,len(os.listdir(normal_data))) \n",
    "normal_sound = os.listdir(normal_data)[random_normal]\n",
    "normal_sound_address = normal_data+normal_sound\n",
    "normal_sound_sample,sample_rate = librosa.load(normal_sound_address)\n",
    "ipd.Audio(normal_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca13b0",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# Choose random sound from normal folder  # 注释：从normal_data目录中随机选择一个声音文件。\n",
    "random_normal = np.random.randint(0, len(os.listdir(normal_data)))  # 使用numpy的randint函数生成一个从0到normal_data目录下文件数量（不包括子目录）的随机整数，作为随机选择声音文件的索引。\n",
    "\n",
    "normal_sound = os.listdir(normal_data)[random_normal]  # 根据生成的随机索引，从normal_data目录中选择一个声音文件的名称。\n",
    "\n",
    "normal_sound_address = normal_data + normal_sound  # 将选择的声音文件名称与目录路径拼接，形成完整的文件路径。\n",
    "\n",
    "normal_sound_sample, sample_rate = librosa.load(normal_sound_address)  # 使用librosa库的load函数加载声音文件，得到声音样本数据normal_sound_sample和对应的采样率sample_rate。\n",
    "\n",
    "ipd.Audio(normal_sound_sample, rate=sample_rate)  # 使用IPython.display库的Audio函数创建一个音频对象，该对象可以用于播放声音。normal_sound_sample是音频样本数据，sample_rate是采样率。这行代码将触发音频播放。\n",
    "```\n",
    "\n",
    "这段代码的作用是从包含正常心跳声音的目录`normal_data`中随机选择一个音频文件，然后加载并播放这个音频样本。这对于快速检查数据集中的音频文件内容非常有用，尤其是在进行数据探索或演示时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d14165",
   "metadata": {},
   "source": [
    "##### Waveform\n",
    "Sound is the pressure of air propagates to our ear. Digital audio file is gotten from a sound sensor that can detects sound waves and converting it to electrical signals.\n",
    "\n",
    "Specifically, it's telling us about the wave's displacement, and how it changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(normal_sound_sample, sr = sample_rate)\n",
    "plt.title(\"Normal Sound\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89faf0eb",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(20,5))  # 使用matplotlib的pyplot模块创建一个新的图形对象。figsize参数设置图形的大小，这里设置为宽20英寸，高5英寸。\n",
    "\n",
    "librosa.display.waveplot(normal_sound_sample, sr=sample_rate)  # 使用librosa库的waveplot函数绘制音频波形图。normal_sound_sample是音频样本数据，sr=sample_rate指定了音频的采样率。\n",
    "\n",
    "plt.title(\"Normal Sound\")  # 设置图形的标题为\"Normal Sound\"。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "\n",
    "plt.ylabel(\"Amplitude\")  # 设置y轴的标签为\"Amplitude\"，表示振幅轴。\n",
    "\n",
    "plt.show()  # 显示图形。在交互式环境中，如Jupyter Notebook，这将在输出区域显示图形；在脚本中运行时，会打开一个窗口显示图形。\n",
    "```\n",
    "\n",
    "这段代码的作用是绘制并显示之前加载的`normal_sound_sample`音频样本的波形图。通过波形图，可以直观地观察音频信号随时间变化的情况，这对于音频分析和理解音频内容非常有帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c46d0d",
   "metadata": {},
   "source": [
    "X axis, represents time. Y-axis measures displacement of air molecules.This is where amplitude comes in. It measures how much a molecule is displaced from its resting position.\n",
    "\n",
    "##### Spectrum\n",
    "A sound spectrum is a representation of a sound – usually a short sample of a sound – in terms of the amount of vibration at each individual frequency. It is usually presented as a graph of either power or pressure as a function of frequency. The power or pressure is usually measured in decibels and the frequency is measured in vibrations per second (or hertz, abbreviation Hz) or thousands of vibrations per second (kilohertz, abbreviation kHz).\n",
    "\n",
    "The spectrum expresses the frequency composition of the sound and is obtained by analyzing the sound. A sound spectrum is usually represented in a coordinate plane where the frequency f is plotted along the axis of abscissas and the amplitude A, or intensity, of a harmonic component with a given frequency is plotted along the axis of ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f21296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_normal = np.fft.fft(normal_sound_sample)\n",
    "magnitude_normal = np.abs(fft_normal)\n",
    "freq_normal = np.linspace(0,sample_rate, len(magnitude_normal)) \n",
    "half_freq = freq_normal[:int(len(freq_normal)/2)]\n",
    "half_magnitude = magnitude_normal[:int(len(freq_normal)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.title(\"Spectrum\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e42f9d",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "fft_normal = np.fft.fft(normal_sound_sample)  # 使用numpy的fft函数对音频样本normal_sound_sample进行快速傅里叶变换（FFT），得到频域信号。\n",
    "\n",
    "magnitude_normal = np.abs(fft_normal)  # 计算FFT结果的绝对值，得到频域信号的幅度。\n",
    "\n",
    "freq_normal = np.linspace(0, sample_rate, len(magnitude_normal))  # 使用numpy的linspace函数生成一个从0到sample_rate的等间隔采样点数组，采样点的数量与音频样本的帧数相同。\n",
    "\n",
    "half_freq = freq_normal[:int(len(freq_normal)/2)]  # 由于FFT的结果是对称的，这里只取频率数组的一半，即从0到Nyquist频率的部分。\n",
    "\n",
    "half_magnitude = magnitude_normal[:int(len(magnitude_normal)/2)]  # 同样，只取幅度数组的一半，与half_freq对应。\n",
    "\n",
    "plt.figure(figsize=(12,8))  # 创建一个新的图形对象，并设置图形的大小为宽12英寸，高8英寸。\n",
    "\n",
    "plt.plot(half_freq, half_magnitude)  # 绘制频率（half_freq）与幅度（half_magnitude）的关系图，即频谱图。\n",
    "\n",
    "plt.title(\"Spectrum\")  # 设置图形的标题为\"Spectrum\"。\n",
    "\n",
    "plt.xlabel(\"Frequency\")  # 设置x轴的标签为\"Frequency\"，表示频率轴。\n",
    "\n",
    "plt.ylabel(\"Magnitude\")  # 设置y轴的标签为\"Magnitude\"，表示幅度轴。\n",
    "\n",
    "plt.show()  # 显示图形。这将触发图形的渲染和显示，如果是在Jupyter Notebook中，图形会显示在输出中；如果是在Python脚本中运行，会弹出一个窗口显示图形。\n",
    "```\n",
    "\n",
    "这段代码的作用是将音频样本从时域转换到频域，并绘制其频谱图。频谱图显示了音频信号在不同频率上的幅度，这有助于分析音频信号的频率成分，对于声音分析和处理非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455d25c",
   "metadata": {},
   "source": [
    "##### Spectogram\n",
    "For us, as human, we sense a sound not only on a particular time by its intensity, but also by its pitch. The pitch is the frequency of the sound - higher pitch corresponding to higher frequency and vice versa. So, to have a representation which is closer to our brain, we can add another dimension - the frequency - to our representation, which is the Spectrogram.\n",
    "\n",
    "A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams.\n",
    "\n",
    "Spectrograms are used extensively in the fields of music, linguistics, sonar, radar, speech processing,seismology, and others. Spectrograms of audio can be used to identify spoken words phonetically, and to analyse the various calls of animals.it can be generated by an optical spectrometer, a bank of band-pass filters, by Fourier transform or by a wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_normal = librosa.stft(normal_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram = np.abs(stft_normal)\n",
    "log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "#plt.set_cmap(\"YlOrBr\")\n",
    "plt.title(\"Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf96e2",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# STFT -> spectrogram  # 注释：将使用短时傅里叶变换（STFT）来生成频谱图（spectrogram）。\n",
    "\n",
    "hop_length = 512  # in num. of samples  # 设置hop_length，即STFT中每次移动的样本数。\n",
    "n_fft = 2048  # window in num. of samples  # 设置n_fft，即STFT中使用的窗口大小（样本数）。\n",
    "\n",
    "# calculate duration hop length and window in seconds  # 计算hop_length和n_fft在时间上的持续时长（秒）。\n",
    "hop_length_duration = float(hop_length)/sample_rate  # 将hop_length转换为秒。\n",
    "n_fft_duration = float(n_fft)/sample_rate  # 将n_fft转换为秒。\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))  # 打印hop_length的持续时间。\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))  # 打印n_fft的持续时间。\n",
    "\n",
    "# perform stft  # 执行短时傅里叶变换。\n",
    "stft_normal = librosa.stft(normal_sound_sample, n_fft=n_fft, hop_length=hop_length)  # 对音频样本normal_sound_sample进行STFT，得到其频谱。\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude  # 计算复数的绝对值以获得幅度。\n",
    "spectrogram = np.abs(stft_normal)  # 取STFT结果的绝对值，得到频谱图的幅度。\n",
    "\n",
    "log_spectrogram = librosa.amplitude_to_db(spectrogram)  # 将幅度转换为分贝值，以获得对数频率功率谱。\n",
    "\n",
    "# display spectrogram  # 显示频谱图。\n",
    "plt.figure(figsize=(15,10))  # 创建一个新的图形对象，并设置图形的大小为宽15英寸，高10英寸。\n",
    "\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)  # 使用librosa的specshow函数显示对数频谱图。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "plt.ylabel(\"Frequency\")  # 设置y轴的标签为\"Frequency\"，表示频率轴。\n",
    "\n",
    "plt.colorbar()  # 显示颜色条，它提供了颜色映射到数值的参考。\n",
    "\n",
    "#plt.set_cmap(\"YlOrBr\")  # 注释掉的代码，用于设置颜色映射，这里使用的是YlOrBr（黄色至橙色至棕色）。\n",
    "\n",
    "plt.title(\"Spectrogram\")  # 设置图形的标题为\"Spectrogram\"。\n",
    "```\n",
    "\n",
    "这段代码的作用是通过短时傅里叶变换（STFT）生成音频样本的频谱图，并以图形的方式显示。频谱图是一种显示信号频率成分随时间变化的热图，广泛应用于声音分析和处理中。通过频谱图，可以更直观地观察音频信号在不同频率和时间上的分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51015756",
   "metadata": {},
   "source": [
    "we have an image that represents a sound. X-axis is for time, y-axis is for frequency and the color is for intensity\n",
    "\n",
    "##### MFCCs\n",
    "We can’t take the raw audio signal as input to our model because there will be a lot of noise in the audio signal. It is observed that extracting features from the audio signal and using it as input to the base model will produce much better performance than directly considering raw audio signal as input. MFCC is the widely used technique for extracting the features from the audio signal.\n",
    "\n",
    "in sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "\n",
    "Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal spectrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n",
    "\n",
    "**MFCCs are commonly derived as follows:**\n",
    "\n",
    "1- Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "\n",
    "2- Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n",
    "\n",
    "3- Take the logs of the powers at each of the mel frequencies.\n",
    "\n",
    "4- Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "\n",
    "5- The MFCCs are the amplitudes of the resulting spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs = librosa.feature.mfcc(normal_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "#plt.set_cmap(\"YlOrBr\")\n",
    "plt.title(\"MFCCs\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2bbc8",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# MFCCs  # 注释：开始进行梅尔频率倒谱系数（Mel-Frequency Cepstral Coefficients，MFCCs）的提取。\n",
    "\n",
    "# extract 25 MFCCs  # 注释：提取25个MFCCs特征。\n",
    "MFCCs = librosa.feature.mfcc(normal_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)  # 使用librosa的mfcc函数从normal_sound_sample音频样本中提取25个MFCCs特征。sample_rate是采样率，n_fft和hop_length分别是STFT的窗口大小和步长。\n",
    "\n",
    "# display MFCCs  # 注释：显示MFCCs特征。\n",
    "plt.figure(figsize=(15,10))  # 创建一个新的图形对象，并设置图形的大小为宽15英寸，高10英寸。\n",
    "\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)  # 使用librosa的specshow函数显示MFCCs特征矩阵。sr是采样率，hop_length是步长。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "\n",
    "plt.ylabel(\"MFCC coefficients\")  # 设置y轴的标签为\"MFCC coefficients\"，表示MFCC系数。\n",
    "\n",
    "plt.colorbar()  # 显示颜色条，它提供了颜色映射到数值的参考。\n",
    "\n",
    "#plt.set_cmap(\"YlOrBr\")  # 注释掉的代码，用于设置颜色映射，这里使用的是YlOrBr（黄色至橙色至棕色）。\n",
    "\n",
    "plt.title(\"MFCCs\")  # 设置图形的标题为\"MFCCs\"。\n",
    "\n",
    "# show plots  # 注释：显示绘制的图形。\n",
    "plt.show()  # 显示图形。如果是在Jupyter Notebook中，图形会显示在输出中；如果是在Python脚本中运行，会弹出一个窗口显示图形。\n",
    "```\n",
    "\n",
    "这段代码的作用是从音频样本中提取MFCCs特征，并将这些特征以热图的形式可视化。MFCCs是一种在语音和音频信号处理中广泛使用的特征，它们能够捕捉到人类对声音感知的某些重要方面。通过MFCCs热图，可以观察到不同时间段内声音信号的频谱特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204c680",
   "metadata": {},
   "source": [
    "#### 2. Murmur sound\n",
    "Heart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. They can be a symptom of many heart disorders, some serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efea1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Choose random soud from murmur folder\n",
    "random_murmur= np.random.randint(0,len(os.listdir(murmur_data))) \n",
    "murmur_sound = os.listdir(murmur_data)[random_murmur]\n",
    "murmur_sound_address = murmur_data+murmur_sound\n",
    "murmur_sound_sample,sample_rate = librosa.load(murmur_sound_address)\n",
    "ipd.Audio(murmur_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423a78e",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# Choose random sound from murmur folder  # 注释：从murmur_data目录中随机选择一个声音文件。\n",
    "\n",
    "random_murmur = np.random.randint(0, len(os.listdir(murmur_data)))  # 使用numpy的randint函数生成一个从0到murmur_data目录下文件数量（不包括目录）的随机整数，作为随机选择声音文件的索引。\n",
    "\n",
    "murmur_sound = os.listdir(murmur_data)[random_murmur]  # 使用随机索引从murmur_data目录中选择一个声音文件的名称。\n",
    "\n",
    "murmur_sound_address = murmur_data + murmur_sound  # 将选择的声音文件名称与目录路径拼接，形成完整的文件路径。\n",
    "\n",
    "murmur_sound_sample, sample_rate = librosa.load(murmur_sound_address)  # 使用librosa库的load函数加载声音文件，得到声音样本数据murmur_sound_sample和对应的采样率sample_rate。\n",
    "\n",
    "ipd.Audio(murmur_sound_sample, rate=sample_rate)  # 使用IPython.display库的Audio函数创建一个音频对象，该对象可以用于播放声音。murmur_sound_sample是音频样本数据，sample_rate是采样率。这行代码将触发音频播放。\n",
    "```\n",
    "\n",
    "这段代码的作用是从包含心脏杂音声音的目录`murmur_data`中随机选择一个音频文件，然后加载并播放这个音频样本。这对于快速检查和听取心脏杂音数据集中的音频文件内容非常有用，尤其是在进行数据探索或演示时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ec173",
   "metadata": {},
   "source": [
    "##### Waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(murmur_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4143f9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(20,5))  # 使用matplotlib的pyplot模块创建一个新的图形对象，设置图形的大小为宽20英寸和高5英寸。\n",
    "\n",
    "librosa.display.waveplot(murmur_sound_sample, sr=sample_rate)  # 使用librosa库的waveplot函数绘制音频样本murmur_sound_sample的波形图。sr=sample_rate参数指定了音频的采样率。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "\n",
    "plt.ylabel(\"Amplitude\")  # 设置y轴的标签为\"Amplitude\"，表示振幅轴。\n",
    "\n",
    "plt.show()  # 调用pyplot模块的show函数来显示图形。在Jupyter Notebook中，这将在输出区域显示图形；在Python脚本中运行时，会弹出一个窗口显示图形。\n",
    "```\n",
    "\n",
    "这段代码的作用是绘制并显示之前加载的心脏杂音音频样本`murmur_sound_sample`的波形图。波形图可以直观地展示音频信号随时间变化的振幅，对于分析和理解音频内容非常有用，尤其是在进行医学信号分析，如心脏杂音的识别和分类时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7309e",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_murmur = np.fft.fft(murmur_sound_sample)\n",
    "magnitude_murmur = np.abs(fft_murmur)\n",
    "freq_murmur = np.linspace(0,sample_rate, len(magnitude_murmur)) \n",
    "half_freq = freq_murmur[:int(len(freq_murmur)/2)]\n",
    "half_magnitude = magnitude_murmur[:int(len(freq_murmur)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c30f9",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "fft_murmur = np.fft.fft(murmur_sound_sample)  # 使用numpy的fft函数对心脏杂音音频样本murmur_sound_sample进行快速傅里叶变换（FFT），得到频域信号。\n",
    "\n",
    "magnitude_murmur = np.abs(fft_murmur)  # 计算FFT结果的绝对值，得到频域信号的幅度。\n",
    "\n",
    "freq_murmur = np.linspace(0, sample_rate, len(magnitude_murmur))  # 使用numpy的linspace函数生成一个从0到sample_rate的等间隔采样点数组，采样点的数量与音频样本的帧数相同，用于表示频率轴。\n",
    "\n",
    "half_freq = freq_murmur[:int(len(freq_murmur)/2)]  # 由于FFT结果是对称的，这里只取频率数组的前一半，即从0到Nyquist频率的部分。\n",
    "\n",
    "half_magnitude = magnitude_murmur[:int(len(magnitude_murmur)/2)]  # 同样，只取幅度数组的前一半，与half_freq对应，形成一半的频率幅度对。\n",
    "\n",
    "plt.figure(figsize=(12,8))  # 创建一个新的图形对象，并设置图形的大小为宽12英寸，高8英寸。\n",
    "\n",
    "plt.plot(half_freq, half_magnitude)  # 使用matplotlib的pyplot模块绘制频率（half_freq）和幅度（half_magnitude）之间的关系图，即一半的频谱图。\n",
    "\n",
    "plt.xlabel(\"Frequency\")  # 设置x轴的标签为\"Frequency\"，表示频率轴。\n",
    "\n",
    "plt.ylabel(\"Magnitude\")  # 设置y轴的标签为\"Magnitude\"，表示幅度轴。\n",
    "\n",
    "plt.show()  # 调用pyplot模块的show函数来显示图形。这将触发图形的渲染和显示，如果是在Jupyter Notebook中，图形会显示在输出中；如果是在Python脚本中运行，会弹出一个窗口显示图形。\n",
    "```\n",
    "\n",
    "这段代码的作用是将心脏杂音音频样本从时域转换到频域，并绘制其频谱图。频谱图显示了音频信号在不同频率上的幅度，有助于分析音频信号的频率成分，对于声音分析和处理非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df9820",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_murmur = librosa.stft(murmur_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_murmur = np.abs(stft_murmur)\n",
    "log_spectrogram_murmur = librosa.amplitude_to_db(spectrogram_murmur)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"plasma\")\n",
    "plt.title(\"Spectrogram_murmur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b0a71",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# STFT -> spectrogram  # 注释：将使用短时傅里叶变换（STFT）来生成频谱图（spectrogram）。\n",
    "\n",
    "hop_length = 512  # in num. of samples  # 设置hop_length，即STFT中每次移动的样本数。\n",
    "n_fft = 2048  # window in num. of samples  # 设置n_fft，即STFT中使用的窗口大小（样本数）。\n",
    "\n",
    "# calculate duration hop length and window in seconds  # 计算hop_length和n_fft在时间上的持续时长（秒）。\n",
    "hop_length_duration = float(hop_length)/sample_rate  # 将hop_length转换为秒。\n",
    "n_fft_duration = float(n_fft)/sample_rate  # 将n_fft转换为秒。\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))  # 打印hop_length的持续时间。\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))  # 打印n_fft的持续时间。\n",
    "\n",
    "# perform stft  # 执行短时傅里叶变换。\n",
    "stft_murmur = librosa.stft(murmur_sound_sample, n_fft=n_fft, hop_length=hop_length)  # 对音频样本murmur_sound_sample进行STFT，得到其频谱。\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude  # 计算复数的绝对值以获得幅度。\n",
    "spectrogram_murmur = np.abs(stft_murmur)  # 取STFT结果的绝对值，得到频谱图的幅度。\n",
    "\n",
    "log_spectrogram_murmur = librosa.amplitude_to_db(spectrogram_murmur)  # 将幅度转换为分贝值，以获得对数频率功率谱。\n",
    "\n",
    "# display spectrogram  # 显示频谱图。\n",
    "plt.figure(figsize=(15,10))  # 创建一个新的图形对象，并设置图形的大小为宽15英寸，高10英寸。\n",
    "\n",
    "librosa.display.specshow(log_spectrogram_murmur, sr=sample_rate, hop_length=hop_length)  # 使用librosa的specshow函数显示对数频谱图。注意这里应该是log_spectrogram_murmur而不是log_spectrogram。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "\n",
    "plt.ylabel(\"Frequency\")  # 设置y轴的标签为\"Frequency\"，表示频率轴。\n",
    "\n",
    "plt.colorbar()  # 显示颜色条，它提供了颜色映射到数值的参考。\n",
    "\n",
    "plt.set_cmap(\"plasma\")  # 设置颜色映射为\"plasma\"，这是一种从低到高亮度的颜色渐变。\n",
    "\n",
    "plt.title(\"Spectrogram_murmur\")  # 设置图形的标题为\"Spectrogram_murmur\"。\n",
    "\n",
    "# 注意：在调用specshow函数时，应使用log_spectrogram_murmur而不是log_spectrogram，否则会显示之前定义的正常声音的频谱图。\n",
    "```\n",
    "\n",
    "这段代码的作用是通过短时傅里叶变换（STFT）生成心脏杂音音频样本的频谱图，并以图形的方式显示。频谱图是一种显示信号频率成分随时间变化的热图，广泛应用于声音分析和处理中。通过频谱图，可以更直观地观察音频信号在不同频率和时间上的分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301baf5",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_murmur = librosa.feature.mfcc(murmur_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"plasma\")\n",
    "plt.title(\"MFCCs\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634336e",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# MFCCs  # 注释：开始进行梅尔频率倒谱系数（Mel-Frequency Cepstral Coefficients，MFCCs）的提取。\n",
    "\n",
    "# extract 25 MFCCs  # 注释：提取25个MFCCs特征。\n",
    "MFCCs_murmur = librosa.feature.mfcc(murmur_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)  # 使用librosa的mfcc函数从murmur_sound_sample音频样本中提取25个MFCCs特征。sample_rate是采样率，n_fft和hop_length分别是STFT的窗口大小和步长。\n",
    "\n",
    "# display MFCCs  # 注释：显示MFCCs特征。\n",
    "plt.figure(figsize=(15,10))  # 创建一个新的图形对象，并设置图形的大小为宽15英寸，高10英寸。\n",
    "\n",
    "librosa.display.specshow(MFCCs_murmur, sr=sample_rate, hop_length=hop_length)  # 使用librosa的specshow函数显示MFCCs特征矩阵。注意这里应使用MFCCs_murmur而不是MFCCs，以显示当前正在处理的心脏杂音的MFCCs。\n",
    "\n",
    "plt.xlabel(\"Time\")  # 设置x轴的标签为\"Time\"，表示时间轴。\n",
    "\n",
    "plt.ylabel(\"MFCC coefficients\")  # 设置y轴的标签为\"MFCC coefficients\"，表示MFCC系数。\n",
    "\n",
    "plt.colorbar()  # 显示颜色条，它提供了颜色映射到数值的参考。\n",
    "\n",
    "plt.set_cmap(\"plasma\")  # 设置颜色映射为\"plasma\"，这是一种颜色渐变方案，用于增强图形的视觉对比度。\n",
    "\n",
    "plt.title(\"MFCCs_murmur\")  # 设置图形的标题为\"MFCCs_murmur\"，更准确地反映正在显示的是心脏杂音的MFCCs。\n",
    "\n",
    "# show plots  # 注释：显示绘制的图形。\n",
    "plt.show()  # 调用pyplot模块的show函数来显示图形。如果是在Jupyter Notebook中，图形会显示在输出中；如果是在Python脚本中运行，会弹出一个窗口显示图形。\n",
    "```\n",
    "\n",
    "请注意，在显示MFCCs时，代码中有一个错误：`specshow(MFCCs, ...)` 应该是 `specshow(MFCCs_murmur, ...)`，以确保显示的是心脏杂音的MFCCs而不是先前计算的正常声音的MFCCs。同样，标题也应该是 \"MFCCs_murmur\" 以匹配显示的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d8400",
   "metadata": {},
   "source": [
    "#### 3. Extrastole sound\n",
    "• Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from extrastole folder\n",
    "random_extrastole= np.random.randint(0,len(os.listdir(extrastole_data))) \n",
    "extrastole_sound = os.listdir(extrastole_data)[random_extrastole]\n",
    "extrastole_sound_address = extrastole_data+extrastole_sound\n",
    "extrastole_sound_sample,sample_rate = librosa.load(extrastole_sound_address)\n",
    "ipd.Audio(extrastole_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03816d18",
   "metadata": {},
   "source": [
    "中文注释：\n",
    "\n",
    "```python\n",
    "# Choose random sound from extrastole folder  # 注释：从extrastole_data目录中随机选择一个声音文件。\n",
    "\n",
    "random_extrastole = np.random.randint(0, len(os.listdir(extrastole_data)))  # 使用numpy的randint函数生成一个从0到extrastole_data目录下文件数量（不包括目录）的随机整数，作为随机选择声音文件的索引。\n",
    "\n",
    "extrastole_sound = os.listdir(extrastole_data)[random_extrastole]  # 使用随机索引从extrastole_data目录中选择一个声音文件的名称。\n",
    "\n",
    "extrastole_sound_address = extrastole_data + extrastole_sound  # 将选择的声音文件名称与目录路径拼接，形成完整的文件路径。\n",
    "\n",
    "extrastole_sound_sample, sample_rate = librosa.load(extrastole_sound_address)  # 使用librosa库的load函数加载声音文件，得到声音样本数据extrastole_sound_sample和对应的采样率sample_rate。\n",
    "\n",
    "ipd.Audio(extrastole_sound_sample, rate=sample_rate)  # 使用IPython.display库的Audio函数创建一个音频对象，该对象可以用于播放声音。extrastole_sound_sample是音频样本数据，sample_rate是采样率。这行代码将触发音频播放。\n",
    "```\n",
    "\n",
    "这段代码的作用是从包含额外心动过速（extrastole）声音的目录`extrastole_data`中随机选择一个音频文件，然后加载并播放这个音频样本。这对于快速检查和听取心动过速数据集中的音频文件内容非常有用，尤其是在进行数据探索或演示时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358063ed",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69834df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(extrastole_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9de02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b47d1a",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_extrastole = np.fft.fft(extrastole_sound_sample)\n",
    "magnitude_extrastole = np.abs(fft_extrastole)\n",
    "freq_extrastole = np.linspace(0,sample_rate, len(magnitude_extrastole)) \n",
    "half_freq = freq_extrastole[:int(len(freq_extrastole)/2)]\n",
    "half_magnitude = magnitude_extrastole[:int(len(freq_extrastole)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bc509",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ea5753",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_extrastole = librosa.stft(extrastole_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_extrastole = np.abs(stft_extrastole)\n",
    "log_spectrogram_extrastole = librosa.amplitude_to_db(spectrogram_extrastole)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"cividis\")\n",
    "plt.title(\"Spectrogram_extrastole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd0e75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c724b3f",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_extrastole = librosa.feature.mfcc(extrastole_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"cividis\")\n",
    "plt.title(\"MFCCs_extrastole\")\n",
    "\n",
    "# show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b025d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dff4e50",
   "metadata": {},
   "source": [
    "#### 4. Artifact sound\n",
    "• In the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7729c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from artifact folder\n",
    "random_artifact= np.random.randint(0,len(os.listdir(artifact_data))) \n",
    "artifact_sound = os.listdir(artifact_data)[random_artifact]\n",
    "artifact_sound_address = artifact_data+artifact_sound\n",
    "artifact_sound_sample,sample_rate = librosa.load(artifact_sound_address)\n",
    "ipd.Audio(artifact_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14aac7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae654c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_sound_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6354ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9debf3d",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(artifact_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55a9ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249423b8",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_artifact = np.fft.fft(artifact_sound_sample)\n",
    "magnitude_artifact = np.abs(fft_artifact)\n",
    "freq_artifact = np.linspace(0,sample_rate, len(magnitude_artifact)) \n",
    "half_freq = freq_artifact[:int(len(freq_artifact)/2)]\n",
    "half_magnitude = magnitude_artifact[:int(len(freq_artifact)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e3f73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57952c7d",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88becce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_artifact = librosa.stft(artifact_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_artifact = np.abs(stft_artifact)\n",
    "log_spectrogram_artifact = librosa.amplitude_to_db(spectrogram_artifact)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"magma\")\n",
    "plt.title(\"Spectrogram_artifacte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39aa05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a614df6",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_artifact = librosa.feature.mfcc(artifact_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"magma\")\n",
    "plt.title(\"MFCCs_artifact\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38790a78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da5fa24",
   "metadata": {},
   "source": [
    "#### 5. Extrahls sound\n",
    "Extra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from extrahls folder\n",
    "random_extrahls= np.random.randint(0,len(os.listdir(extrahls_data))) \n",
    "extrahls_sound = os.listdir(extrahls_data)[random_extrahls]\n",
    "extrahls_sound_address = extrahls_data+extrahls_sound\n",
    "extrahls_sound_sample,sample_rate = librosa.load(extrahls_sound_address)\n",
    "ipd.Audio(extrahls_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db709a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acafae94",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(extrahls_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871d3df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb6b809",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188985a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_extrahls = np.fft.fft(extrahls_sound_sample)\n",
    "magnitude_extrahls = np.abs(fft_extrahls)\n",
    "freq_extrahls = np.linspace(0,sample_rate, len(magnitude_extrahls)) \n",
    "half_freq = freq_extrahls[:int(len(freq_extrahls)/2)]\n",
    "half_magnitude = magnitude_extrahls[:int(len(freq_extrahls)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33595305",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd7842f",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc78fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_extrahls = librosa.stft(extrahls_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_extrahls = np.abs(stft_extrahls)\n",
    "log_spectrogram_extrahls = librosa.amplitude_to_db(spectrogram_extrahls)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"inferno\")\n",
    "plt.title(\"Spectrogram_extrahlse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70727b8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb3dce74",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_extrahls = librosa.feature.mfcc(extrahls_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"inferno\")\n",
    "plt.title(\"MFCCs_extrahls\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5478f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a31c23",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Follwing function loop on every audio file and extract the mfcc features and the output is the a numpy array contain these mfcc's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6228112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_data (folder, file_names, duration=10, sr=22050):\n",
    "    input_length=sr*duration\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            sound_file=folder+file_name\n",
    "            print (\"load file \",sound_file)\n",
    "            X, sr = librosa.load( sound_file, sr=sr, duration=duration) \n",
    "            dur = librosa.get_duration(y=X, sr=sr)\n",
    "            # pad audio file same duration\n",
    "            if (round(dur) < duration):\n",
    "                print (\"fixing audio lenght :\", file_name)\n",
    "                y = librosa.util.fix_length(X, input_length)                \n",
    "            # extract normalized mfcc feature from data\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=25).T,axis=0)             \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file)        \n",
    "        feature = np.array(mfccs).reshape([-1,1])\n",
    "        data.append(feature)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd853a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc48689",
   "metadata": {},
   "source": [
    "## Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e21860",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple encoding of categories, convert to only 3 types:\n",
    "# Normal (Include extrahls and extrastole)\n",
    "# Murmur\n",
    "# Artifact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Map label text to integer\n",
    "CLASSES = ['artifact','murmur','normal']\n",
    "NB_CLASSES=len(CLASSES)\n",
    "\n",
    "# Map integer value to text labels\n",
    "label_to_int = {k:v for v,k in enumerate(CLASSES)}\n",
    "print (label_to_int)\n",
    "print (\" \")\n",
    "int_to_label = {v:k for k,v in label_to_int.items()}\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6a482",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 KHz\n",
    "SAMPLE_RATE = 22050\n",
    "# seconds\n",
    "MAX_SOUND_CLIP_DURATION=10\n",
    "\n",
    "artifact_files = fnmatch.filter(os.listdir(artifact_data), 'artifact*.wav')\n",
    "artifact_sounds = load_file_data (folder=artifact_data, file_names = artifact_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "artifact_labels = [0 for items in artifact_files]\n",
    "\n",
    "normal_files = fnmatch.filter(os.listdir(normal_data), 'normal*.wav')\n",
    "normal_sounds = load_file_data(folder=normal_data,file_names=normal_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "normal_labels = [2 for items in normal_sounds]\n",
    "\n",
    "extrahls_files = fnmatch.filter(os.listdir(extrahls_data), 'extrahls*.wav')\n",
    "extrahls_sounds = load_file_data(folder=extrahls_data,file_names=extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrahls_labels = [2 for items in extrahls_sounds]\n",
    "\n",
    "murmur_files = fnmatch.filter(os.listdir(murmur_data), 'murmur*.wav')\n",
    "murmur_sounds = load_file_data(folder=murmur_data,file_names=murmur_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "murmur_labels = [1 for items in murmur_files]\n",
    "\n",
    "\n",
    "extrastole_files = fnmatch.filter(os.listdir(extrastole_data), 'extrastole*.wav')\n",
    "extrastole_sounds = load_file_data(folder=extrastole_data,file_names=extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrastole_labels = [2 for items in extrastole_files]\n",
    "\n",
    "print (\"Loading Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e08a51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43052d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabel_datala files\n",
    "Bunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Bunlabelledtest*.wav')\n",
    "Bunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Bunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Bunlabelledtest_labels = [-1 for items in Bunlabelledtest_sounds]\n",
    "\n",
    "Aunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Aunlabelledtest*.wav')\n",
    "Aunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Aunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Aunlabelledtest_labels = [-1 for items in Aunlabelledtest_sounds]\n",
    "\n",
    "\n",
    "print (\"Loading of unlabel data done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb9b20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48de69e",
   "metadata": {},
   "source": [
    "### concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f404af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine set-a and set-b \n",
    "x_data = np.concatenate((artifact_sounds, normal_sounds,extrahls_sounds,murmur_sounds,extrastole_sounds))\n",
    "\n",
    "y_data = np.concatenate((artifact_labels, normal_labels,extrahls_labels,murmur_labels,extrastole_labels))\n",
    "\n",
    "test_x = np.concatenate((Aunlabelledtest_sounds,Bunlabelledtest_sounds))\n",
    "test_y = np.concatenate((Aunlabelledtest_labels,Bunlabelledtest_labels))\n",
    "\n",
    "print (\"combined training data record: \",len(y_data), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ea847",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac82078",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2a6b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61596d61",
   "metadata": {},
   "source": [
    "### train_test_validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133edf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle - whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "# split data into Train, Validation and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.8, random_state=42, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "y_train = np.array(tf.keras.utils.to_categorical(y_train, len(CLASSES)))\n",
    "y_test = np.array(tf.keras.utils.to_categorical(y_test, len(CLASSES)))\n",
    "y_val = np.array(tf.keras.utils.to_categorical(y_val, len(CLASSES)))\n",
    "test_y=np.array(tf.keras.utils.to_categorical(test_y, len(CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0164c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5f81f2",
   "metadata": {},
   "source": [
    "### Correct imbalnced data using class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_COUNT = 578\n",
    "COUNT_0 = 40  #artifact\n",
    "COUNT_1 = 129 #murmur\n",
    "COUNT_2 = 409 #normal\n",
    "weight_for_0 = TRAIN_IMG_COUNT / (3 * COUNT_0)\n",
    "weight_for_1 = TRAIN_IMG_COUNT / (3 * COUNT_1)\n",
    "weight_for_2 = TRAIN_IMG_COUNT / (3 * COUNT_2)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80593ffd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ec3347",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau, ModelCheckpoint,TensorBoard,ProgbarLogger\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78d8dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0742e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.05, recurrent_dropout=0.20, return_sequences=True), input_shape = (25,1)))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-4), metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef9011",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_saver = ModelCheckpoint('set_a_weights.h5', monitor='val_loss', \n",
    "                               save_best_only=True, save_weights_only=True)\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91522f3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4963d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cac80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train, \n",
    "                  batch_size=3, \n",
    "                  epochs=30,\n",
    "                  class_weight=class_weight,\n",
    "                  callbacks=[weight_saver, annealer],\n",
    "                  validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219f53a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece957b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55833851",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c7e19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ead5a0",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_pred = model.predict(x_test, batch_size=5)\n",
    "#check scores\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print (\"Model evaluation accuracy: \", round(scores[1]*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b2fd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d05e0d",
   "metadata": {},
   "source": [
    "## Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81716a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('heart_sounds.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb68f2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction class \n",
    "y_pred = np.asarray(model.predict(x_test, batch_size=32))\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "print (\"prediction test return :\",y_pred[1], \"-\", int_to_label[y_pred[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fb2bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing (file_path, duration=10, sr=22050):\n",
    "  input_length=sr*duration\n",
    "  process_file=[]\n",
    "  X, sr = librosa.load(file_path, sr=sr, duration=duration) \n",
    "  dur = librosa.get_duration(y=X, sr=sr)\n",
    "  # pad audio file same duration\n",
    "  if (round(dur) < duration):\n",
    "    y = librosa.util.fix_length(X, input_length)                \n",
    "  mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40, n_fft=512,hop_length=2048).T,axis=0)\n",
    "  feature = np.array(mfccs).reshape([-1,1])\n",
    "  process_file.append(feature)\n",
    "  process_file_array = np.asarray(process_file)\n",
    "  return process_file_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7631b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_test, batch_size=32))\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c54cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda97d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"artifact\", \"murmur\",\"normal\"]\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506d85f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae6ffcd",
   "metadata": {},
   "source": [
    "## Treatment options for abnormal heartbeat sounds:\n",
    "\n",
    "he treatment depends on the cause. You may need to make lifestyle changes, like increasing your activity level or changing your diet (for example, limiting your caffeine intake). If you smoke, your doctor will help you stop smoking. You might also require medication to control your abnormal heartbeat, as well as any secondary symptoms.\n",
    "\n",
    "For serious abnormalities that don’t go away with behavioral changes or medication, your doctor can recommend:\n",
    "\n",
    "1- cardiac catheterization to diagnose a heart problem\n",
    "\n",
    "2- catheter ablation to destroy tissue that causes abnormal rhythms\n",
    "\n",
    "3- cardioversion by medication or an electrical shock to the heart\n",
    "\n",
    "4- implantation of a pacemaker or cardioverter defibrillator\n",
    "\n",
    "5- surgery to correct an abnormality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f2e46",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "Breath sounds, https://medlineplus.gov/ency/article/007535.htm\n",
    "\n",
    "Classifying Heart Sounds Challenge, http://www.peterjbentley.com/heartchallenge/#aboutdata\n",
    "\n",
    "Audio Deep Learning Made Simple: Sound Classification, Step-by-Step, https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n",
    "\n",
    "Heart Sounds Topic Review, https://www.healio.com/cardiology/learn-the-heart/cardiology-review/topic-reviews/heart-sounds\n",
    "\n",
    "Deep Learning (for Audio) with Python course, https://youtube.com/playlist?list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da541f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d0ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
