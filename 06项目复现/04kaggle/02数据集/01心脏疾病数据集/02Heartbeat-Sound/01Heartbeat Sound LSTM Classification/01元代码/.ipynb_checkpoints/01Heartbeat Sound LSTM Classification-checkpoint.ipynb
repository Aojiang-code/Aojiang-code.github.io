{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e75e0c2",
   "metadata": {},
   "source": [
    "**01Heartbeat Sound LSTM Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237e251",
   "metadata": {},
   "source": [
    "## Heartbeat sounds\n",
    "\n",
    "Heart sounds are produced from a specific cardiac event such as closure of a valve or tensing of a chordae tendineae.\n",
    "\n",
    "• S1 result from the closing of the mitral and tricuspid valves.\n",
    "\n",
    "• S2 produced by the closure of the aortic and pulmonic valves.\n",
    "\n",
    "In medicine we call the ‘lub’ sound \"S1\" and the ‘dub’ sound \"S2\".\n",
    "\n",
    "You can learn short intro about heart sounds from this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=dBwr2GZCmQM\n",
    "\n",
    "Why using Deep Learning in this ?\n",
    "\n",
    "It is a subset of Neural Networks ( a computing systems which is insprired by biological neural networks ). And by stacking a lot of Neural Networks layers, we will get a Deep Learning model which might contains multiple billions parameters and is capable of handle very complicated problems that no classical algorithm can be effective.\n",
    "\n",
    "Deep Learning is especially powerful with unstructrured data (Image, Text, Sound, Video, …).\n",
    "\n",
    "## About data:\n",
    "Is challenge published in 2012 to classify the heart sound to some categories from ‘AISTATS’ . Data has been gathered from two sources (A) and (B).\n",
    "\n",
    "A) from the general public via the iStethoscope Pro.\n",
    "\n",
    "B) from a clinic trial in hospitals using the digital stethoscope DigiScope.\n",
    "\n",
    "Before we work on this nootebook we handle the data folders and conctante both sources(A&B) to be easy to deal with it.\n",
    "\n",
    "Original Data here: http://www.peterjbentley.com/heartchallenge/\n",
    "\n",
    "## Import main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa #To deal with Audio files\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/heartbeat-sound/Heartbeat_Sound/\"\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f98f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarin_data      = data_path \n",
    "unlabel_data        = data_path  + \"/unlabel/\"\n",
    "\n",
    "normal_data     = tarin_data + '/normal/'\n",
    "murmur_data     = tarin_data + '/murmur/'\n",
    "extrastole_data = tarin_data + '/extrastole/'\n",
    "artifact_data   = tarin_data + '/artifact/'\n",
    "extrahls_data   = tarin_data + \"/extrahls/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc50976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal files:\", len(os.listdir(normal_data))) #length of normal training sounds\n",
    "print(\"Murmur files:\",len(os.listdir(murmur_data))) #length of murmur training sounds \n",
    "print(\"Extrastole files\", len(os.listdir(extrastole_data))) #length of extrastole training sounds \n",
    "print(\"Artifact files:\",len(os.listdir(artifact_data))) #length of artifact training sounds \n",
    "print(\"Extrahls files:\",len(os.listdir(extrahls_data))) #length of extrahls training sounds \n",
    "\n",
    "print('TOTAL TRAIN SOUNDS:', len(os.listdir(normal_data)) \n",
    "                              + len(os.listdir(murmur_data))\n",
    "                              + len(os.listdir(extrastole_data))\n",
    "                              + len(os.listdir(artifact_data))\n",
    "                              + len(os.listdir(extrahls_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test sounds: \", len(os.listdir(unlabel_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b70fb",
   "metadata": {},
   "source": [
    "## EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58877106",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([len(os.listdir(normal_data)),\n",
    "              len(os.listdir(murmur_data)),\n",
    "              len(os.listdir(extrastole_data)),\n",
    "              len(os.listdir(artifact_data)),\n",
    "              len(os.listdir(extrahls_data))])\n",
    "labels = ['normal', 'murmur', 'extrastole', 'artifact', 'extrahls']\n",
    "plt.pie(x, labels = labels, autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad9b37",
   "metadata": {},
   "source": [
    "### Visualizing random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_random_sample(folder_name):\n",
    "  #to hear the audio sample\n",
    "  random_sample             = np.random.randint(0,len(os.listdir(folder_name)))\n",
    "  sample_sound              = os.listdir(folder_name)[random_sample]\n",
    "  sample_address            = folder_name + sample_sound\n",
    "  sample_sound, sample_rate = librosa.load(sample_address)\n",
    "  sample_audio              = ipd.Audio(sample_sound, rate=sample_rate)\n",
    "  return sample_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72529e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize_random_sample(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609aef0c",
   "metadata": {},
   "source": [
    "#### 1. Normal sound\n",
    "Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cf362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from normal folder\n",
    "random_normal= np.random.randint(0,len(os.listdir(normal_data))) \n",
    "normal_sound = os.listdir(normal_data)[random_normal]\n",
    "normal_sound_address = normal_data+normal_sound\n",
    "normal_sound_sample,sample_rate = librosa.load(normal_sound_address)\n",
    "ipd.Audio(normal_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d14165",
   "metadata": {},
   "source": [
    "##### Waveform\n",
    "Sound is the pressure of air propagates to our ear. Digital audio file is gotten from a sound sensor that can detects sound waves and converting it to electrical signals.\n",
    "\n",
    "Specifically, it's telling us about the wave's displacement, and how it changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(normal_sound_sample, sr = sample_rate)\n",
    "plt.title(\"Normal Sound\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c46d0d",
   "metadata": {},
   "source": [
    "X axis, represents time. Y-axis measures displacement of air molecules.This is where amplitude comes in. It measures how much a molecule is displaced from its resting position.\n",
    "\n",
    "##### Spectrum\n",
    "A sound spectrum is a representation of a sound – usually a short sample of a sound – in terms of the amount of vibration at each individual frequency. It is usually presented as a graph of either power or pressure as a function of frequency. The power or pressure is usually measured in decibels and the frequency is measured in vibrations per second (or hertz, abbreviation Hz) or thousands of vibrations per second (kilohertz, abbreviation kHz).\n",
    "\n",
    "The spectrum expresses the frequency composition of the sound and is obtained by analyzing the sound. A sound spectrum is usually represented in a coordinate plane where the frequency f is plotted along the axis of abscissas and the amplitude A, or intensity, of a harmonic component with a given frequency is plotted along the axis of ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f21296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_normal = np.fft.fft(normal_sound_sample)\n",
    "magnitude_normal = np.abs(fft_normal)\n",
    "freq_normal = np.linspace(0,sample_rate, len(magnitude_normal)) \n",
    "half_freq = freq_normal[:int(len(freq_normal)/2)]\n",
    "half_magnitude = magnitude_normal[:int(len(freq_normal)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.title(\"Spectrum\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455d25c",
   "metadata": {},
   "source": [
    "##### Spectogram\n",
    "For us, as human, we sense a sound not only on a particular time by its intensity, but also by its pitch. The pitch is the frequency of the sound - higher pitch corresponding to higher frequency and vice versa. So, to have a representation which is closer to our brain, we can add another dimension - the frequency - to our representation, which is the Spectrogram.\n",
    "\n",
    "A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams.\n",
    "\n",
    "Spectrograms are used extensively in the fields of music, linguistics, sonar, radar, speech processing,seismology, and others. Spectrograms of audio can be used to identify spoken words phonetically, and to analyse the various calls of animals.it can be generated by an optical spectrometer, a bank of band-pass filters, by Fourier transform or by a wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_normal = librosa.stft(normal_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram = np.abs(stft_normal)\n",
    "log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "#plt.set_cmap(\"YlOrBr\")\n",
    "plt.title(\"Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51015756",
   "metadata": {},
   "source": [
    "we have an image that represents a sound. X-axis is for time, y-axis is for frequency and the color is for intensity\n",
    "\n",
    "##### MFCCs\n",
    "We can’t take the raw audio signal as input to our model because there will be a lot of noise in the audio signal. It is observed that extracting features from the audio signal and using it as input to the base model will produce much better performance than directly considering raw audio signal as input. MFCC is the widely used technique for extracting the features from the audio signal.\n",
    "\n",
    "in sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "\n",
    "Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal spectrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n",
    "\n",
    "**MFCCs are commonly derived as follows:**\n",
    "\n",
    "1- Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "\n",
    "2- Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n",
    "\n",
    "3- Take the logs of the powers at each of the mel frequencies.\n",
    "\n",
    "4- Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "\n",
    "5- The MFCCs are the amplitudes of the resulting spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs = librosa.feature.mfcc(normal_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "#plt.set_cmap(\"YlOrBr\")\n",
    "plt.title(\"MFCCs\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204c680",
   "metadata": {},
   "source": [
    "#### 2. Murmur sound\n",
    "Heart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. They can be a symptom of many heart disorders, some serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efea1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Choose random soud from murmur folder\n",
    "random_murmur= np.random.randint(0,len(os.listdir(murmur_data))) \n",
    "murmur_sound = os.listdir(murmur_data)[random_murmur]\n",
    "murmur_sound_address = murmur_data+murmur_sound\n",
    "murmur_sound_sample,sample_rate = librosa.load(murmur_sound_address)\n",
    "ipd.Audio(murmur_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ec173",
   "metadata": {},
   "source": [
    "##### Waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(murmur_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7309e",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_murmur = np.fft.fft(murmur_sound_sample)\n",
    "magnitude_murmur = np.abs(fft_murmur)\n",
    "freq_murmur = np.linspace(0,sample_rate, len(magnitude_murmur)) \n",
    "half_freq = freq_murmur[:int(len(freq_murmur)/2)]\n",
    "half_magnitude = magnitude_murmur[:int(len(freq_murmur)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df9820",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_murmur = librosa.stft(murmur_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_murmur = np.abs(stft_murmur)\n",
    "log_spectrogram_murmur = librosa.amplitude_to_db(spectrogram_murmur)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"plasma\")\n",
    "plt.title(\"Spectrogram_murmur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301baf5",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_murmur = librosa.feature.mfcc(murmur_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"plasma\")\n",
    "plt.title(\"MFCCs\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d8400",
   "metadata": {},
   "source": [
    "#### 3. Extrastole sound\n",
    "• Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from extrastole folder\n",
    "random_extrastole= np.random.randint(0,len(os.listdir(extrastole_data))) \n",
    "extrastole_sound = os.listdir(extrastole_data)[random_extrastole]\n",
    "extrastole_sound_address = extrastole_data+extrastole_sound\n",
    "extrastole_sound_sample,sample_rate = librosa.load(extrastole_sound_address)\n",
    "ipd.Audio(extrastole_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358063ed",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69834df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(extrastole_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b47d1a",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_extrastole = np.fft.fft(extrastole_sound_sample)\n",
    "magnitude_extrastole = np.abs(fft_extrastole)\n",
    "freq_extrastole = np.linspace(0,sample_rate, len(magnitude_extrastole)) \n",
    "half_freq = freq_extrastole[:int(len(freq_extrastole)/2)]\n",
    "half_magnitude = magnitude_extrastole[:int(len(freq_extrastole)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea5753",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_extrastole = librosa.stft(extrastole_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_extrastole = np.abs(stft_extrastole)\n",
    "log_spectrogram_extrastole = librosa.amplitude_to_db(spectrogram_extrastole)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"cividis\")\n",
    "plt.title(\"Spectrogram_extrastole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c724b3f",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_extrastole = librosa.feature.mfcc(extrastole_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"cividis\")\n",
    "plt.title(\"MFCCs_extrastole\")\n",
    "\n",
    "# show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff4e50",
   "metadata": {},
   "source": [
    "#### 4. Artifact sound\n",
    "• In the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7729c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from artifact folder\n",
    "random_artifact= np.random.randint(0,len(os.listdir(artifact_data))) \n",
    "artifact_sound = os.listdir(artifact_data)[random_artifact]\n",
    "artifact_sound_address = artifact_data+artifact_sound\n",
    "artifact_sound_sample,sample_rate = librosa.load(artifact_sound_address)\n",
    "ipd.Audio(artifact_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae654c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_sound_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9debf3d",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(artifact_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249423b8",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_artifact = np.fft.fft(artifact_sound_sample)\n",
    "magnitude_artifact = np.abs(fft_artifact)\n",
    "freq_artifact = np.linspace(0,sample_rate, len(magnitude_artifact)) \n",
    "half_freq = freq_artifact[:int(len(freq_artifact)/2)]\n",
    "half_magnitude = magnitude_artifact[:int(len(freq_artifact)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57952c7d",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88becce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_artifact = librosa.stft(artifact_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_artifact = np.abs(stft_artifact)\n",
    "log_spectrogram_artifact = librosa.amplitude_to_db(spectrogram_artifact)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"magma\")\n",
    "plt.title(\"Spectrogram_artifacte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a614df6",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_artifact = librosa.feature.mfcc(artifact_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"magma\")\n",
    "plt.title(\"MFCCs_artifact\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5fa24",
   "metadata": {},
   "source": [
    "#### 5. Extrahls sound\n",
    "Extra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random soud from extrahls folder\n",
    "random_extrahls= np.random.randint(0,len(os.listdir(extrahls_data))) \n",
    "extrahls_sound = os.listdir(extrahls_data)[random_extrahls]\n",
    "extrahls_sound_address = extrahls_data+extrahls_sound\n",
    "extrahls_sound_sample,sample_rate = librosa.load(extrahls_sound_address)\n",
    "ipd.Audio(extrahls_sound_sample,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafae94",
   "metadata": {},
   "source": [
    "##### Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "librosa.display.waveplot(extrahls_sound_sample, sr = sample_rate)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6b809",
   "metadata": {},
   "source": [
    "##### Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188985a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_extrahls = np.fft.fft(extrahls_sound_sample)\n",
    "magnitude_extrahls = np.abs(fft_extrahls)\n",
    "freq_extrahls = np.linspace(0,sample_rate, len(magnitude_extrahls)) \n",
    "half_freq = freq_extrahls[:int(len(freq_extrahls)/2)]\n",
    "half_magnitude = magnitude_extrahls[:int(len(freq_extrahls)/2)]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(half_freq,half_magnitude)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7842f",
   "metadata": {},
   "source": [
    "##### Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc78fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT -> spectrogram\n",
    "hop_length = 512 # in num. of samples\n",
    "n_fft = 2048 # window in num. of samples\n",
    "\n",
    "# calculate duration hop length and window in seconds\n",
    "hop_length_duration = float(hop_length)/sample_rate\n",
    "n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "# perform stft\n",
    "stft_extrahls = librosa.stft(extrahls_sound_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# calculate abs values on complex numbers to get magnitude\n",
    "spectrogram_extrahls = np.abs(stft_extrahls)\n",
    "log_spectrogram_extrahls = librosa.amplitude_to_db(spectrogram_extrahls)\n",
    "\n",
    "# display spectrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"inferno\")\n",
    "plt.title(\"Spectrogram_extrahlse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3dce74",
   "metadata": {},
   "source": [
    "##### MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 25 MFCCs\n",
    "MFCCs_extrahls = librosa.feature.mfcc(extrahls_sound_sample, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=25)\n",
    "\n",
    "# display MFCCs\n",
    "plt.figure(figsize=(15,10))\n",
    "librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC coefficients\")\n",
    "plt.colorbar()\n",
    "plt.set_cmap(\"inferno\")\n",
    "plt.title(\"MFCCs_extrahls\")\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a31c23",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Follwing function loop on every audio file and extract the mfcc features and the output is the a numpy array contain these mfcc's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6228112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_data (folder, file_names, duration=10, sr=22050):\n",
    "    input_length=sr*duration\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            sound_file=folder+file_name\n",
    "            print (\"load file \",sound_file)\n",
    "            X, sr = librosa.load( sound_file, sr=sr, duration=duration) \n",
    "            dur = librosa.get_duration(y=X, sr=sr)\n",
    "            # pad audio file same duration\n",
    "            if (round(dur) < duration):\n",
    "                print (\"fixing audio lenght :\", file_name)\n",
    "                y = librosa.util.fix_length(X, input_length)                \n",
    "            # extract normalized mfcc feature from data\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=25).T,axis=0)             \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file)        \n",
    "        feature = np.array(mfccs).reshape([-1,1])\n",
    "        data.append(feature)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc48689",
   "metadata": {},
   "source": [
    "## Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e21860",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple encoding of categories, convert to only 3 types:\n",
    "# Normal (Include extrahls and extrastole)\n",
    "# Murmur\n",
    "# Artifact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Map label text to integer\n",
    "CLASSES = ['artifact','murmur','normal']\n",
    "NB_CLASSES=len(CLASSES)\n",
    "\n",
    "# Map integer value to text labels\n",
    "label_to_int = {k:v for v,k in enumerate(CLASSES)}\n",
    "print (label_to_int)\n",
    "print (\" \")\n",
    "int_to_label = {v:k for k,v in label_to_int.items()}\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 KHz\n",
    "SAMPLE_RATE = 22050\n",
    "# seconds\n",
    "MAX_SOUND_CLIP_DURATION=10\n",
    "\n",
    "artifact_files = fnmatch.filter(os.listdir(artifact_data), 'artifact*.wav')\n",
    "artifact_sounds = load_file_data (folder=artifact_data, file_names = artifact_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "artifact_labels = [0 for items in artifact_files]\n",
    "\n",
    "normal_files = fnmatch.filter(os.listdir(normal_data), 'normal*.wav')\n",
    "normal_sounds = load_file_data(folder=normal_data,file_names=normal_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "normal_labels = [2 for items in normal_sounds]\n",
    "\n",
    "extrahls_files = fnmatch.filter(os.listdir(extrahls_data), 'extrahls*.wav')\n",
    "extrahls_sounds = load_file_data(folder=extrahls_data,file_names=extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrahls_labels = [2 for items in extrahls_sounds]\n",
    "\n",
    "murmur_files = fnmatch.filter(os.listdir(murmur_data), 'murmur*.wav')\n",
    "murmur_sounds = load_file_data(folder=murmur_data,file_names=murmur_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "murmur_labels = [1 for items in murmur_files]\n",
    "\n",
    "\n",
    "extrastole_files = fnmatch.filter(os.listdir(extrastole_data), 'extrastole*.wav')\n",
    "extrastole_sounds = load_file_data(folder=extrastole_data,file_names=extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrastole_labels = [2 for items in extrastole_files]\n",
    "\n",
    "print (\"Loading Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43052d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabel_datala files\n",
    "Bunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Bunlabelledtest*.wav')\n",
    "Bunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Bunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Bunlabelledtest_labels = [-1 for items in Bunlabelledtest_sounds]\n",
    "\n",
    "Aunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Aunlabelledtest*.wav')\n",
    "Aunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Aunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Aunlabelledtest_labels = [-1 for items in Aunlabelledtest_sounds]\n",
    "\n",
    "\n",
    "print (\"Loading of unlabel data done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de69e",
   "metadata": {},
   "source": [
    "### concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f404af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine set-a and set-b \n",
    "x_data = np.concatenate((artifact_sounds, normal_sounds,extrahls_sounds,murmur_sounds,extrastole_sounds))\n",
    "\n",
    "y_data = np.concatenate((artifact_labels, normal_labels,extrahls_labels,murmur_labels,extrastole_labels))\n",
    "\n",
    "test_x = np.concatenate((Aunlabelledtest_sounds,Bunlabelledtest_sounds))\n",
    "test_y = np.concatenate((Aunlabelledtest_labels,Bunlabelledtest_labels))\n",
    "\n",
    "print (\"combined training data record: \",len(y_data), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac82078",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61596d61",
   "metadata": {},
   "source": [
    "### train_test_validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133edf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle - whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "# split data into Train, Validation and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.8, random_state=42, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "y_train = np.array(tf.keras.utils.to_categorical(y_train, len(CLASSES)))\n",
    "y_test = np.array(tf.keras.utils.to_categorical(y_test, len(CLASSES)))\n",
    "y_val = np.array(tf.keras.utils.to_categorical(y_val, len(CLASSES)))\n",
    "test_y=np.array(tf.keras.utils.to_categorical(test_y, len(CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f81f2",
   "metadata": {},
   "source": [
    "### Correct imbalnced data using class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_COUNT = 578\n",
    "COUNT_0 = 40  #artifact\n",
    "COUNT_1 = 129 #murmur\n",
    "COUNT_2 = 409 #normal\n",
    "weight_for_0 = TRAIN_IMG_COUNT / (3 * COUNT_0)\n",
    "weight_for_1 = TRAIN_IMG_COUNT / (3 * COUNT_1)\n",
    "weight_for_2 = TRAIN_IMG_COUNT / (3 * COUNT_2)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec3347",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau, ModelCheckpoint,TensorBoard,ProgbarLogger\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0742e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.05, recurrent_dropout=0.20, return_sequences=True), input_shape = (25,1)))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-4), metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_saver = ModelCheckpoint('set_a_weights.h5', monitor='val_loss', \n",
    "                               save_best_only=True, save_weights_only=True)\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cac80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train, \n",
    "                  batch_size=3, \n",
    "                  epochs=30,\n",
    "                  class_weight=class_weight,\n",
    "                  callbacks=[weight_saver, annealer],\n",
    "                  validation_data=(x_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55833851",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ead5a0",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_pred = model.predict(x_test, batch_size=5)\n",
    "#check scores\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print (\"Model evaluation accuracy: \", round(scores[1]*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d05e0d",
   "metadata": {},
   "source": [
    "## Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81716a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('heart_sounds.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction class \n",
    "y_pred = np.asarray(model.predict(x_test, batch_size=32))\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "print (\"prediction test return :\",y_pred[1], \"-\", int_to_label[y_pred[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing (file_path, duration=10, sr=22050):\n",
    "  input_length=sr*duration\n",
    "  process_file=[]\n",
    "  X, sr = librosa.load(file_path, sr=sr, duration=duration) \n",
    "  dur = librosa.get_duration(y=X, sr=sr)\n",
    "  # pad audio file same duration\n",
    "  if (round(dur) < duration):\n",
    "    y = librosa.util.fix_length(X, input_length)                \n",
    "  mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40, n_fft=512,hop_length=2048).T,axis=0)\n",
    "  feature = np.array(mfccs).reshape([-1,1])\n",
    "  process_file.append(feature)\n",
    "  process_file_array = np.asarray(process_file)\n",
    "  return process_file_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_test, batch_size=32))\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda97d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"artifact\", \"murmur\",\"normal\"]\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6ffcd",
   "metadata": {},
   "source": [
    "## Treatment options for abnormal heartbeat sounds:\n",
    "\n",
    "he treatment depends on the cause. You may need to make lifestyle changes, like increasing your activity level or changing your diet (for example, limiting your caffeine intake). If you smoke, your doctor will help you stop smoking. You might also require medication to control your abnormal heartbeat, as well as any secondary symptoms.\n",
    "\n",
    "For serious abnormalities that don’t go away with behavioral changes or medication, your doctor can recommend:\n",
    "\n",
    "1- cardiac catheterization to diagnose a heart problem\n",
    "\n",
    "2- catheter ablation to destroy tissue that causes abnormal rhythms\n",
    "\n",
    "3- cardioversion by medication or an electrical shock to the heart\n",
    "\n",
    "4- implantation of a pacemaker or cardioverter defibrillator\n",
    "\n",
    "5- surgery to correct an abnormality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f2e46",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "Breath sounds, https://medlineplus.gov/ency/article/007535.htm\n",
    "\n",
    "Classifying Heart Sounds Challenge, http://www.peterjbentley.com/heartchallenge/#aboutdata\n",
    "\n",
    "Audio Deep Learning Made Simple: Sound Classification, Step-by-Step, https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n",
    "\n",
    "Heart Sounds Topic Review, https://www.healio.com/cardiology/learn-the-heart/cardiology-review/topic-reviews/heart-sounds\n",
    "\n",
    "Deep Learning (for Audio) with Python course, https://youtube.com/playlist?list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da541f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d0ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
