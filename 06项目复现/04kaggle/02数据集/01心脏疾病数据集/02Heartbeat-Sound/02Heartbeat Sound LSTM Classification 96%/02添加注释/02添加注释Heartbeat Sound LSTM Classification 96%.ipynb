{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa14be9",
   "metadata": {},
   "source": [
    "**02Heartbeat Sound LSTM Classification 96%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7755ca",
   "metadata": {},
   "source": [
    "## Heartbeat sounds\n",
    "Heart sounds are produced from a specific cardiac event such as closure of a valve or tensing of a chordae tendineae.\n",
    "\n",
    "• S1 result from the closing of the mitral and tricuspid valves.\n",
    "\n",
    "• S2 produced by the closure of the aortic and pulmonic valves.\n",
    "\n",
    "In medicine we call the ‘lub’ sound \"S1\" and the ‘dub’ sound \"S2\".\n",
    "\n",
    "You can learn short intro about heart sounds from this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=dBwr2GZCmQM\n",
    "\n",
    "## About data:\n",
    "Is challenge published in 2012 to classify the heart sound to some categories from ‘AISTATS’ . Data has been gathered from two sources (A) and (B).\n",
    "\n",
    "A) from the general public via the iStethoscope Pro.\n",
    "\n",
    "B) from a clinic trial in hospitals using the digital stethoscope DigiScope.\n",
    "\n",
    "Before we work on this nootebook we handle the data folders and conctante both sources(A&B) to be easy to deal with it.\n",
    "\n",
    "Original Data here: http://www.peterjbentley.com/heartchallenge/\n",
    "\n",
    "## Import main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53202ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa #To deal with Audio files\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, SeparableConv1D, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, add, Flatten, Dense, BatchNormalization, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalMaxPooling2D, Activation, LeakyReLU, ReLU\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score,roc_auc_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/heartbeat-sound/Heartbeat_Sound/\"\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af920c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarin_data      = data_path \n",
    "unlabel_data    = data_path  + \"/unlabel/\"\n",
    "\n",
    "normal_data     = tarin_data + '/normal/'\n",
    "murmur_data     = tarin_data + '/murmur/'\n",
    "extrastole_data = tarin_data + '/extrastole/'\n",
    "artifact_data   = tarin_data + '/artifact/'\n",
    "extrahls_data   = tarin_data + \"/extrahls/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4762b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal files:\", len(os.listdir(normal_data))) #length of normal training sounds\n",
    "print(\"Murmur files:\",len(os.listdir(murmur_data))) #length of murmur training sounds \n",
    "print(\"Extrastole files\", len(os.listdir(extrastole_data))) #length of extrastole training sounds \n",
    "print(\"Artifact files:\",len(os.listdir(artifact_data))) #length of artifact training sounds \n",
    "print(\"Extrahls files:\",len(os.listdir(extrahls_data))) #length of extrahls training sounds \n",
    "\n",
    "print('TOTAL TRAIN SOUNDS:', len(os.listdir(normal_data)) \n",
    "                              + len(os.listdir(murmur_data))\n",
    "                              + len(os.listdir(extrastole_data))\n",
    "                              + len(os.listdir(artifact_data))\n",
    "                              + len(os.listdir(extrahls_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test sounds: \", len(os.listdir(unlabel_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ac601",
   "metadata": {},
   "source": [
    "## EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([len(os.listdir(normal_data)),\n",
    "              len(os.listdir(murmur_data)),\n",
    "              len(os.listdir(extrastole_data)),\n",
    "              len(os.listdir(artifact_data)),\n",
    "              len(os.listdir(extrahls_data))])\n",
    "labels = ['normal', 'murmur', 'extrastole', 'artifact', 'extrahls']\n",
    "plt.pie(x, labels = labels, autopct = '%.0f%%', radius= 1.5, textprops={'fontsize': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e3c86",
   "metadata": {},
   "source": [
    "he figure shows imbalanced data so we need to fix it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514392b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to rondom audio from specific class\n",
    "def random_sound (audio_class):\n",
    "    random_sound = np.random.randint(0,len(os.listdir(audio_class))) \n",
    "    sound = os.listdir(audio_class)[random_sound]\n",
    "    sound = audio_class+sound\n",
    "    sound,sample_rate = librosa.load(sound)\n",
    "    return ipd.Audio(sound,rate=sample_rate),sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af897094",
   "metadata": {},
   "source": [
    "### Waveform\n",
    "Sound is the pressure of air propagates to our ear. Digital audio file is gotten from a sound sensor that can detects sound waves and converting it to electrical signals. Specifically, it's telling us about the wave's displacement, and how it changes over time.\n",
    "\n",
    "X axis, represents time. Y-axis measures displacement of air molecules.This is where amplitude comes in. It measures how much a molecule is displaced from its resting position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show waveform of audio from dataset \n",
    "# X axis, represents time.\n",
    "# Y-axis measures displacement of air molecules.\n",
    "# This is where amplitude comes in. It measures how much a molecule is displaced from its resting position.  \n",
    "def show_audio_waveform(audio_sample):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    librosa.display.waveplot(audio_sample, sr = 22050)\n",
    "#     plt.title(\"Sound\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2956b60",
   "metadata": {},
   "source": [
    "### Spectrum\n",
    "A sound spectrum is a representation of a sound – usually a short sample of a sound – in terms of the amount of vibration at each individual frequency. It is usually presented as a graph of either power or pressure as a function of frequency. The power or pressure is usually measured in decibels and the frequency is measured in vibrations per second (or hertz, abbreviation Hz) or thousands of vibrations per second (kilohertz, abbreviation kHz).\n",
    "\n",
    "The spectrum expresses the frequency composition of the sound and is obtained by analyzing the sound. A sound spectrum is usually represented in a coordinate plane where the frequency f is plotted along the axis of abscissas and the amplitude A, or intensity, of a harmonic component with a given frequency is plotted along the axis of ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edec942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show spectrum of audio from dataset \n",
    "def show_audio_spectrum(audio_sample):\n",
    "    sample_rate = 22050\n",
    "    fft_normal = np.fft.fft(audio_sample)\n",
    "    magnitude_normal = np.abs(fft_normal)\n",
    "    freq_normal = np.linspace(0,sample_rate, len(magnitude_normal)) \n",
    "    half_freq = freq_normal[:int(len(freq_normal)/2)]\n",
    "    half_magnitude = magnitude_normal[:int(len(freq_normal)/2)]\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(half_freq,half_magnitude)\n",
    "    plt.title(\"Spectrum\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e024dc",
   "metadata": {},
   "source": [
    "### Spectogram\n",
    "For us, as human, we sense a sound not only on a particular time by its intensity, but also by its pitch. The pitch is the frequency of the sound - higher pitch corresponding to higher frequency and vice versa. So, to have a representation which is closer to our brain, we can add another dimension - the frequency - to our representation, which is the Spectrogram.\n",
    "\n",
    "A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams.\n",
    "\n",
    "Spectrograms are used extensively in the fields of music, linguistics, sonar, radar, speech processing,seismology, and others. Spectrograms of audio can be used to identify spoken words phonetically, and to analyse the various calls of animals.it can be generated by an optical spectrometer, a bank of band-pass filters, by Fourier transform or by a wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show specrogram of audio from dataset \n",
    "# the output is an image that represents a sound. \n",
    "# X-axis is for time, y-axis is for frequency and the color is for intensity\n",
    "def show_spectrogram (audio_sample):    \n",
    "    # STFT -> spectrogram\n",
    "    hop_length = 512 # in num. of samples\n",
    "    n_fft = 2048 # window in num. of samples\n",
    "    sample_rate = 22050\n",
    "\n",
    "    # calculate duration hop length and window in seconds\n",
    "    hop_length_duration = float(hop_length)/sample_rate\n",
    "    n_fft_duration = float(n_fft)/sample_rate\n",
    "\n",
    "    print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
    "    print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
    "\n",
    "    # perform stft\n",
    "    stft_normal = librosa.stft(audio_sample, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # calculate abs values on complex numbers to get magnitude\n",
    "    spectrogram = np.abs(stft_normal)\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "    # display spectrogram\n",
    "    plt.figure(figsize=(15,10))\n",
    "    librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar()\n",
    "    #plt.set_cmap(\"YlOrBr\")\n",
    "    plt.title(\"Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db9618",
   "metadata": {},
   "source": [
    "### MFCCs\n",
    "We can’t take the raw audio signal as input to our model because there will be a lot of noise in the audio signal. It is observed that extracting features from the audio signal and using it as input to the base model will produce much better performance than directly considering raw audio signal as input. MFCC is the widely used technique for extracting the features from the audio signal.\n",
    "\n",
    "in sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "\n",
    "Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal spectrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n",
    "\n",
    "**MFCCs are commonly derived as follows:**\n",
    "\n",
    "1- Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "\n",
    "2- Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n",
    "\n",
    "3- Take the logs of the powers at each of the mel frequencies.\n",
    "\n",
    "4- Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "\n",
    "5- The MFCCs are the amplitudes of the resulting spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbe552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs\n",
    "# extract 52 MFCCs\n",
    "def show_mfcc_features(audio_sample):\n",
    "    hop_length = 512 # in num. of samples\n",
    "    n_fft = 2048 # window in num. of samples\n",
    "    sample_rate = 22050\n",
    "    \n",
    "    MFCCs = librosa.feature.mfcc(audio_sample, n_fft=n_fft, hop_length=hop_length, n_mfcc=52)\n",
    "\n",
    "    # display MFCCs\n",
    "    plt.figure(figsize=(15,10))\n",
    "    librosa.display.specshow(MFCCs, sr=sample_rate, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"MFCC coefficients\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MFCCs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad20b74",
   "metadata": {},
   "source": [
    "### Dataset Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9003dd6",
   "metadata": {},
   "source": [
    "#### 1. Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcd91e",
   "metadata": {},
   "source": [
    "Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e628723",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_audio, normal_sample  = random_sound(normal_data)\n",
    "normal_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_waveform(normal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e22979",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_spectrum(normal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(normal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d714d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc_features(normal_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee2b92",
   "metadata": {},
   "source": [
    "#### 2. Murmur sound\n",
    "Heart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. They can be a symptom of many heart disorders, some serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28388ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_audio, murmur_sample  = random_sound(murmur_data)\n",
    "murmur_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_waveform(murmur_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_spectrum(murmur_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(murmur_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc_features(murmur_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664384c6",
   "metadata": {},
   "source": [
    "#### 3. Extrastole sound\n",
    "• Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5be329",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrastole_audio, extrastole_sample  = random_sound(extrastole_data)\n",
    "extrastole_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_waveform(extrastole_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_spectrum(extrastole_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8969f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(extrastole_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196810e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc_features(extrastole_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d51d9",
   "metadata": {},
   "source": [
    "#### 4. Artifact sound\n",
    "• In the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_audio, artifact_sample  = random_sound(artifact_data)\n",
    "artifact_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbaaadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_waveform(artifact_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e03de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_spectrum(artifact_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(artifact_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe70aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc_features(artifact_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19b3cb",
   "metadata": {},
   "source": [
    "#### 5. Extrahls sound\n",
    "Extra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eea05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrahls_audio, extrahls_sample  = random_sound(extrahls_data)\n",
    "extrahls_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_waveform(extrahls_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio_spectrum(extrahls_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef343a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(extrahls_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc_features(extrahls_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfeb0e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data,x):\n",
    "    noise = np.random.randn(len(data))\n",
    "    data_noise = data + x * noise\n",
    "    return data_noise\n",
    "\n",
    "def shift(data,x):\n",
    "    return np.roll(data, x)\n",
    "\n",
    "def stretch(data, rate):\n",
    "    data = librosa.effects.time_stretch(data, rate)\n",
    "    return data\n",
    "\n",
    "def pitch_shift (data , rate):\n",
    "    data = librosa.effects.pitch_shift(data, sr=220250, n_steps=rate)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78577436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_data (folder, file_names, duration=10, sr=22050):\n",
    "    '''\n",
    "        Extract MFCC feature from the Sound data from the audio data. \n",
    "        Augmentation of sound data by adding Noise, streaching and shifting.\n",
    "        52 features are extracted from each audio data and used to train the model.\n",
    "        \n",
    "        Args: \n",
    "            dir_: Input directory to the Sound input file.\n",
    "        \n",
    "        Returns:\n",
    "            data: list of features extracted from the sound file.\n",
    "    '''\n",
    "    input_length=sr*duration\n",
    "    features = 52\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            sound_file = folder+file_name\n",
    "            X, sr = librosa.load( sound_file, sr=sr, duration=duration) \n",
    "            dur = librosa.get_duration(y=X, sr=sr)\n",
    "            # pad audio file same duration\n",
    "            if (round(dur) < duration):\n",
    "                print (\"fixing audio lenght :\", file_name)\n",
    "                X = librosa.util.fix_length(X, input_length)  \n",
    "                \n",
    "            # extract normalized mfcc feature from data\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=features).T,axis=0) \n",
    "            feature = np.array(mfccs).reshape([-1,1])\n",
    "            data.append(feature)\n",
    "            \n",
    "            stretch_data_1 = stretch (X, 0.8)\n",
    "            mfccs_stretch_1 = np.mean(librosa.feature.mfcc(y=stretch_data_1, sr=sr, n_mfcc=features).T,axis=0) \n",
    "            feature_1 = np.array(mfccs_stretch_1).reshape([-1,1])\n",
    "            data.append(feature_1)\n",
    "            \n",
    "            stretch_data_2 = stretch (X, 1.2) \n",
    "            mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=stretch_data_2, sr=sr, n_mfcc=features).T,axis=0) \n",
    "            feature_2 = np.array(mfccs_stretch_2).reshape([-1,1])\n",
    "            data.append(feature_2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file)        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f37815",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple encoding of categories, convert to only 3 types:\n",
    "# Normal (Include extrahls and extrastole)\n",
    "# Murmur\n",
    "# Artifact\n",
    "\n",
    "# Map label text to integer\n",
    "CLASSES = ['artifact','murmur','normal']\n",
    "NB_CLASSES=len(CLASSES)\n",
    "\n",
    "# Map integer value to text labels\n",
    "label_to_int = {k:v for v,k in enumerate(CLASSES)}\n",
    "print (label_to_int)\n",
    "print (\" \")\n",
    "int_to_label = {v:k for k,v in label_to_int.items()}\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43698b",
   "metadata": {},
   "source": [
    "### Data Augmenation and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 KHz\n",
    "SAMPLE_RATE = 22050\n",
    "# 10 seconds\n",
    "MAX_SOUND_CLIP_DURATION=10\n",
    "\n",
    "artifact_files = fnmatch.filter(os.listdir(artifact_data), 'artifact*.wav')\n",
    "artifact_sounds = load_file_data (folder=artifact_data, file_names = artifact_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "artifact_labels = [0 for items in artifact_sounds]\n",
    "\n",
    "normal_files = fnmatch.filter(os.listdir(normal_data), 'normal*.wav')\n",
    "normal_sounds = load_file_data(folder=normal_data,file_names=normal_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "normal_labels = [2 for items in normal_sounds]\n",
    "\n",
    "extrahls_files = fnmatch.filter(os.listdir(extrahls_data), 'extrahls*.wav')\n",
    "extrahls_sounds = load_file_data(folder=extrahls_data,file_names=extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrahls_labels = [2 for items in extrahls_sounds]\n",
    "\n",
    "murmur_files = fnmatch.filter(os.listdir(murmur_data), 'murmur*.wav')\n",
    "murmur_sounds = load_file_data(folder=murmur_data,file_names=murmur_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "murmur_labels = [1 for items in murmur_sounds]\n",
    "\n",
    "\n",
    "extrastole_files = fnmatch.filter(os.listdir(extrastole_data), 'extrastole*.wav')\n",
    "extrastole_sounds = load_file_data(folder=extrastole_data,file_names=extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "extrastole_labels = [2 for items in extrastole_sounds]\n",
    "\n",
    "print (\"Loading Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabel_datala files\n",
    "Bunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Bunlabelledtest*.wav')\n",
    "Bunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Bunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Bunlabelledtest_labels = [-1 for items in Bunlabelledtest_sounds]\n",
    "\n",
    "Aunlabelledtest_files = fnmatch.filter(os.listdir(unlabel_data), 'Aunlabelledtest*.wav')\n",
    "Aunlabelledtest_sounds = load_file_data(folder=unlabel_data,file_names=Aunlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
    "Aunlabelledtest_labels = [-1 for items in Aunlabelledtest_sounds]\n",
    "\n",
    "\n",
    "print (\"Loading of unlabel data done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ffa3bf",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine set-a and set-b \n",
    "x_data = np.concatenate((artifact_sounds, normal_sounds,extrahls_sounds,murmur_sounds,extrastole_sounds))\n",
    "\n",
    "y_data = np.concatenate((artifact_labels, normal_labels,extrahls_labels,murmur_labels,extrastole_labels))\n",
    "\n",
    "test_x = np.concatenate((Aunlabelledtest_sounds,Bunlabelledtest_sounds))\n",
    "test_y = np.concatenate((Aunlabelledtest_labels,Bunlabelledtest_labels))\n",
    "\n",
    "print (\"combined training data record: \",len(y_data), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef1f98",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c21303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle - whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "# split data into Train, Validation and Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.8, random_state=42, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=42, shuffle=True)\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "y_train = np.array(tf.keras.utils.to_categorical(y_train, len(CLASSES)))\n",
    "y_test = np.array(tf.keras.utils.to_categorical(y_test, len(CLASSES)))\n",
    "y_val = np.array(tf.keras.utils.to_categorical(y_val, len(CLASSES)))\n",
    "test_y=np.array(tf.keras.utils.to_categorical(test_y, len(CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecc9e7",
   "metadata": {},
   "source": [
    "### Correct Imbalnced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weight \n",
    "TRAIN_IMG_COUNT = 578\n",
    "COUNT_0 = 40  #artifact\n",
    "COUNT_1 = 129 #murmur\n",
    "COUNT_2 = 409 #normal\n",
    "weight_for_0 = TRAIN_IMG_COUNT / (3 * COUNT_0)\n",
    "weight_for_1 = TRAIN_IMG_COUNT / (3 * COUNT_1)\n",
    "weight_for_2 = TRAIN_IMG_COUNT / (3 * COUNT_2)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649b0a7",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lstm = x_train\n",
    "x_val_lstm = x_test\n",
    "x_test_lstm = x_val\n",
    "\n",
    "y_train_lstm = y_train\n",
    "y_val_lstm = y_test\n",
    "y_test_lstm = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc885e06",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f490f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "\n",
    "lstm_model.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(52, 1)))\n",
    "lstm_model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "\n",
    "lstm_model.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(52, 1)))\n",
    "lstm_model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "\n",
    "lstm_model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "lstm_model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "\n",
    "lstm_model.add(LSTM(256, return_sequences=True))\n",
    "lstm_model.add(LSTM(128))\n",
    "\n",
    "\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "\n",
    "lstm_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f986bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "lstm_model.compile(optimizer=optimiser,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cb = [EarlyStopping(patience=20,monitor='val_accuracy',mode='max',restore_best_weights=True),\n",
    "      ModelCheckpoint(\"/kaggle/working/Heart_LSTM_CNN_1.h5\",save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249029d",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_model.fit(x_train_lstm, y_train_lstm, \n",
    "                         validation_data=(x_val_lstm, y_val_lstm),\n",
    "                         batch_size=8, epochs=250, \n",
    "                         class_weight=class_weight,callbacks = cb )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63afd00",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(x_val_lstm, y_val_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "#   plt.plot(epochs, loss, label='training_loss')\n",
    "#   plt.plot(epochs, val_loss, label='val_loss')\n",
    "#   plt.title('Loss')\n",
    "#   plt.xlabel('Epochs')\n",
    "#   plt.legend()\n",
    "#   plt.grid()\n",
    "\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.grid()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend();\n",
    "\n",
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96ed82",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"artifact\" ,\"murmur \", \"normal\"]\n",
    "\n",
    "preds = lstm_model.predict(x_test_lstm)\n",
    "classpreds = [ np.argmax(t) for t in preds ]\n",
    "y_testclass = [np.argmax(t) for t in y_test_lstm]\n",
    "cm = confusion_matrix(y_testclass, classpreds)\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = sns.heatmap(cm, cmap='Blues', annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.title('')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_testclass, classpreds, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce209c",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f91bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heart_prediction (file_path, duration=10, sr=22050):\n",
    "    classes=[\"artifact\",\"murmur\",\"normal\"]\n",
    "    input_length=sr*duration\n",
    "    \n",
    "    X, sr = librosa.load(file_path, sr=sr, duration=duration) \n",
    "    dur = librosa.get_duration(y=X, sr=sr)\n",
    "    \n",
    "    # pad audio file same duration\n",
    "    if (round(dur) < duration):\n",
    "        X = librosa.util.fix_length(X, input_length)\n",
    "        \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=52, n_fft=512,hop_length=2048).T,axis=0)\n",
    "    feature = np.array(mfccs).reshape([-1,1])\n",
    "    \n",
    "    preds=lstm_model.predict(mfccs.reshape(1,52,1))\n",
    "    preds=classes [np.argmax(preds)]\n",
    "    confidence = np.amax(preds)\n",
    "    return preds, confidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5572fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac0692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43412b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
